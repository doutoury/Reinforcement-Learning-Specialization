{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. Week_4-1_Learning Parameterized Policies.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPCmMJBbdQvlFnUzaMuCZag",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doutoury/Reinforcement_Learning_Specialization/blob/main/3_Week_4_1_Learning_Parameterized_Policies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yydQ9qjsKBRW"
      },
      "source": [
        "# Week_4 <br>\n",
        "\n",
        "INDEX <br><br>\n",
        "\n",
        "\n",
        "  - Learning Parameterized Policies <br>\n",
        "    - Learning Policies Directly <br>\n",
        "    - Advantages of Policy Parameterization <br><br>\n",
        "\n",
        "  - Policy Gradient for Contunuing Tasks <br>\n",
        "    - The Objective for Learning Policies <br>\n",
        "    - The Policy Gradient Theorem <br><br>\n",
        "\n",
        "  - Actor-Critic for Continuing Tasks <br>\n",
        "    - Estimating the Policy Gradient <br>\n",
        "    - Actor-Critic Algorithm <br><br>\n",
        "  \n",
        "  - Policy Parameterizations <br>\n",
        "    - Actor-Critic with Softmax Policies <br>\n",
        "    - Demonstration with Actor-Critic <br>\n",
        "    - Gaussian Policies for Continuous Actions <br>\n",
        "    - Week 4 Summary <br><br>\n",
        "  \n",
        "  - Course Wrap-up <br>\n",
        "    - Course 4 Preview\n",
        "\n",
        "\n",
        "<br><br><br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39ARKswlLNZg"
      },
      "source": [
        "## __1. Learning Parameterized Policies__ <br><br>\n",
        "\n",
        "\n",
        "  - Learning Policies Directly <br><br>\n",
        "\n",
        "  - Advantages of Policy Parameterization\n",
        "\n",
        "\n",
        "<br><br><br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrLl-g65LbKc"
      },
      "source": [
        "## $\\cdot$ Learning Policies Directly <br><br>\n",
        "\n",
        "\n",
        "  - Understand how to define policies as parameterized functions <br><br>\n",
        "\n",
        "  - Define one class of parameterized policies based on the softmax function\n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "So far, <br>\n",
        "all the methods we've looked at for learning good policies estimate action values. <br>\n",
        "Every control algorithm we've studied was built on the framework of generalized policy iteration. <br><br>\n",
        "\n",
        "In this module, <br>\n",
        "we'll explore a new class of methods <br>\n",
        "where the policies are parameterized directly.\n",
        "\n",
        "<br><br><br><br><br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AXKaFttE3AE"
      },
      "source": [
        "### An Example of a Policy without Action-values <br><br>\n",
        "\n",
        "\n",
        "Let's think about <br>\n",
        "\" what it means to specify a policy directly \". <br><br>\n",
        "\n",
        "We'll do this in Mountain Car example. <br><br>\n",
        "\n",
        "Previously, <br>\n",
        "we used $\\epsilon - greedy$ to convert approximate action values into a policy. <br>\n",
        "But we can also consider policy which maps states directly to actions without first computing action values.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1PIk7ILxgwn103RkNBPxWd1q0eTdRCrmd\" alt=\"1-01\" width=\"500\">\n",
        "\n",
        "<br>\n",
        "\n",
        "For example in mountain car, <br>\n",
        "we can define such a policy. <br><br>\n",
        "\n",
        "Simply choose accelerate right, when the velocity is positive, <br>\n",
        "$\\pi(\\text{Right}|s) = 1$ <br>\n",
        "And otherwise choose the accelerate left action. <br>\n",
        "$\\pi(\\text{Left}|s) = 1$ \n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "Put in other way, <br>\n",
        "this policy accelerates in whatever direction we are already moving. <br><br>\n",
        "\n",
        "In fact, <br>\n",
        "this simple energy pumping policy is close to optimal. <br>\n",
        "This policy does not make use of action values at all.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "But this was just an example to stimulate your intuitions <br>\n",
        "and to show you that we don't need action-values to construct policies. <br><br>\n",
        "\n",
        "We're not going to specify policies by hand. <br>\n",
        "Rather, we will learn them ( policies ) !\n",
        "\n",
        "<br><br><br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvTm-K4LFWNo"
      },
      "source": [
        "### Parameterizing Policies Directly <br><br>\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ClThfD0iJUt6p7HgLdCA8Pl9pbWaSjN1\" alt=\"1-02\" width=\"500\">\n",
        "\n",
        "<br>\n",
        "\n",
        "We can use the language of function approximation to both represent and learn policies directly ! <br><br>\n",
        "\n",
        "We'll use the greek letter $\\theta$ for the policies parameter vector. <br>\n",
        "This distinguishes it from the parameters $W$ for the approximate value function. <br><br>\n",
        "\n",
        "We use the notation $\\pi(a|s,\\theta)$, to denote the parameterized policy. <br>\n",
        "For a given input state $s$ and action $a$, <br>\n",
        "the parameterized policy function will output the probability of taking that action in that state. <br><br>\n",
        "\n",
        "This mapping will be controlled by the parameters $\\theta$.\n",
        "\n",
        "\n",
        "<br><br><br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE-AsIkkKXEi"
      },
      "source": [
        "### Constraints on the Policy Parameterization <br><br>\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1YVnD2U1mw7MMWx3PxBKdRJU3ueM-22r2\" alt=\"1-03\" width=\"500\">\n",
        "\n",
        "<br>\n",
        "\n",
        "the parameterized function has to generate a valid policy. <br>\n",
        "This means, it has to generate a valid probability distribution over actions for every state. <br><br>\n",
        "\n",
        "Specifically, <br>\n",
        "the probabilities selecting an action must be greater than or equal to $0$. <br>\n",
        "And for each state, the sum of the probabilities over all actions must be $1$. <br><br>\n",
        "\n",
        "It requires some thought to satisfy these conditions for a parameterized function. <br><br>\n",
        "\n",
        "For example, <br>\n",
        "this means we can not use a linear function directly ! <br>\n",
        "( like we did with value function approximation ) <br>\n",
        "There is no easy way to guarantee that a linear function will sum to $1$. <br><br>\n",
        "\n",
        "Instead, <br>\n",
        "we will need to restrict the class of functions we can use to construct policies !\n",
        "\n",
        "<br><br><br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IylOH-XlOxL2"
      },
      "source": [
        "### The Softmax Policy Parameterization <br><br>\n",
        "\n",
        "\n",
        "Let's consider a simple but effective way to satisfy these conditions <br>\n",
        "called the Softmax policy.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1eUCHKMBjmtBR4hqGbjhhQ7EdpJ9gIKxv\" alt=\"1-04\" width=\"500\">\n",
        "\n",
        "<br>\n",
        "\n",
        "Here is the definition of a Softmax Policy. \n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "The function $h(s,a,\\theta)$ shown here <br>\n",
        "is called the action preference. <br><br>\n",
        "\n",
        "A higher preference for a particular action in a state <br>\n",
        "means that the action is more likely to be selected. <br><br>\n",
        "\n",
        "The action preference $h()$ <br>\n",
        "is a function of the state $s$ and action $a$, as well as a parameter vector $\\theta$. \n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "Computing the probability of selecting an action with the softmax $\\pi(a|s,\\theta)$ is simple. <br>\n",
        "We take the action preference, exponentiate it, and then divide by the sum over all the actions for the same thing. <br>\n",
        "$\\pi(a|s,\\theta) \\quad \\doteq \\quad \\displaystyle \\frac{e^{h(s,a,\\theta)}}{\\displaystyle \\sum_{b \\in A} e^{h(s,b,\\theta)}}$ <br><br>\n",
        "\n",
        "The exponential function $e^x$ <br>\n",
        "guarantees the probability is positive for each action ! <br><br>\n",
        "\n",
        "The denominator $\\sum$<br>\n",
        "normalizes the ouput of each action such that the sum over actions is $1$ ! \n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1pS0AR-2vmCV7B2gAog-xhkqponDEbzmb\" alt=\"1-05\" width=\"500\">\n",
        "\n",
        "<br>\n",
        "\n",
        "The action preference <br>\n",
        "can be parameterized in any way we like since the Softmax will enforce the constraints of a probability distribution. <br><br>\n",
        "\n",
        "For example, <br>\n",
        "the action preferences could be a linear function of the state-action features <br>\n",
        "or something more complex like the output of a Neural Network !\n",
        "\n",
        "<br><br><br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9voS1QthPOhH"
      },
      "source": [
        "### Example $\\quad : \\;$ Softmax Policies <br><br>\n",
        "\n",
        "\n",
        "Here's what we get when we pass a particular set of action preferences through the softmax.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1Qp-4sELCNOTnllSS8JPWEhDDGGUGyYXL\" alt=\"1-06\" width=\"500\">\n",
        "\n",
        "<br>\n",
        "\n",
        "The input preferences $h$ can be arbitrarily large, or even negative ! <br><br>\n",
        "\n",
        "No matter how big the preference gets, the probability will never be greater than $1$. \n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1r3zVPYXmBpa3MvooMKg8VJU3q1JDGRw0\" alt=\"1-07\" width=\"500\">\n",
        "\n",
        "<br>\n",
        "\n",
        "If one preference is much larger than all the others, <br>\n",
        "the action probability would be close to one. \n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1oKImndtkbU-nm9HyQpLZqgXlH-YBXyii\" alt=\"1-08\" width=\"500\">\n",
        "\n",
        "<br>\n",
        "\n",
        "If one preference is very small, <br>\n",
        "the softmax policy will still select the action with non-zero probability. <br><br>\n",
        "\n",
        ">For example, <br>\n",
        ">if the preference is negative and the other is positive, <br>\n",
        ">the negative action will still have non-zero probability. \n",
        "\n",
        "<br>\n",
        "\n",
        "Finally, <br>\n",
        "actions with similar preferences will be chosen with near equal probability under a softmax policy.\n",
        "\n",
        "\n",
        "<br><br><br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEGkLDOkU13p"
      },
      "source": [
        "### Action preferences are not action values <br><br>\n",
        "\n",
        "\n",
        "It's important to distinguish between action preferences and action values ! <br><br>\n",
        "\n",
        "Preferences <br>\n",
        "it indicate how much the agent prefers each action. but they are not summaries of future reward. <br><br>\n",
        "\n",
        "Only the relative differences between preferences are impoartant. (?) <br>\n",
        "For example, <br>\n",
        "we could add $+100$ to all the preferences and it would not matter.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1MoATfSs862S8acnLPCrKhczmkELo33PE\" alt=\"1-09\" width=\"500\">\n",
        "\n",
        "<br>\n",
        "\n",
        "An epsilon greedy policy derived from action values can behave very diferently <br>\n",
        "than a softmax policy over action preferences ! \n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1mdFiF-ubhsq6rrxOO-WrBBPsO_ae_X47\" alt=\"1-10\" width=\"500\">\n",
        "\n",
        "<br>\n",
        "\n",
        "In epsilon-greedy, <br>\n",
        "the action corresponding to the highest valued action is selected with high prabability. <br>\n",
        "The probability selecting all the other actions is quite small. <br>\n",
        "Actions with nearly the same but lower action value are selected with much lower probability. <br>\n",
        "Actions with very poor action values are still selected frequently due to the epsilon exploration step. \n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "So even if the agent learns an action has terrible consequences, <br>\n",
        "it will continue to select that action much more frequently <br>\n",
        "than it would under the softmax policy.\n",
        "\n",
        "<br><br><br><br><br>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0KED2BvLeLw"
      },
      "source": [
        "#### Summary <br><br>\n",
        "\n",
        "\n",
        "  - We can parameterize policies directly <br><br>\n",
        "\n",
        "  - We need to define parameterizations that produce valid probability distributions <br><br>\n",
        "\n",
        "  - You should understand softmax policy parameterization\n",
        "\n",
        "\n",
        "<br><br><br><br><br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhSO4vHdMgbm"
      },
      "source": [
        "## $\\cdot$ Advantages of Policy Parameterization <br><br>\n",
        "\n",
        "\n",
        "  - Understand some of the advantages of using parameterized policies\n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "You now know it's possible to learn parameterized policies directly. <br>\n",
        "This means we can have more flexibility in how we solve our problems. <br><br>\n",
        "\n",
        "Now, we consider learning approximate values and learnig approximate policies. <br><br>\n",
        "\n",
        "\" Is it really helpful directly learn policies ? \"\n",
        "\n",
        "\n",
        "<br><br><br><br><br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnFwQFleNdwf"
      },
      "source": [
        "### The Flexibility of Stochastic Policies <br><br>\n",
        "\n",
        "\n",
        "There are many advantages to directly learning the parameters of a policy. <br><br>\n",
        "\n",
        "First, <br>\n",
        "the agent can make it's policy more greedy over time automomously. <br>\n",
        "\n",
        "Why would we want that ? <br>\n",
        "In the beginning, the agent's estimates are not that accurate. <br>\n",
        "So we would want the agent to explore a lot. <br><br>\n",
        "\n",
        "As the estimates become more accurate, the agent should become more and more greedy. \n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1YDi8QGh5cb4EN5SyDQc-ANuX-rMF9ISa\" alt=\"1-11\" width=\"500\">\n",
        "\n",
        "<br>\n",
        "\n",
        "Recall the Epsilon greedy policy we used before. <br><br>\n",
        "\n",
        "The epsilon step chooses a random action to ensure continual exploration. <br>\n",
        "However, the epsilon probability puts a cap on how good the resulting policy can be. \n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1sxo9WmMYV5mT6ZU2FidEOQAG4Bt0_l9F\" alt=\"1-12\" width=\"500\">\n",
        "\n",
        "<br>\n",
        "\n",
        "We could, of course, switch to a greedy policy <br>\n",
        "when we think the agent has explored adequately. <br><br>\n",
        "\n",
        "But we want our agents to be autonomous. <br>\n",
        "We don't want them to rely on us to decide when exploration is done.\n",
        "\n",
        "\n",
        "<br><br><br>\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1FvBqnJXtXqAoLBldfuek3vKfbrZAv0TL\" alt=\"1-13\" width=\"500\">\n",
        "\n",
        "<br>\n",
        "\n",
        "We can avoid this issue with parameterized policies ! <br><br>\n",
        "\n",
        "The policy can start off(?) stochastic to guarantee expiration. <br><br>\n",
        "\n",
        "Then, <br>\n",
        "as learning progresses, the policy can natually converge towrads a deterministic greedy policy. <br><br>\n",
        "\n",
        "A softmax policy can adequately approximate a deterministic policy by making one action preference very large. \n",
        "\n",
        "<br><br><br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdqYI2e2SYen"
      },
      "source": [
        "### Stochastic policies might be better than Deterministic policies under function approximation <br><br>\n",
        "\n",
        "\n",
        "In the Tabular setting, <br>\n",
        "we learn there's always a deterministic optimal policy. <br><br>\n",
        "\n",
        "In function approximation, <br>\n",
        "we may not be able to represent this deterministic policy.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1CJpO6FWiawXYotE43Y-Zl9umWts0i8ft\" alt=\"1-14\" width=\"500\">\n",
        "\n",
        "<br>\n",
        "\n",
        "In the function approximation, <br>\n",
        "Instead, the optimal approximate policy might be a stochstic policy ! <br><br>\n",
        "\n",
        "This suggests it might be useful to learn stochastic policies. \n",
        "\n",
        "\n",
        "<br><br><br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kI3itQFtT10I"
      },
      "source": [
        "### Stochastic policies can be useful with function approximation <br><br>\n",
        "\n",
        "\n",
        "We can see why this is true by considering an example. \n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1CzNuMN9FcbBPufHPJu_EKZ-UlooB3bH8\" alt=\"1-15\" width=\"500\">\n",
        "\n",
        "<br>\n",
        "\n",
        "Imagine the agent is in a corridor. <br>\n",
        "It always starts in the far left state. \n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1BFHl4H9_32eMuBD4e3huw0qpyEgU_6SA\" alt=\"1-16\" width=\"500\">\n",
        "\n",
        "<br>\n",
        "\n",
        "In the left and right states, <br>\n",
        "the actions left and right have their usual consequences. <br>\n",
        "In the middel state, however, <br>\n",
        "the left and right actions are switched. <br>\n",
        "( Moving left will take the agent right state. Moving right takes the agent to the left state ) <br><br>\n",
        "\n",
        "The reward is $-1$ in every step <br>\n",
        "the task is episodic. <br><br>\n",
        "\n",
        "\n",
        "Imagine the functions approximation treats all these states as the same. <br>\n",
        "All three states share the same approximate value.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "#### Deterministic policy <br><br>\n",
        "\n",
        "If we choose to limit ourselves to deterministic policies, <br>\n",
        "we would have no choice but to pick the same action in all the states. <br><br>\n",
        "\n",
        "This would give us just two choices, <br>\n",
        "always move left or always move right. \n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1k6cZuLhhroI_fx9M9Dk5Rynte6elPqv3\" alt=\"1-17\" width=\"500\">\n",
        "\n",
        "<br>\n",
        "\n",
        "If we always move left, <br>\n",
        "we will never leave the start state. <br>\n",
        "So the expected return for that policy is negative infinity $-\\infty$. <br>\n",
        "$v_{\\text{always left}}(S) = -\\infty$ \n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1mjwu-uqbTuNyGgw7lMZMM-iUXvwU35e2\" alt=\"1-18\" width=\"500\">\n",
        "\n",
        "<br>\n",
        "\n",
        "If we always move right, <br>\n",
        "we will reach the middle state and them mover back to the start state, and continue this forever. <br>\n",
        "So the expected return is also negative infinity. <br>\n",
        "$v_{\\text{always right}}(S) = -\\infty$ \n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "#### Stochastic policy <br><br>\n",
        "\n",
        "If we are allowed to choose actions stochastically, <br>\n",
        "we can do much better ( than Deterministic policy ? ) \n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=17KkWrLvsHbgrGWxIMMec4znAo0CFDMa6\" alt=\"1-19\" width=\"500\">\n",
        "\n",
        "<br>\n",
        "\n",
        "We may get stuck for a while, <br>\n",
        "but as long as each action has a non-zero probability, we will eventually reach the terminal state. <br><br>\n",
        "\n",
        "In fact, <br>\n",
        "the best policy under this function approximation is a little particular. <br>\n",
        "Choose the right action 59% of the time and the left action the rest of the time. <br><br>\n",
        "\n",
        "This policy achieves an expected return around $-11.6$. <br>\n",
        "Clearly better than negative infinity $-\\infty$ ! <br>\n",
        "$v_{\\pi_{\\theta}} = -11.6$ \n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "This example may seem a bit contrived, but similar situation can arise in the real world ! <br>\n",
        "\n",
        ">For example, <br>\n",
        ">in our own room, a robot gets s tuck in the corner of the room <br>\n",
        ">because it had a deterministic policy and a limited function approximator.\n",
        "\n",
        "<br><br><br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SNRq9Fj2InE"
      },
      "source": [
        "### Sometimes it is easier to learn a good policy <br><br>\n",
        "\n",
        "Sometimes <br>\n",
        "the policy is more simple than the value function ! \n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1qEjr_P0txJjWhsTSm2k8IGZROs8suU1C\" alt=\"1-20\" width=\"500\">\n",
        "\n",
        "<br>\n",
        "\n",
        "Remember the mountain car problem. <br><br>\n",
        "\n",
        "We spent a lot of time designing a value-based agent to laern an optimal policy. <br>\n",
        "However, in this problem, the energy pumping policy we saw previously is nearly optimal. <br><br>\n",
        "\n",
        "The agent simply select it's action in agreement with the current velocity. <br>\n",
        "If the velocity is negative, then the agent takes the accelerate left action. <br>\n",
        "If the velocity is positive, then the agent take the accelerate right action. <br><br>\n",
        "\n",
        "This policy allows the agent to quickly escape from the valley. <br>\n",
        "As you can see, this is quite a simple policy. \n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "On the other hand, <br>\n",
        "The value function is quite complex ! \n",
        "\n",
        "<br><br><br><br><br>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM6Y_J1AAnQn"
      },
      "source": [
        "### Summary <br><br>\n",
        "\n",
        "\n",
        "You should now understand that parameterized stochastic policies are useful ! <br>\n",
        "because ... <br><br>\n",
        "\n",
        "  - They can autonomously decrease exploration over time <br><br>\n",
        "\n",
        "  - They can avoid failures due to deterministic policies with limited function approximation <br><br>\n",
        "\n",
        "  - Sometimes the policy is less complicated than the value function\n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "In the upcoming videos, <br>\n",
        "we'll see there is not a strict distinction between action value methods and policy based methods ! <br><br>\n",
        "\n",
        "In fact, <br>\n",
        "we can use action values to make it easier to learn a parameterized policy !\n",
        "\n",
        "<br><br><br><br><br>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}
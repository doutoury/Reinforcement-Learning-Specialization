{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week_5-2_Planning","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyMfklBPcJ42naJJ98crg0fL"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"39ARKswlLNZg","colab_type":"text"},"source":["## __2. Planning__ <br><br>\n","\n","\n","  - Random Tabular Q-planning <br><br>\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"SrLl-g65LbKc","colab_type":"text"},"source":["## __$\\cdot$ Random Tabular Q-planning__ <br><br>\n","\n","\n","  - Explain how planning is used to improve policies <br><br>\n","\n","  - Describe random-sample one-step tabular Q-planning \n","\n","<br><br><br>\n","\n","\n","\n","We'll talk about ... <br>\n","One particular view of Plannig. \n","\n",">We now know what models are. <br>\n",">We can discuss how we might use them in Reinforment Learning. <br>\n",">$\\rightarrow \\quad$ That is how one can leverage a model to better infom decision-making without having to interact with the world. <br>\n",">$\\rightarrow \\quad$ We call this process \" Planning with model experience \". \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZcnaCS1aIoyz","colab_type":"text"},"source":["### __Planning__ <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1-k-RcI12qr_O6TtgJciSqauaoEVyo5CV\" alt=\"2-01\" width=\"500\">\n","\n","<br>\n","\n","Planning is a process <br>\n","  - __takes a model__ as intput <br>\n","  - and __produces an impoved policy__. \n","\n","<br><br><br>\n","\n","\n","\n","\n","#### One possible approach to planning <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1Int9ucBQ1mbKvTzK-XjSCs9cqy7QvmZ5\" alt=\"2-02\" width=\"500\">\n","\n","<br>\n","\n","\n","  1. First sample experience from the model. <br>\n","  >This is like imagining possible scenarios in the world <br>\n","  >based on your understand of how the world works. \n","\n","<br>\n","\n","  2. This gernerated experience can then be used to perform updates to the value function <br>\n","  >as if these interactions actually occurred in the world. \n","\n","<br>\n","\n","  3. Behaving greedily with respect to these improved values results in improved policy. \n","\n","\n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"dMYDE2mQrvnT","colab_type":"text"},"source":["### Connection with Q-learning <br><br>\n","\n","\n","1 | 2\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1lZnbUrm25oOeEl2RGlwAtmeYg3zJb1co\" alt=\"2-03\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1_xLUIjK8ETra1XmevTVzVPuz4HpvI3y4\" alt=\"2-04\" width=\"500\">\n","\n","<br>\n","\n","In direct R.L. $\\quad$ ( recall ) <br><br>\n","\n","\n","Q-learning uses experience from the environment, <br>\n","and performs an update to improve a policy. \n","\n","<br><br>\n","\n","In Q-planning <br><br>\n","\n","we use experience from the model <br>\n","and perform a similar update to improve a policy. \n","\n","<br><br>\n","\n","$\\Rightarrow \\quad$ Random-sample one-step tabular Q-planning <br>\n","$\\qquad$ illustrates this idea. \n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"sRTsew6_tsPl","colab_type":"text"},"source":["### __Random-sample one-step tabular Q-planning__ <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1lNW_jiKWY564tilQH_G4h2JnG4_Opj3H\" alt=\"2-05\" width=\"500\">\n","\n","<br>\n","\n","#### __Random-sample one-step tabular Q-planning__ <br><br>\n","\n","This approach assumes we have a sample modle of the transition dynamics. <br>\n","It also assumes that we have a strategy for sampling relevant state-action pairs. <br>\n",">One possible option is to sample states and actions uniformly. \n","\n","<br>\n","\n","  1. Sampling <br><br>\n","\n","  This algorithm first choose a state-action pair at random <br>\n","  from the set of all states and actions. <br><br>\n","\n","  It then queries the sample model with this state-action pair <br>\n","  to produce sample of th next state and rewards. <br><br>\n","\n","\n","  2. Q-learning update <br><br> \n","  \n","  It then performs a Q-learning update on this model transition. <br><br>\n","\n","\n","  3. Greedy policy improvement <br><br>\n","\n","  Finally, it improves the policy by greedifying with respect to the updated action-values. \n","\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1zH3L5OuwrfE","colab_type":"text"},"source":["### __Key point $\\quad : \\;$ Planning only uses imagines experience__ <br><br>\n","\n","\n","(before) Q-learning | Q-planning\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1t1OvC2r53rtw8ABiOedjobcrKMzVkJn2\" alt=\"2-06\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1lxfJCUJD_q8kRNRck9APJwrFABJo4Mex\" alt=\"2-07\" width=\"500\">\n","\n","\n","<br><br>\n","\n","\n","A key point <br>\n","this planning method only uses simulated (or imagined) experience. <br><br>\n","\n","All of these updates can be done without behaving in the world <br>\n","or parallel with the interaction loop. \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hHmzUohLw0I7","colab_type":"text"},"source":["### __Advantages of Planning__ <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1dwu5ENNiuUj4UndIO_ZAfh5cT49wDTjQ\" alt=\"2-08\" width=\"500\">\n","\n","\n","Imagine that Actions can only be taken at specific time points, <br>\n","but learning updates can be executed relatively fast. <br>\n","This results in some waiting time from after the Learning Updates and to when the next action is taken. <br>\n","We can fill in this waiting gime with Planinng Updates !\n","\n","<br><br><br>\n","\n","\n","\n","#### Example $\\quad : \\;$ Standing robot near a cliff <br><br>\n","\n","\n","1 | 2\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1toe3Qr7ZeVtU5XSqCWd9lyIjeTUDyiw7\" alt=\"2-09\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=10_GA4mWHAaB8cY3QYnYBe4Q7rocNiVtz\" alt=\"2-10\" width=\"500\">\n","\n","<br><br>\n","\n","\n","Imagine we have a robot that is standing near a cliff. <br><br>\n","\n","It's model knows about the outcome of stepping off of the cliff, <br>\n","but it is not accurately represented in it's value function or policy. <br><br>\n","\n","It can generate simulated experience of stepping off of the cliff <br>\n","and perform many planning steps on these transitions. <br><br>\n","\n","It's value finction will now better reflect that stepping off of the cliff is bad, <br>\n","and it's policy will have it's step away from the cliff. \n","\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WxAy4ciH3Ais","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - Planning methods use simulated experience to improve policies <br><br>\n","\n","  - Random-sample one-step tabular Q-learning  uses a sample model to randomly generate experience <br><br>\n","\n","  - Q-plannning's value function is then improved using a Q-learning updates \n","\n","\n","<br><br><br><br><br>\n","\n","\n"]}]}
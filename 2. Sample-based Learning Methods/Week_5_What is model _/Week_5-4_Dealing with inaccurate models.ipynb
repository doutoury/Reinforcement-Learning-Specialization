{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week_5-4_Dealing with inaccurate models","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyNVHHqHDxgKKliQjzWtq0i0"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"39ARKswlLNZg","colab_type":"text"},"source":["## __4. Dealing with inaccurate models__ <br><br>\n","\n","\n","  - What if the model is inaccurate ? <br><br>\n","\n","  - In-Depth woth changing environment <br><br>\n","\n","  - Drew Bangnell : self-driving, robotics , and Model based R.L. <br><br>\n","\n","  - Week 4 Summary\n","\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"SrLl-g65LbKc","colab_type":"text"},"source":["## __$\\cdot$ What if the model is inaccurate ?__ <br><br>\n","\n","\n","  - Identify ways in which models can be inaccurate <br><br>\n","\n","  - Explain the effects of planning with an inaacurate model <br><br>\n","\n","  - Describe how Dyna can plan successfully with an incomplete model \n","\n","\n","<br><br><br>\n","\n","So far in the course, <br>\n","we've talked about how agents can improve their policies or value functions <br>\n","by planning with experience generated from a model. <br><br>\n","\n","But what happens if the agent plans with experience form an inaccurate model ? \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Utg-m0croCnh","colab_type":"text"},"source":["### __How models can be inaccurate__ <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1QyvdKppFOy-h12Thu5kImrufanWcppR_\" alt=\"3-24\" width=\"500\">\n","\n","<br>\n","\n","\" What dose it mean for a model to be inaccurate ? \" <br><br>\n","\n","Models are inaccurate <br>\n","when transitions they store are different from transitions that happen in the environment. \n","\n","\n","<br><br><br>\n","\n","\n","\n","__Incomplete models__ <br><br>\n","\n","At the beginning of learning, <br>\n","the agent hasn't tried most of the actions in almost all of the state. <br>\n","The transitions associated with trying those actions in those states are simply missing from the model. <br>\n","We call models of missing transitions \" incomplete models \". <br><br>\n","\n","<br><br>\n","\n","\n","__Inaccurate models__<br><br>\n","\n","The model couls also be inaccurate if the environment changes. <br>\n","Taking an action in a state could result in a different next state and reward than what the agent observed before the change ! <br>\n","We say the model is inaccurate because what actually happens is different from what the model says. \n","\n","<br><br><br>\n","\n","\n","\" So, what happens when we plan with inaccurate models ? \" \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"j9M88rbXvxe3","colab_type":"text"},"source":["### __Planning with an inaccurate model__ <br><br>\n","\n","\n","\" What happens when we plan with inaccurate models ? \" <br><br>\n","\n","\n","The effect of planning with inaccurate models depends on \" how the model is inaccurate \". \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1ky_MgJuN9slnmRUfIGzt6ntcydlloftE\" alt=\"3-25\" width=\"500\">\n","\n","<br>\n","\n","In the beginning, <br>\n","the model is \" incomplete \". <br><br>\n","\n","Since the model can't produce a next state or a reward, <br>\n","it can't be used for planning.  \n","\n","\n","<br><br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1ymOMjS13CPoPUd18PldQ7hs28Ab99qCs\" alt=\"3-26\" width=\"500\">\n","\n","<br>\n","\n","However, as the agent interacts with the invironment, <br>\n","the model stores more and more transitions. <br><br>\n","\n","\n","<br><br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1_lGvdKkU-nd-FkjZY3ycsQxT6MoYJmMh\" alt=\"3-27\" width=\"500\">\n","\n","<br>\n","\n","The model stores more and more transitions, <br>\n","Then, the agent can perform updates by simuldating transitions it's seen before. <br>\n","That means that as long as the agent has seen some transitions, it can plan with the model.\n","\n","<br><br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1RdQuNUDVZPb3LD6qdCBoBlOzWD0hG9Hb\" alt=\"3-28\" width=\"500\">\n","\n","<br>\n","\n","\" Now let's think about how changes in the environment affect planning \". <br><br>\n","\n","Imagine that the agent has already visited every state-action pair and stored it's experience in the model. <br>\n","Then, the environment changes ! <br>\n","A transtition in the model no longer reflects the transition in the environment. <br><br>\n","\n","\" What will happen when the agent tries to plan using it's ' inaccurate model ' ? \" <br><br>\n","\n","If the agent tries to perform a planning update with one of the incorrect transitions in the model, <br>\n","$\\Rightarrow \\quad$ the value function or policy that the agent updates might change in the wrong direction ! \n","\n","<br><br>\n","\n","\n","__Remember that__ <br>\n","Plannning improves the policy with respect to what the model thinks will happen, <br>\n","rather than what will really happen in the environment ! <br>\n","$\\Rightarrow \\quad$ The model becomes outdated when the environment changes. <br>\n","$\\Rightarrow \\quad$ Planning with that model will likely make the agent's policy worse with repect to the environment.\n","\n","<br><br><br>\n","\n","\n","\n","Now that we've gone over the two main ways that models can be inaccurate.\n","\n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"x21xkbHc1fPJ","colab_type":"text"},"source":["### __Dyna(-Q) with an incomplete model__ <br><br>\n","\n","\n","\" how agent could plan successfully with incomplete models ? \" <br><br>\n","\n","\n","We've actually gone over one such algorithm already, Dyna-Q. <br>\n","let's see how Dyna-Q does planning with an incomplete model.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=16GIDZ8466KbylhBuTeey5M0CRiRgjQtd\" alt=\"3-29\" width=\"1000\">\n","\n","<br>\n","\n","In the planning step, <br>\n","we, must determine which state-action pair's to query the model with. <br><br>\n","\n","\n","The model only knows the next state and reward from state-action pairs it has already visited. $\\quad$ ( checked with the red-box ) <br>\n","Therefore, Dyna-Q can only do planning updates from previously visited state-action pairs. <br><br>\n","\n","\n","Dyna-Q only plans the transitions it has already seen. <br>\n","So, in the first few time steps of learning, <br>\n","Dyna-Q might do quite a few planning updates with the same transition. <br><br>\n","\n","\n","However, as Dyna-Q visits more state-action pairs in the environment, <br>\n","it's planning updates become more evenly distributed throughout the state-action space. \n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CULotptj4NH8","colab_type":"text"},"source":["### Summary <br><br>\n","\n","  - Models can be inaccurate if (either) : <br>\n","    - They are incomplete <br>\n","    - The environment changes <br><br>\n","  \n","  - Planning with an inaccurate model improves the policy or value function with respect to the model, <br>\n","  and not the environment <br><br>\n","\n","  - Dyna-Q can plan with an incomplete model <br>\n","  by only sampling state-action pairs that have been previously visited \n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4HfBFEDU6BJU"},"source":["## __$\\cdot$ In-Depth with changing environment__ <br><br>\n","\n","\n","  - Explain how model inaccuracies produce another exploration-exploitation trade-off <br><br>\n","\n","  - Describe how Dyna-Q+ addresses this trade-off\n","\n","\n","<br><br><br>\n","\n","We previously talked about $\\quad$ \" how agents can plan with models that are __incomplete__ \". <br>\n","Now let's discuss $\\qquad \\qquad \\;\\;$ \" how agents can plan with models that are __inaccurate__ \" <br>\n","$\\qquad \\qquad \\qquad \\qquad \\qquad \\quad$ ( because of changes in the environment ) \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Te9geH1W7Lre","colab_type":"text"},"source":["### Exploration and Exploitation for model accuracy <br><br>\n","\n","\n","When the agent's model is inaccurate, <br>\n","planning will likely make the policy or value function worse with repect to the environment. <br><br>\n","\n","That means the agent has a vested interest in making sure it's model stays accurate. \n","<br><br><br>\n","\n","\n","\n","#### Example <br><br>\n","\n","\n","1 | 2\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1XLu-DmpfJydNAHws_QyjT8RPuW1q9UUr\" alt=\"3-30\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1cjfn5Pmqp344l-m0DLKtY1kTezriLNLu\" alt=\"3-31\" width=\"500\">\n","\n","<br>\n","\n","\n","In this exampel, <br>\n","the robust model initially says that the rabbit will go directly to the carrot if it chooses to move right. \n","\n","<br><br>\n","\n","\n","1 | 2\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1RtUMOFcysD2LuE8Z5RV1C36ZrX7QF-8w\" alt=\"3-32\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1zhR3Jm5_matN0WFUvhbxIGrmyrxQdWU7\" alt=\"3-33\" width=\"500\">\n","\n","<br>\n","\n","However, after moving right, the rabbit find itself in the middle square. <br>\n","after experiencing this transition, the rabbit can update it's model of the world. <br>\n","The update reflect that choosing the move right action, in the leftmost square, will result in mocing to the middle square. \n","\n","<br><br><br>\n","\n","\n","\n","We saw that rabbit corrected it's model after experiencing a transition in the environment. <br>\n","In general, an agent might want to double-check that all it's model's transitions are correct. <br>\n","However, double-checking transitions with low valued actions will often lead to low reward. <br>\n","In changing environments, the agent's model might become inaccurate at any time.\n","\n","\n","<br><br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1qK1jkNkAFi_wxms_kj9eiExqgexlYIPS\" alt=\"3-34\" width=\"500\"> \n","\n","<br>\n","\n","So the agent has to make a choice; <br>\n","Explore ro make sure it's model is accurate, or <br>\n","Exploit the model to compute the optimal policy, assuming that the model is correct. <br><br>\n","\n","When the environment changes, the model will be incorrect. <br>\n","It will remain incorrect until the agent revisits that part of the environment that changed and updates the model. <br>\n","This suggest that the agent should epxlore places it has not been to in a while. \n","\n","<br><br><br>\n","\n","\n","\n","Let's think about how that might work. <br>\n","Roughly speaking, the model is more likely to be wrong in states the agent has not visited in a long time. \n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SmYH42D2uPIy","colab_type":"text"},"source":["### More inaccuracy and time <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=18each2zN1FN_1WDqPM_wFvNpdV5_Secp\" alt=\"3-35\" width=\"500\">\n","\n","<br>\n","\n","For example, <br><br>\n","\n","$t=0$ <br>\n","let's say the rabbit knows the turtle starts in the following cell. <br><br>\n","\n","\n","$t=2$ <br>\n","Since the turtle moves slowly, it can't move very far in the first few time-steps. <br><br>\n","\n","\n","$t=100$ <br>\n","However, after a long time, the turtle might be in a totally different cell than it started in, <br>\n","making the rabbit's model incorrect !\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6bm5g0pQw2zg","colab_type":"text"},"source":["### Bonus rewards for exploration <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1rFV-_CQxOPcsMsY1uPjfUyTBLPj8nvrW\" alt=\"3-36\" width=\"500\">\n","\n","<br>\n","\n","To encourage the agent to revisit it's state periodically, <br>\n","We can add a bonus to the reward used in planning. <br><br><br>\n","\n","\n","\n","$\\kappa \\; * \\; \\sqrt{\\tau}$ <br><br>\n","\n","This bonus is simply Kappa times the square root of Tau, where <br><br>\n","\n","  - $r$ is the reward from the model <br><br>\n","\n","  - $\\tau$ is the amount of time it's been since the state-action pair was last visited in the environment. <br>\n","  ( $\\tau$ is not updated in the planning loop, that would not be a real visit )\n","  - $\\kappa$ is a small constant that controls the influence of the bonus on the planning update. <br>\n","  ( If $\\kappa = 0$, we would ignore the bonus completely )\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KC2qlpMpzWy6","colab_type":"text"},"source":["### The Dyna-Q+ algorithm <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=17NJ3T-_qUdBdzjBADHHvFr0eFd-sGOme\" alt=\"3-37\" width=\"500\">\n","\n","<br>\n","\n","Adding this exploration bonus to the planning updates results in the Dyna-Q+ algorithm. <br>\n","By artificially increasing the rewards used in planning, <br>\n","we increase the value of state-action pairs it haven't been visited recently. <br><br>\n","\n","\" How exactly does this encourage exploration ? \" <br><br>\n","\n","Imagince a state-action pair $(S,A)$ that has not been visited in a long time. <br>\n","That means the $\\tau$ will be large. <br>\n","As $\\tau$ grows, the bonus becomes bigger and bigger. <br>\n","Eventually, planning will change the policy to go directly to $S$ due to the large bonus. <br><br>\n","\n","When the agent finally visits state $S$, it might see a big reward, or it might be disappointed. <br>\n","Either way, the model will be updated to reflect the dynamics of the environment. \n","\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1Yi7YsxO1GEg","colab_type":"text"},"source":["### Dyna-Q v.s. Dyna-Q+ in a changing environment <br><br>\n","\n","\n","Let's look at the shortcut maze <br>\n","to understand the difference between Dyna-Q and Dyna-Q+. <br><br>\n","\n","\n","#### Example <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1TjMSJhSDDmlE10qWmk8M7lIM-wTjg3Dn\" alt=\"3-38\" width=\"500\">\n","\n","<br>\n","\n","In this example, <br>\n","the object is to get to the goal from the start state as fast as possible. <br><br>\n","\n","$R = +0$ everywhere <br>\n","$R_t = +1$ terminal transition <br>\n","$\\gamma \\leq 1$ the discount factor is less than 1 because we want to encourage the agent to rech the goal quickly. <br>\n","$\\epsilon - greedy$ action selection used in both Dyna-Q and Dyna-Q+.\n","\n","<br><br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1ZbaQBGPncLE6qgJCaTiKQ7esl4eZRpSq\" alt=\"3-39\" width=\"500\">\n","\n","For the first half of the experiment, <br>\n","Dyna-Q and Dyna-Q+ behave very similarly. <br><br>\n","\n","In this case, <br>\n","the incresed exploration by Dyna-Q+ helps define a good policy more quickly. <br>\n","That is why Dyna-Q+ line is above the Dyna-Q line. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1ughL-G0bI_erKSHGDzfOLqkd6MIKuQRP\" alt=\"3-40\" width=\"500\">\n","\n","<br>\n","\n","Now, we create a shortcut on the right side of the wall <br>\n","Let's see how Dyna-Q and Dyna-Q+ adapt to that change. <br><br>\n","\n","Can they find the shorter path to the goal through the right side of the wall ? <br><br>\n","\n","\n","The result <br><br>\n","\n","Dyna-Q+ finds a shortcut soon after the environment changes, <br><br>\n","\n","Dyna-Q on the other hand, does not find the shortcut in the time allotted. <br><br>\n","\n","Eventually, Dyna-Q will find the shortcut by re-exploring the entire state-action space using $\\epsilon - greedy$. <br>\n","However, doing so would require at least 7 exploratory actions in a row. <br>\n","So it could take a very long time. <br><br>\n","\n","\n","In this environment, <br>\n","the persistent and systematic exploration of Dyna-Q+ was key. \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9Rm4Pqs56Dfn","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - Model accuracy poduces a variant of the exploration-exploitation trade-off <br>\n","  ( the agent has to explore to make sure it's model is accurate ) <br><br>\n","\n","  - Dyna-Q+ uses exploration bonuses to explore the environment \n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2m5LgH8p3Uu6","colab_type":"text"},"source":["### Week 4 Summary <br><br>\n","\n","\n","We learned about planning, learning, and acting. <br>\n","Let's go over the concepts and algorithms we've covoered in this module.\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LIF-eBnb4Um6","colab_type":"text"},"source":["### Lesson1. Types of modules <br><br>\n","\n","\n","We introduced models and talked about the difference between distribution and sample models. <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1c5luFXMVIM-sETnJO1x7V93j2jRFlH-X\" alt=\"5-01\" width=\"500\"> \n","\n","<br>\n","\n","Distribution models store the probability of every possible outcome. <br>\n","However, this may require a large amount of memory. <br><br>\n","\n","Sample models are usually much more compact than distribution models <br>\n","because they don't explicitly store the probability of each outcome. \n","\n","\n","<br><br><br>\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"z3QxyW-k7FB6","colab_type":"text"},"source":["### Lesson2. Random-sample one-step tabular Q-learning <br><br>\n","\n","\n","We introduced One-step Q-planning. <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1S4Z4V8DqVIs4YmPLslUrpByBFjcq7owf\" alt=\"5-02\" width=\"500\"> \n","\n","<br>\n","\n","Q-planning uses the same update as Q-learning. <br>\n","However, it uses expericence generated by a model instead of real experience. <br>\n","\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4oUShZuY7pTX","colab_type":"text"},"source":["### Lesson3. the Dyna-Q architecture and the Dyna-Q algorithm <br><br>\n","\n","\n","We introduced the Dyna architecture. <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1s3DDF4PKSUcqA1N34Ua654UN_-uCYvYT\" alt=\"5-03\" width=\"500\">\n","\n","<br>\n","\n","Dyna incorporates both planning and learning in a single agent. <br>\n","We saw that a Dyna Q-agent can learn much more quickly by using many planning updates. \n","\n","\n","<br><br><br>\n","\n","\n","\n","### Lesson 4. model's inaccyracy and Dyna-Q+ <br><br>\n","\n","\n","Models aren't always accurate. <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1eYj8mCS4z1jzYK8D7ehfrxtKLdD4SPMz\" alt=\"5-04\" width=\"500\">\n","\n","<br>\n","\n","We talked about how inaccurate models affect planning. <br>\n","We pointed out how Dyna-Q can plan effectively using an incomplete model but not if the model is wrong. <br><br>\n","\n","\n","Finally, we introduced Dyna-Q+. <br>\n","This algorithm uses a reward bonus in it's planning updates to encourage exploration. <br>\n","By exploring, Dyna-Q+ keeps it's model up-to-date and accurate, resulting in better performance. \n","\n","\n","<br><br><br><br><br>\n","\n","\n"]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week_5-3_Dyna as a formalism for planning","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyP+uYCkPwUSE+XGfPs1p6Ii"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"39ARKswlLNZg","colab_type":"text"},"source":["## __3. Dyna as a formalism for planning__ <br><br>\n","\n","\n","  - The Dyna Architecture <br><br>\n","\n","  - The Dyna Algorithm <br><br>\n","\n","  - Dyna & Q-learning in a Simple Maze\n","\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"SrLl-g65LbKc","colab_type":"text"},"source":["## __$\\cdot$ The Dyna Architecture__ <br><br>\n","\n","\n","  - Understand how simulating experience form the model differs from interacting with the environment <br><br>\n","\n","  - Understand how the Dyna architecture mixes direct R.L. updates and Planning upates\n","\n","<br><br><br>\n","\n","\n","\n","We've talked about two sources of experience. <br>\n","  1. The environment <br>\n","  2. The model \n","\n","<br><br>\n","\n","In direct R.L., <br>\n","the agent learns from experience generated by interacting with the world like in Q-learning. <br><br>\n","\n","\n","In planning, <br>\n","But the agent can also generate simulated experience from the model and use it for planning, like in Q-planning. <br><br>\n","\n","\n","Today, <br>\n","We'll talk about one way to combine direct R.L. and planning, <br>\n","$\\rightarrow \\quad$ called Dyna. \n","\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FyXrcaXk65yR","colab_type":"text"},"source":["### __Combining Q-learning and Q-planning__ <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1z3Bc3pxg3lk0q4-_lmOVFDjwRN5Bs8_9\" alt=\"3-01\" width=\"500\">\n","\n","<br><br>\n","\n","\n","#### __\" We can combine Q-learning and Q-planning through the Dyna architecture ! \"__ \n","\n","<br><br>\n","\n","\n","$\\qquad \\begin{align} &\\text{Q-learning} &&: \\text{performing updates using environment experience} \\\\\n","& \\qquad + \\\\\n","&\\text{Q-planning} &&: \\text{performing updates using simulated experience from the model} \\\\\n","& \\qquad \\Downarrow \\\\\n","&\\text{Dyna architecture} && \\text{( combine two control methods )} \\end{align}$\n","\n","\n","<br><br><br>\n","\n","\n","\n","#### __Main components of Dyna__ <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1qPadW6GTGvmGq6nJ2KytK666SnbA2vfh\" alt=\"3-02\" width=\"500\">\n","\n","<br><br>\n","\n","\n","  - __Environment__ <br><br>\n","\n","  - __Policy__ <br><br>\n","  \n","> We have the usual environment and policy. <br>\n",">They generate a stream of experience.\n","\n","<br>\n","\n","$\\Rightarrow \\quad$ \" We __use this experience__ to __perform direct R.L. updates__ ! \"\n","\n","<br><br><br>\n","\n","\n","\n","#### __Updates process of Dyna__ <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1rDE3wEBgsrmBUk5_5q3QBhopdLqYihtR\" alt=\"3-03\" width=\"500\">\n","\n","<br><br>\n","\n","\n","  1. __Direct R.L. updates__ $\\quad$ ( performing ) <br><br>\n","\n","$\\rightarrow \\quad$ using this __environment experience__ $\\qquad$ ( directly )\n","\n","<br><br>\n","\n","  2. __Planning updates__ $\\quad$ ( performing ) <br><br>\n","\n","$\\rightarrow \\quad$ using __model (experience)__ $\\qquad \\qquad \\qquad$ ( come from some where ) <br><br>\n","  \n",">environment experience can be used to learn the model <br>\n",">This model will be used to generate model experience \n","\n","<br><br><br>\n","\n","\n","\n","#### __Process search control__ <br><br>\n","\n","\n","  - \" How the model generates this simulated experience ? \" <br><br>\n","\n","  - \" What states the agent will plan from ? \" <br><br>\n","\n","\n",">In addition, <br>\n",">We want to control how the model generates this simulates experience. <br>\n",">What states the agent will plan from ? <br>\n",">We call this process search control.\n","\n","<br><br>\n","\n","$\\Rightarrow \\quad$ \" __Planning updates__ are performed using __the experience generated by the model !__ \" \n","\n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"TBMKRZs2GWqY","colab_type":"text"},"source":["### Example $\\quad : \\;$ how does Dyna work ? <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1MA6pdXbNK45QZFPkI4RFvq6VTHD1-qUi\" alt=\"3-04\" width=\"500\">\n","\n","<br>\n","\n","#### Simple maze problem $\\quad$ ( discounted problem ) <br><br>\n","\n","A robot trying to find it's way out <br>\n","As usual, the robots starts out knowing nothing. <br><br>\n","\n","\n","$\\begin{align} &R &&: \\begin{cases} +1 \\qquad \\text{at the goal} \\\\\n","                                    +0 \\qquad \\text{otherwise} \\end{cases} \\\\\n"," \\end{align}$\n","\n","\n","<br><br><br>\n","\n","\n","#### Direct R.L. updates <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1TGlr3HdgxNUtmqdbr1zM3imIySK1uRDe\" alt=\"3-05\" width=\"500\"> \n","\n","<br>\n","\n","It randomly traverses the maze until it stumble on the gaol for the first time. <br><br>\n","\n","$Q(s,a) \\quad \\leftarrow \\quad Q(s,a) + \\alpha (r + \\gamma \\cdot max_{a'} Q(s',a') - Q(s,a)$ <br>\n","A robot only updates one action-value ( depicted in blue ) <br>\n","This was the only transition that generated non-zero reward. <br>\n","This update is done by direct R.L. \n","\n","\n","<br><br><br>\n","\n","\n","\n","#### ( Model learning ) <br><br>\n","\n","\n","Dyna makes use of all the experience generated during the first episode to learn a model (of) the environment. <br>\n","The yellow states correspond to states visited during the first episode. <br>\n","( The robot didn't visit all of them, but it visited most of them ) <br><br>\n","\n","\n","\n","\n","<br><br><br>\n","\n","\n","\n","\n","#### Planning updates <br><br>\n","\n","\n","\n","1 | 2\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1NECWl2K-Y_extL4sfsv7_eRStnaG1eSj\" alt=\"3-07\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1l3vWvAKRkSB4izas-qWVC1mzSAaD0eiF\" alt=\"3-08\" width=\"500\">\n","\n","<br>\n","\n","Dyna performs planning on every time step. <br><br>\n","\n","However, planning has no impact on the policy during the first episode, <br>\n","even though the model became more accurate on each time step. <br><br>\n","\n","After the first episode completes, planning can really shine. <br>\n",">In out cartoon, the thought bubble represents planning happening in the robot's mind. <br>\n",">The grid world in the bubble represents the robot's model of the world. <br>\n",">The yellow states represent where the model knows what will happen next. <br>\n",">The model knows about all the states the robot visited during the first episode.\n","\n","<br><br>\n","\n","\n","Let's look at what happens <br>\n","if we run the planning loop at the beginning of the second episode. <br><br>\n","\n","Dyna can simulate transitions from any of the state-action pairs the agent visited during the first episode. <br>\n","( In this case, there are a lof of them ) <br>\n","Planning replays the simulated transitions as if they happened in the real world. <br><br>\n","\n","Each step of planning<br>\n","simply applies the Q-learning update to a simulated transition, and updates the value function.\n","\n","\n","<br><br><br>\n","\n","\n","\n","\n","#### Approve the policy <br><br>\n","\n","\n","With enough planning steps, <br>\n","the agent can approve the policy in all the states visited during the first episode. <br>\n",">Here, planning produce a pretty good policy. <br>\n",">The policy is not perfect. <br>\n",">( Can you see where take suboptimal actions ? ) <br><br>\n","\n","The policy is not perfect. <br>\n","The policy is still random in states that were not visited before. <br>\n","We represent these states for the policy was not updated with white squares. <br><br>\n","\n","\n","Let's see how the plan(ed?) policy works in the real world ! <br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1snOTzJfGp8zCsECl5Zl_87RyLI0Zj1eE\" alt=\"3-09\" width=\"500\"> \n","\n","<br>\n","\n","The robot now knows <br>\n","how to get to the goal quickly after just one episode. \n","\n","<br><br>\n","\n","In comparison, <br><br>\n","\n","  - Q-learning require many episodes to achieve similar performance. <br><br>\n","\n","  - Dyna does more computation per step, <br>\n","  but uses limited experience more efficiently. <br><br>\n","\n","\n","This is only a cartoon. <br>\n","Dyna would act slightly differently than shown here. <br>\n","The robot would continue exploring and planning on each step of the second episode. <br><br>\n","\n","So the policy would take random actions once in a while <br>\n","and the policy would continue to change, and improve due to planning. \n","\n","<br><br><br>\n","\n","\n",">We will discuss <br>\n",">\" how to implement Dyna \" next time !\n","\n","\n","\n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XE8MmK07VSrL","colab_type":"text"},"source":["### Summary <br><br>\n","\n","  - The Dyna architecture uses direct R.L. (updates) and planning updates <br>\n","  to learn from both environment and model experience <br><br>\n","\n","  - Direct R.L. updates use environment experience to improve a policy or value function <br><br>\n","\n","  - Planning updates use model experience to improve a policy or value function \n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gffWlDFKidux"},"source":["## __$\\cdot$ The Dyna Algorithm__ <br><br>\n","\n","\n","  - Describe how tabular Dyna-Q works <br><br>\n","\n","  - Identify the direct-R.L. updates and Planning updates in Tabular Dyna-Q <br><br>\n","\n","  - Identify the model learning and search control components of Tabular Dyna-Q\n","\n","\n","<br><br>\n","\n","\n","\n",">We've discussed, <br>\n",">how Dyna unifies planning, learning, and acting. <br>\n",">This unification introduce some additional concepts like model learning and search control. <br>\n",">Today we'll take a closer look at how all these pieces fit together. \n","\n",">We'll discuss. <br>\n",">a specific instance of the Dyna architecture, called tabular Dyna-Q.\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PyOEaZ6EpQya","colab_type":"text"},"source":["### 1. Describe how Tabular Dyna-Q works <br><br>\n","\n","\n","<img src=\"\" alt=\"\" width=\"\">\n","\n","\n","First, <br>\n","let's discuss how Tabular Dyna-Q algorithm learns a model ! \n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"iX4K5AbFj-_6","colab_type":"text"},"source":["### Example $\\quad : \\;$ how Tabular Dyna-Q works <br><br>\n","\n","\n","\n","1 | 2| 3\n","--- | --- | ---\n","<img src=\"https://drive.google.com/uc?id=1DL5x97g6ZJashHFEQzggXAjqQnDkYX8o\" alt=\"3-10\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1VGplzQBu8Ezgv1xE3BkJwaX7ymuSmNl0\" alt=\"3-11\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1ycU9eDcITYzRRK3pXac1lXmuD9wC0j6h\" alt=\"3-12\" width=\"500\">\n","when the rabbit chooses to move right in state A, <br> the rabbit transitions to state B with a reward of +0. <br> There's only one outcome. | When the rabbit moves right in stat B, <br> it stays in state B with a reward of one. <br> There's only one outcome. | When the rabbit moves left in state B, <br> it takes the rabbit to state A with a reward of 0. <br> There's only one outcome.\n","\n","<br><br><br>\n","\n","\n","\n","Tabular Dyna-Q assumes deterministic transitions. <br>\n","> <br>\n",">when the rabbit chooses to move right in state A, there's only one outcome. <br>\n",">The rabbit transitions to state B with a reward of +0. <br>\n","> ~ <br>\n",">\n","\n","After the rabbit has experienced these transitions, the model knows what happens in these state-action pairs. <br>\n","If a transition occrred once, it will always unfold that way in the future. <br>\n","The model can be certain about this. \n","\n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Pd5OYpoypozm","colab_type":"text"},"source":["### The Dyna-Q algorithm $\\quad$ ( especially Tabular Dyna-Q ) <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1f-foEaemuhtBycC34KZIs-sM7KK2Mxc7\" alt=\"3-13\" width=\"1000\">\n","\n","<br>\n","\n","1 <br><br>\n","\n","First, we have the ususal agent environment interaction loop. <br><br>\n","\n","In the current state, <br>\n","The agent selects an action according to it's $\\epsilon - greedy$ policy. <br>\n","It then observes the resulting reward and next sate. <br>\n","It performs a Q-learning update with this transition, $\\quad \\leftarrow$ called Direct-R.L. <br><br>\n","\n",">If we were to stop here, <br>\n",">we would get exactly the Q-learning algorithm. \n","\n","<br><br>\n","\n","\n","\" This is where thing start to differ from model-free methods ! \"\n","\n","\n","<br><br>\n","\n","\n","2 <br><br>\n","\n","Second, because we assume the environment is deterministic, <br><br>\n","\n","Dyna-Q will this transition and pereform a model learning step with it. <br>\n","To do so, the algorithm memorizes the next state and reward for the given state-action pair. \n","\n","<br><br>\n","\n","\n","3 <br><br>\n","\n","Third, Dyna-Q performs several steps of planning. <br>\n","Each planning step consists of three steps. <br><br>\n","\n","  - Search control <br>\n","  Search control selects a previously visited state-action pair at random. <br>\n","  >It must be a state-action pair the agent has seen before\n","  >Otherwise, the model would now know what happens next\n","  - Model query <br>\n","  Given the state-action pair, we create the model for the next state and reward. <br>\n","  We have now gernerated a model transition. \n","  - Value update \n","  Finally, Dyna-Q performs a Q-learning update with this simulated transition.\n","\n","\n","<br><br>\n","\n","The planning step is repeated many times ! <br><br>\n","\n","The most important thing to remember is that <br>\n","Dyna-Q performs many planning updates for each environment transition. <br><br><br>\n","\n","\n","That's Tabular Dyna-Q ! <br>\n","A simple instance of the Dyna architecture. \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EiemiKHjGKDp","colab_type":"text"},"source":["### Example $\\quad : \\;$ Dyna-Q algorithm <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1_RanwX09BWDCLlkSsI4wa1_ejEmoxsdi\" alt=\"3-14\" width=\"500\">\n","\n","<br>\n","\n","Robot in maze example. <br>\n","( This is just a cartoon to build intuition ) <br><br>\n","\n","It starts out knwoing nothing about the environment. <br>\n","The robot gradually builds a model while interacting with the environment. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1IXE1Bl7SoP-bI1aURm_wMvuwUW2WAdCk\" alt=\"3-15\" width=\"500\">\n","\n","<br>\n","\n","The first episode takes a 184 steps. <br>\n","In that time, the agent only change the value of the state-action pair beside the goal. <br><br>\n","\n","Without planning, <br>\n","the next few episode would probably be just as long ! <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1O7EHPFvw5t6-qE7vzdQn4r7TC9FomAD7\" alt=\"3-16\" width=\"500\">\n","\n","<br>\n","\n","Let's look at the next episode, <br>\n","and study the impact of doing 100 steps of planning on every time step. <br><br>\n","\n","The last time we talked about this robot, <br>\n","it was doing way more planning on each step. <br>\n","So this time around, it will take longer for Dyna-Q to find a good policy. \n","\n","<br><br>\n","\n","\n","After the just one step, <br>\n","Planning updated the values of two more states. <br>\n","Now the agent knows the correct actions in the final corridor.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1t3-Mc6bJkJbzD7yoRnKxnwS_Zz3D4sjg\" alt=\"3-17\" width=\"500\">\n","\n","<br>\n","\n","Let's run Dyna-Q a bit longer. <br>\n","The impact of planning is quite dramatic. <br>\n","Dyna-Q propagates the reward information across the entire state space. <br>\n","Eventually, the agent knows an effective policy for navigating to the goal from most states. \n","\n","<br><br>\n","\n","\n","The policy computed by planning only requires 18 steps to reach the goal. <br>\n","After just two episodes ! ( That's more than 10 times shorter than the first episode ) \n","\n","<br><br><br>\n","\n","\n","\n","Tabular Dyna-Q performed many more updates to the value function than it would have without polanning ! <br>\n","Dyna-Q makes better use of it's limited interaction with the environment. \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AWIPEr3YLEzC","colab_type":"text"},"source":["### Summary <br><br>\n","\n","  - Tabular Dyna-Q mixes planning, learning, and acting through the value function <br><br>\n","\n","  - We studied how Dyna-Q updates it's value function in an example gridworld\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pzuMiobVJ9Sm"},"source":["## __$\\cdot$ Dyna & Q-laening in a Simple Maze__ <br><br>\n","\n","\n","  - Describe how learning from both environment and model experience impacts performance <br><br>\n","\n","  - Explain how having an accurate model allows the agent to learn with fewer environment interactions\n","\n","\n","<br><br>\n","\n","\n",">We've discussed the Dyna architecture <br>\n",">  - how Dyna architecture combines planning, leaning, and action. <br>\n",">  - how Dyna-Q operates in a grid world. <br><br>\n",">\n",">$\\rightarrow \\;\\;\\;$ let's get down to business and run some experiments to see how well Dyna-Q really works !\n","\n","<br>\n","\n","Let's use a small gird world <br>\n","to compare tabular Dyna-Q and model free Q-learning. \n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"iqBYovrsMFxB","colab_type":"text"},"source":["### Example $\\quad : \\;$ Comparing Dyna-Q with different amounts of planning $\\quad$ ( Robot in maze ) <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=14dnccjE7ubEFrhgI-wzWzurAZL9b9Dda\" alt=\"3-18\" width=\"500\">\n","\n","<br>\n","\n","Episodic problem with discount value $\\gamma$ <br>\n","initializing the action value estimates to $0$ <br>\n","$A : $ {left, right, up, down} the agent has 4 actoins. <br>\n","$R : +1$ Transitioning into the goal state <br>\n","$R : +0$ Otherwise <br>\n","$\\gamma = 0.95$ <br>\n","$\\alpha = 0.1$ <br>\n","$\\epsilon = 0.1 $ <br>\n","\n","<br><br>\n","\n","\n","Run the experiment for 50 episodes 30 times, and then average the results. <br><br>\n","\n","\" We'll compare three agents ! \" \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FJyjAP0TOqbW","colab_type":"text"},"source":["### Result of comparing Dyna-Q with different amounts of planning <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1C03te09xJzfkuN0_d3fu6kel-J__2jsq\" alt=\"3-19\" width=\"500\">\n","\n","<br><br>\n","\n","\n","Plot __the average number of steps__ the agent took to complete __each episode__ after more and more episodes. <br><br>\n","\n","\n",">If the agent is doing well, <br>\n",">then the number of steps should decarease with more episodes. <br>\n",">That is, lower on the y-axis is better. \n","\n","<br><br><br>\n","\n","\n","\n","  - 0 planning steps $\\quad$ ( direct R.L. ) <br><br>\n","\n","  If we run Dyna-Q with 0 planning steps <br>\n","  $\\rightarrow \\quad$ we get exactly the Q-learning algorithm ! <br><br>\n","\n","  It slowly gets better <br>\n","  but plateaus at around 14 steps per episode.\n","\n","<br><br><br>\n","\n","\n","\n","  - 5 planning steps <br><br>\n","\n","  If we run Dyna-Q with 5 planning steps <br><br>\n","\n","  it reaches the same performance as Q-learning <br>\n","  but much more quickly.\n","\n","<br><br><br>\n","\n","\n","\n","  - 50 planning steps\n","\n","  If we run Dyna-Q with 50 planning steps <br><br>\n","\n","  it only takes about 3 episodes to find a good policy ! <br>\n","  Dyna-Q is far more efficient.\n","\n","<br><br><br>\n","\n","\n","\n","Loook at the y-axis <br><br>\n","\n","  - After 2 episodes <br>\n","  the policy found by Q-learning ( 0 planning steps ) still requires hundreds of steps to escape the maze. <br><br>\n","\n","  - For the same number of environment interactions ( steps ? ), <br>\n","  Dyna-Q ( with planning steps ) can learn a lot more. \n","\n","<br><br><br>\n","\n","\n","\n","$\\Rightarrow \\quad$ \" This show how planning makes better use of environment experience if the model is corrct \"\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XEM4zWy1WIzG","colab_type":"text"},"source":["### Performance depending on amounts of planning (???) <br><br>\n","\n","\n","Visualizing what's going on <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1UNpGJ-VYXdpMUnooAmLoROUD78E5D3tz\" alt=\"3-20\" width=\"500\">\n","\n","<br>\n","\n","arrow : showing the greedy action according to the estimated value in each state <br>\n","no arrow : meaning every action is equally likely under the policy \n","\n","\n","<br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=171CLkNt3tgHIO4rfk_2eZQ97dlGhGN_x\" alt=\"3-21\" width=\"500\">\n","\n","<br>\n","\n","After one episode, <br>\n","Q-learning updates just one action value. <br>\n","( The one corresponding to the up action in the state next to the goal ) <br><br>\n","\n","This is the only state <br>\n","where a transition with a non-zero reward was experiences. <br><br>\n","\n",">( ??? ) <br>\n",">It would take several more episodes to bootstrap this state's value back to other nearby state. <br>\n",">( ??? ) <br>\n","\n","\n","<br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1mHXsu8QNBiNMgzAekuSbpVO34HED0Wuu\" alt=\"3-22\" width=\"500\">\n","\n","<br>\n","\n","Let's look more closely at how search control impacts planning. <br><br>\n","\n","Our robot will do things slightly differently than Dyna-Q. <br>\n","But that's just emphasize our point more clearly. <br><br>\n","\n","We'll start with 10 planning steps <br>\n","and we'll call __the planning loop__ 10 times in a row, <br>\n","so that's 100 step's of planning in total. <br><br>\n","\n","As you can see, many of the planning updates fail to change the value function. <br>\n","In fact, the agent only updated the valued of 2 state-action pairs after 100 steps of planning. \n","\n","\n","<br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1VdiKhY09Y3u3PzLXdpeHiYCKIefRJzj2\" alt=\"3-23\" width=\"500\">\n","\n","<br>\n","\n","Let's keep going. <br>\n","This time we'll try 100 planning steps per call <br>\n","and let the agent run a bit longer. <br><br>\n","\n","Even with 100 planning steps, <br>\n","the first few calls update only a handful of action values. \n","\n","<br><br><br>\n","\n","\n","\n","\" Why does the agent needs so many planning steps to learn the value function and a reasonable policy ? \" <br><br>\n","\n","The agent needs many planning steps <br>\n","because Search control samples state-action pairs randomly ! <br><br>\n","\n","The planning update will not have any effect <br>\n","if the sample state-action pair produces a 0 TD-error. <br><br>\n","\n","This happened a lot in this environment <br>\n","because all the rewards are 0, and so are the initial values. <br><br>\n","\n","What would happen if we ordered the state-action pairs in a more efficient way ? <br>\n","It turns out the agent could learn a good policy only using a 6 of the planning updates. <br><br>\n","\n","In larger environments, <br>\n","random search control becomes even more problematic. <br><br>\n","\n",">If you want to learn more about this topic, <br>\n",">check out section 8.4 of the textbook. \n","\n","<br><br><br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PyqbnI7blxST","colab_type":"text"},"source":["### Summay <br><br>\n","\n","  - We demonstrated Tabular Dyna-Q (algorithm) in a simple gridworld <br><br>\n","\n","  - Planning can dramatically speed-up learning <br><br>\n","\n","  ( In our experiment, the more planning we did the better the agent performed )\n","\n","\n","<br><br><br><br><br>\n","\n","\n"]}]}
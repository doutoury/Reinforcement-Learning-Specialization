{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2. Week_2-1_Introduction to Monte Carlo Methods","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyNC03ZzuA43AaqB1gXXLK2T"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yydQ9qjsKBRW","colab_type":"text"},"source":["# Week_2 <br>\n","\n","INDEX <br><br>\n","\n","\n","  - Introdunction to Monte Carlo Methods <br>\n","    - What is Monte Carlo ? <br>\n","    - Using Monte Carlo for prediction <br>\n","  \n","  - Monte Carlo for Control <br>\n","    - Using Monte Carlo for action values <br>\n","    - Using Monte Carlo methods for generalized policy iteration <br>\n","\n","  - Exploration Methods for Monte Carlo <br>\n","    - Epsilon-soft policies <br>\n","\n","  - Off-policy Learning for Prediction <br>\n","    - Why does off-policy learning matter ? <br>\n","    - Importance sampling <br>\n","    - Off-policy Monte Carlo prediction\n","\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"39ARKswlLNZg","colab_type":"text"},"source":["## __1. Introduction to Monte Carlo__ <br><br>\n","\n","\n","  - What is Monte Carlo ? <br><br>\n","\n","  - Using Monte Carlo for prediction\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"SrLl-g65LbKc","colab_type":"text"},"source":["## $\\cdot$ What is Monte Carlo ? <br><br>\n","\n","\n","  - Understand how __Monte-Carlo methods__ can be used to estimate __value function__ from __sample interaction__ <br><br>\n","  \n","  - Identify problems that can be solved using Monte-Carlo methods\n","\n","<br><br><br>\n","\n","\n","내생각 <br>\n",">어떤 사건들의 모집단에 대한 분포(or 확률)에 대해 몰라도, 충분히 많은 random sampling 시행을 하면 각 sample (표본집단)의 결과가 모집단의 분포(or 확률)로 근사적으로 따라간다 ! <br><br>\n",">\n",">$\\rightarrow \\quad$ 평균estimate 의 정의가 Sum_(prob. * event) 인데, 충분히 많은 samplin을 한 표본집단에 대한 event 들에 그 표본집단에서의 prob.을 대응시켜 표본평균estimate 을 계산하여도 모집단의 평균에 근사한다 ! <br><br>\n",">\n",">$\\Rightarrow \\quad$ 이러한 통계적 근사방법론을 Bellman equation 을 통한 value function 구할 때 평균 $\\mathbb{E}$ 계산에 이용한다 ! ( 환경에서 사건에 대한 발생가능성 확률 environment dynamic $p$ 를 대체 가능 ! )\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8t7HMwtBFxPA","colab_type":"text"},"source":["### Benefit of Monte Carlo Method <br><br>\n","\n","#### __Repeat random sampling__ <br>\n","\n","  - Monte Carlo methods allow us to __estimate values__ directrly __from our experience__, <br>\n","  from sequences of states, actions, and rewards. <br><br>\n","\n","\n","#### __without the prior knowledge__ of the __environment's dynamics $p$__, <br>\n","\n","  - the agent can accurately estimate a value function <br>\n","  ( So learning from experience is stricking ) <br><br>\n","\n",">The term Monte Carlo is often used more broadly for any __estimation method__ <br>\n","that relies on __repeated random sampling__. In R.L.\n","\n","<br><br>\n","\n","\n","내 생각 정리 <br>\n",">Environment dynamics ($p$) 를 현실에서 알 수 없는 경우가 많으므로, <br>\n","이전처럼 $p$ 가 정해진 문제들에서는 가능했던 Bellman equation 풀이가 어렵다. <br><br>\n",">\n",">따라서, Environment dynamics $p$ 가 주어지지 않더라도, Bellman equation 없이 <br>\n",">Reward $G$ 의 '평균 $\\mathbb{E}$ 을 sampling 을 통하여 표본집단의 평균으로 근사치를 구하는(?) <br>\n",">Monte-Carlo method 로 value function $v$ (or $q$) 에 접근한다 ! \n","\n","\n","<br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"khIbQSoRtRyf","colab_type":"text"},"source":["### Limit of pure Dynamic Programming approach <br><br>\n","\n","\n","#### __\" We do not know environment dynamics function $p$ ! \"__<br><br>\n","\n","\n","$\\qquad$ Earlier we talked about <br>\n","$\\qquad$ How R.L. is connected to Dynamic programming. <br>\n",">To use a __pure dynamic programming__ appoach, <br>\n",">the agent needs to know the __environment's transition probabilities $p$__. (env. dynamics $p$)\n","\n","<br>\n","\n","$\\qquad$ In some problems, <br>\n","$\\qquad$ we simply __don't know__ the __environment transition probabilities $p$__. <br>\n",">Imagine a meteorologist is trying to predict the weather. How the weather change depends on a vriaty of environmental factors. We simply don'tknow the exact probability of weather patterns in the future. \n","\n","<br>\n","\n","$\\qquad$ Calculating the __transition probabilities__ used in __dynamic programming__ <br>\n","$\\qquad$ is __difficult__ even for the reasonable tasks. &ensp; ( The computation can be error-prone and tedious ) <br>\n",">Think about predicting the outcome of twelve rolls of dice. the sheer number and complexity of DP calculations make them tedious and error-prone both in tems of coding and numerical precision. \n","\n","\n","<br><br><br>\n","\n","\n","#### __Recall : Dynamic Programming__ <br>\n","\n",">MDP 상황에서 ( $s$, $a$, $p$, $\\pi$, $\\gamma$? 주어진 상태 ) <br>\n",">Bellman Equation 따라 구해진 values ($a$, $s$) 를 iterative 업데이트 하며, 주어진 환경전이함수 $p$ 에 따라, 주어진 정책 $\\pi$ 에 따라 optimal value 와 optimal policy 를 찾아가는 과정 (?) <br>\n",">모든 state 와 action 을 다 업데이트하며 가치values 를 추적(estimate)해 갔다 ! (필요했..)\n","\n","<br><br><br>\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QqRhSq0nJH98","colab_type":"text"},"source":["### Example : Benefit of Monte-Carlo method <br><br>\n","\n","\n","Here's where Monte Carlo can help. <br>\n","Let's try to find the average sum of 12 dice rolls. <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1lkShLeXAYAzZkvPC-AOCj4Mk_HSrAjQX\" alt=\"1-01\" width=\"500\">\n","\n","<br><br>\n","\n","\n","__Monte Carlo methods__ <br>\n","__don't__ exhaustively __sweep through all__ possible outcomes. <br><br>\n","\n","\n","In fact, you __don't need__ to know the __probability of any outcomes__ to be able to use Monte Carlo methods. <br>\n","Instead, __Monte Carlo__ methods __estimate__ values __by averaging__ over a large number __of random samples__. <br><br>\n","\n","\n","$\\qquad$ Let's roll 12 dice a few times and asee the result. <br>\n","$\\rightarrow \\quad$ __\" The sampled average is fairly close to the true average ! \"__ <br><br>\n","\n","$\\qquad$ The sampled average is 41.57 <br>\n","$\\qquad$ The true average is 42 <br>\n","\n","\n","<br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RfRzIYjJFIQB","colab_type":"text"},"source":["### __Monte-Carlo methods for Policy Evaluation__ <br><br>\n","\n","\n","$\\Rightarrow \\quad$ Evaluate state-value with MC <br><br><br>\n","\n","\n","\n","__Recall__ <br><br>\n","\n","$\\qquad$ In __Reinfocement Learning__, <br>\n","$\\qquad$ we want to __learn a value function__ ! <br><br>\n","\n","$\\Rightarrow \\quad$ \" Value functions represent __expected returns !__ \" <br><br><br>\n","\n","\n","&emsp; | sample average\n","--- | ---\n","observation 1 | <img src=\"https://drive.google.com/uc?id=1lTgVdXxmbVS8WRMoTLqvTC39DFIaEpG4\" alt=\"1-02\" width=\"400\">\n","observation 2 | <img src=\"https://drive.google.com/uc?id=1EY2_V5uWwdx4Sl-Oc8I-mtqY9go6kpJV\" alt=\"1-03\" width=\"400\">\n","observation 3 | <img src=\"https://drive.google.com/uc?id=19nmlC9p-MNiNyxaC8DRCJW4ngk83Jp5a\" alt=\"1-04\" width=\"400\">\n","observation 4 | <img src=\"https://drive.google.com/uc?id=1--gA76WDcpllefplk_laEQvQYJm4JiBr\" alt=\"1-05\" width=\"400\">\n","observation 5 | <img src=\"https://drive.google.com/uc?id=1tBtAFH94UDei_8GLmbyIqTIbcxuyYTn3\" alt=\"1-06\" width=\"400\">\n","observation 6 | <img src=\"https://drive.google.com/uc?id=1AbCHudcVkWaMc3iqONcXUYs-ic-hSZtx\" alt=\"1-07\" width=\"400\">\n","observation 7 | <img src=\"https://drive.google.com/uc?id=1iW5SOvV09EApU02T-5dbzfoIMeX5n1op\" alt=\"1-08\" width=\"400\">\n","observation 8 | <img src=\"https://drive.google.com/uc?id=1N1yWo4VO_i3_n9px1lggcm2XvB6WuK1V\" alt=\"1-09\" width=\"400\">\n","\n","<br>\n","\n","So a Monte-Carlo method for learning a value function <br>\n","$\\qquad$ It would first __observe multiple returns__ $G_t$ from the __same state__ $s_t$. ( for many episodes ) <br>\n","$\\qquad$ Then, it could average those observed returns to estimate the expected return from that state. <br><br>\n","\n","As the number of samples increases, <br>\n","$\\qquad$ the average tends to get closer and closer to the expected return. <br>\n","$\\qquad$ The more returns the agent observes from a state, <br>\n","$\\qquad$ the more likely it is that the sample average is close to the state value. <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1yPqURUY9bVQLjDnZwQd0AHtIlZDE18r5\" alt=\"1-10\" width=\"500\">\n","\n","\n",">These __Returns__ can only be observed __at the end of an episode__. <br>\n","So we will focus on Monte-Carlo methods for episodic tasks. \n","\n","<br><br>\n","\n","\n","\n","#### Recall $\\quad : \\;$ Return 과 Reward 차이 &ensp; [ 1. Week_2 (?) ] <br>\n",">Return $\\quad : \\;$ 한 state $s$ 에 대한 return 은 그 state 에서 실행할 수 있는 <br>\n",">$\\qquad \\qquad \\;$ Episode 의 결과에 대한 보상합 <br>\n",">Reward $\\;\\;\\; : \\;$ 한 state $s$ 에서 한 실행중인 <br>\n",">$\\qquad \\qquad \\;$ 한 Episode 과정에서 단계적으로 얻게 되는 보상들. <br>\n",">( 시작 state 에서 출발한 episode 과정 중에 있는 다른 state 를 지날 때 받는 reward 도 포함 ! )\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vaA8I6S7LB9S","colab_type":"text"},"source":["### Monte-Carlo method __in episodic task__ <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1mt8Tk_y-hAbL-2Xtplx46gJUOtitVIDc\" alt=\"1-11\" width=\"500\">\n","\n","<br>\n","\n","__Monte-Carlo__ methods __in R.L.__ look a bit like __Bandit methods__. <br><br>\n","\n","  - In Bandit methods <br>\n","  $\\qquad$ the value of an arm is estimated using the average payoff sampled by pulling that arm. <br><br>\n","\n","  - In Monte-Carlo methods <br>\n","  $\\qquad$ We consider policies instead of arms. <br>\n","  $\\qquad$ The value of state $s$ under a given policy $\\pi$ is estimated using the average of return <br>\n","$\\qquad$ sampled by following that policy from $s$ to termination (state). \n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"gdmhIDiXOGOt","colab_type":"text"},"source":["### Algorithm of Monte-Carlo method <br><br>\n","\n","__Algorithm__ <br>\n","$\\qquad$ for estimating the __state-value function__ $V(S_t)$ of a policy by __Monte-Carlo method__. <br>\n","\n","<img src=\"https://drive.google.com/uc?id=1lgWGNTQnV_tFb5iMUd9iEy_lThtYS_nG\" alt=\"1-12\">\n","\n","\n","  - __List holding the returns obsrved__ <br><br>\n","  The __Monte-Carlo algorithm__ has to __keep track of multiple observed returns__. <br>\n","  Let's introduce __a list of returns ($G_t$)__, one for each state ($S_t$). <br>\n","  Then we generate an episode following policy $\\pi$ \n","\n","<br><br><br>\n","\n","\n","#### List $\\;\\;$ $Returns(R_t)$ <br><br>\n","\n","$\\qquad$ __\" Each list $Returns(S_t)$__ holds the __returns $G_t$ observed from state $S_t$ \"__ <br><br>\n","\n",">&ensp; For each state ($S_t$) in the episode, <br>\n",">&ensp; we compute the return ($G_t$) and store it in the list of returns ($Returns(S_t)$). <br>\n","\n","<br><br><br>\n","\n","\n","  - __Algorithm for the list holding returns__ <br><br>\n","  \" How can we do that efficiently ? \" <br>\n","  $\\rightarrow \\quad$ Loop backward process !\n","\n","<br><br><br>\n","\n","\n","\n","#### Computing returns efficiently <br>\n","( about one episode observed )<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1Ip64YOg-XmVmYKneGuZ0w4jXwaI1rfgz\" alt=\"1-13\" width=\"500\"> \n","\n","<br>\n","\n","Episode's end $\\quad : \\;$ At time step 5 <br>\n","Discount factor $\\; : \\;$ $\\gamma = 0.5$ <br>\n","Return $\\qquad \\quad \\;\\; : \\;$ $G_0, G_1, G_2, G_3, G_4, G_5$ <br>\n","Reward of seq. $\\;\\; : \\;$ 3, 4, 7, 1, 2 &emsp; ( observed in one episode ??? )<br><br>\n","\n","Let's start by writing down the equation for each return. <br>\n","Notice that each return is included in the equation for the previous time step's return. <br><br>\n","\n","$\\rightarrow \\quad$ That means we can avoid duplicating computations by starting at $G_5$ and working out way backwards ! <br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1jmp-e-Ytrvk0H2Bvk4y-38JHLns8TE6v\" alt=\"1-13\" width=\"500\"> \n","\n","<br>\n","\n","The episode ends at $t = 5$, <br>\n","so $\\; G_5 = 0$ by definition. <br>\n","And then we can compute the previous time step's return backward. <br><br>\n","\n","\n","$G_4 = 0 + 0.5 * 0 = 2$ <br>\n","$G_3 = 1 + 0.5 * 2 = 2$ <br>\n","$G_2 = 7 + 0.5 * 2 = 8$ <br>\n","$G_1 = 4 + 0.5 * 8 = 8$ <br>\n","$G_0 = 3 + 0.5 * 8 = 7$ <br>\n","\n","\n","<br><br>\n","\n","$\\Rightarrow \\quad$ 일일이 Rewards 의 할인평균 compute 하는 것보다 뒤로 <br>\n","$\\qquad$ 마지막 Reward 하나로 재귀적 compute 하는 것이 더 효율 적임을 말하는 건가 ?!\n","\n","<br><br><br>\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5WbmyqrPaoD-","colab_type":"text"},"source":["#### Algorithm $\\quad : \\;$ Loop backward process (?) <br><br>\n","\n","\n","By working __backwards__ from the __terminal (state)__ time-step, <br>\n","we can __efficiently__ compute the __returns ($G$)__ for __each state__ encountered during the __episode__. <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1t7nB_GmvjiVL7LpQRpHNZhDmdKgzqZ9y\" alt=\"1-15\">\n","\n","<br>\n","\n","The first return is just the last rewards. (뒤에서 부터 계산하므로) <br>\n","So we add the last reward to the list of returns for $s_{t-1}$ (one step prior ?). <br>\n","Then, we set the value ($v$) of $s_{t-1}$ to be the average of returns $s_{t-1}$. <br><br>\n","\n","On the previous time step <br>\n","$_{t-2}$ we calculate the return as before then add it to the list of returns for $s_{t-2}$. <br>\n","Finally, we update the value ($v$) of $s_{t-2}$ <br><br>\n","\n","... <br><br>\n","\n","\n","If we continue this loop until the end, <br>\n","we'll have __updated the values__ for __all the states__ visited in the __current episode__. \n","\n","<br><br><br>\n","\n","\n","\n","Then we can repeat the whole process __over many episodes__ <br>\n","and eventually learn a good __estimate__ for the __value function ($v(s)$)__. \n","\n","\n","<br><br><br>\n","\n","\n","\n","\n","#### To the Sample average <br><br>\n","\n","$\\quad$ Sample average than all average ! <br><br>\n","\n",">You might be wondering if we can __avoid__ keeping __all the sampled__ returns in a list. In fact, we can. <br><br>\n",">\n",">We can __incrementally update__ the __sample average__ estimated using the formula here. <br>\n",">Recall what we discussed using such incremental updates when estimating action values for bandits. We use the conceptually simpler sample average in this module to focus on the key ideas from __Monte Carlo__. After this module, we will switch to using the __incremental update__. \n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"8obnxq1yR2VH","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - We talked about how __Monte-Carlo__ methods __learn__ directly from __interaction__ <br>\n","  ( Notably, they don't need a model of the environments dynamics )<br>\n","\n","  - We showed a __Monte-Carlo algorithm__ for learning __state-values__ in __episodic problems__\n","\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Sd9eUM2Cg-lS"},"source":["## $\\cdot$ Using Monte Carlo for prediction <br><br>\n","\n","\n","  - Use __Monte-Carlo Prediction__ to estimate the __value function__ for a given policy. \n","  \n","<br><br><br>\n","\n","\n","\n","The __essential__ flavor of __Monte-Carlo__ <br>\n",">Imagine <br>\n",">asking 1000 people about their experience doing something and <br>\n",">deciding to do that thing if most of their experiences turned out well. \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2RKqdzmgg-lZ"},"source":["### Example : Blackjack <br><br>\n","\n","\n","\n","Let's look at an example using __Monte-Carlo__ for __prediction__ ! <br><br>\n","\n","\n","__Blacjack__ <br>\n",">Blacjack is played with a standard deck of 52 cards. <br>\n",">The objective is to collect cards so that their sum is as largest as possible without exceeding 21. <br>\n",">Face cards (K, Q, J) are counted as 10. <br>\n",">Ace can count as 11 or 1 based on the player's preference. <br>\n",">The game begins with two cards dealt both the player and the dealer. The player can see one of the dealer's cards, but the other is face down. <br><br>\n",">\n",">If the player immediately has 21, they win <br>\n",">unless the dealer also has 21, in which case they draw. <br><br>\n",">\n",">If the player doesn't have 21 immediately, they can request more cards one at a time or hit. <br>\n",">If the sum of the player's cards ever exceeds 21, we say the player goes bust and loses. <br>\n",">Otherwise when the player decides to stop requesting cards or sticks, it becomes the dealer's turn. <br><br>\n",">\n",">The dealer only hits if the sum of their cards is less then 17, if the dealer goes bust and the player wins. <br>\n",">Otherwise, the winner of the game is a player who's sum is closer to 21. \n","\n","<br>\n","\n","<img src=\"https://drive.google.com/uc?id=1ikM98JPJTmKbbDuN9eCA2i67Xx_2jEom\" alt=\"1-16\" width=\"500\">\n","\n","<br><br>\n","\n","\n","\n","#### __Problem formulation__ <br><br>\n","\n","\n","\n","We can formulate this problem as as __undiscounted MDP__ <br>\n","where __each game__ of blackjack corresponds to an __episode__. <br><br>\n","\n",">__Undiscounted MDP__ <br>\n",">Episodic MDP 이기 때문에, 한 episode 의 종료시점이 있으음으로 인해 Return $\\mathbb{E}[G_t]$ 가 무한대로 발산하지 않으므로 $\\mathbb{E}[G_t]$ 를 구성하는 $R_t$에 미래에 대한 할인율 discount factor를 곱해주지 않아도 된다 !\n","\n","<br>\n","\n","One game $\\qquad \\qquad \\qquad \\quad \\; : \\;$ One episode <br><br>\n","\n","Reward $\\qquad \\qquad \\qquad \\qquad \\; : \\;$ -1 for a loss / 0 for a draw / 1 for a win <br>\n","$\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad$ ( given at the end of the game ) <br><br>\n","\n","Action $\\qquad \\qquad \\qquad \\qquad \\;\\; : \\;$ Hit or Stick <br>\n","\n","States (200 in total) * $\\qquad \\quad \\; : \\;$ Combination of  $\\begin{cases} \\cdot \\; \\text{Whether the player has a usable ace (yes/no)} \\\\\n","\\cdot \\; \\text{The sum of the player's cards (12 ~ 21)} \\\\\n","\\cdot \\; \\text{The card the dealer shows (Ace ~ 10)} \\end{cases}$ <br>\n"," $\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\rightarrow \\qquad \\qquad \\qquad$ __( described as 3-dimentional space of states ! )__\n","\n","Sampling with replacement $\\quad : \\;$ Cards are dealt from a deck with replacement \n","\n","<br><br><br>\n","\n","\n","\n","Rewards are given at the end of the game with -1 for a loss, 0 for draw, +1 for a win. <br>\n","( Here is only one reward for whole one episode. So reward get directly to be the return of the one episode (?) ) <br>\n","The player has two actions, hit or stick. The player decides to hit or stck based on three variables, whether they have a usable ace which is an ace that can count as 11 without their sum exceeding 21, the sum of their cards and the card the dealer shows. <br>\n","There are 200 states in total. __( It's described as a combination of 3 features(?) 3-dimentionally ! )__<br>\n","We assume the cards are dealt from the deck with replacement. This means that there's no point in keeping track of the cards that had been dealt and that the state repects the Markov property. \n","\n","\n","\n","\n","\n","<br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gFPS_RPJOTlZ","colab_type":"text"},"source":["#### __States / Returns(S) / V(S)__ <br><br>\n","\n","\n","Let's use __Monte Carlo__ to learn the __value function__ <br>\n","for the policy that sticks when the player's sum is 20 or 21. \n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8V0M1lfrDQjY","colab_type":"text"},"source":["\n","\n","\n","\n","\n","__One epdisode case example :__ <br><br>\n","\n","\n","Suppose <br><br>\n","\n","In the first state, <br>\n","the agent's card shows a total of 13 with no usable ace, and <br>\n","the dealer's visible card shows 10. <br><br>\n","\n","Since the agent's policy is fixed, it hits and gets a card of 7 moving it to 20 (as a sum). <br>\n","The agent sticks because it's sum is 20, and now the dealer's turn. <br><br>\n","\n","The dealer draws a 9 and goes bust losing the game. <br>\n","It results in +1 reward for the agent. <br><br>\n","\n","\n","$\\rightarrow \\quad$ Now that the first __episode__ is __over__, <br>\n","$\\qquad$ the agent can perform a __Monte Carlo update__ and so __start learning__. <br><br>\n","\n","\n","\n","The __discount factor $\\gamma = 1$__. <br>\n","The __return__ $G_k$ from __each state__ $s$ in a episode is $+1$. <br>\n","( because there are __only__ non-zero reward +1 and zero reward $0$ at the end of the episode \n",") <br>\n","\n","\n","<br><br>\n","\n","\n","state $S$ <br> (usable ace, sum, dealer) | $\\text{Returns}(S)$ | $V(S)$ | picture\n","--- | --- | --- | ---\n","$A = (\\text{No-ace}, 20, 10)$ | $\\text{Returns}(A) = [1]$ | $$1$$ | <img src=\"https://drive.google.com/uc?id=1DeDi4UdDpUBDGSa_0OhVMJYRgho52vYv\" alt=\"1-18\" width=\"400\">\n","$B = (\\text{No-ace}, 13, 10)$ | $\\text{Returns}(B) = [1]$ | $$1$$ | <img src=\"https://drive.google.com/uc?id=1MOMuHSdP2SDKi3CKHA1P6kyuC3ew_kvO\" alt=\"1-19\" width=\"400\">\n","\n","<br>\n","\n","Let's look at the __states__ starting __from the end__ of the episode, and <br>\n","__working backwords__ ! <br><br>\n","\n","state A $\\;\\; : \\;$ one step backward state <br>\n","$\\qquad \\qquad$ the card sum was 20 with no usable ace, and the dealer had a visible 10. <br>\n","$\\qquad \\qquad$ Add +1 ($G_0 = 1$) to the list of returns for state A. <br>\n","$\\qquad \\qquad$ Set the value of state A eqaul to the average of the list. <br><br>\n","\n","state B $\\;\\; : \\;$ two step backward state <br>\n","$\\qquad \\qquad$ the agent shows 13 with no usable ace, and the dealer has a visible 10. <br>\n","$\\qquad \\qquad$ Add +1 ($G_1 = 1$) to the list of returns for state B. <br>\n","$\\qquad \\qquad$ Set the value of state B equal to the average of the list. <br><br><br>\n","\n","\n","We've now processed every state in the first one episode, <br>\n","and completed the Monte Carlo updates for that first episode ! \n","\n","\n","<br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jg3wD0biPN01","colab_type":"text"},"source":["__Note__ &ensp; : Reason for using a list to returns (생각 정리) <br><br>\n","\n","In this example, <br>\n",">One episode has just one reward when the game ends. <br>\n",">So the reward for one episode directly becomes the return for that episode. <br><br>\n",">\n",">Every return for each episode are cumulated to the list of $Returns(s)$ to do average sum <br>\n",">to estimate the resulting reward for the specific one state. <br><br>\n",">\n",">In the dynamic programming method, <br>\n",">Doing check every return of every episode for one specific state <br><br>\n",">\n",">In the Monte Carlo method, <br>\n",">Doing check just some sapled returns of some episodes for one specific state !\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"eJ7mtkG0OErI","colab_type":"text"},"source":["__Multiple episodes :__ <br><br>\n","\n","\n","state-value function graph <br>\n","\n","<img src=\"https://drive.google.com/uc?id=1Sj4fAC_lnJhMeczezWvaGTSnLjD2rjaB\" alt=\"1-20\">\n","\n","<br>\n","\n","What would happen if the agent played __many games__ of blackjack ? <br>\n","( Sampling process ? ) <br><br>\n","\n","Let's look at the value function the agent learns after 10,000 episodes. <br>\n","We'll plot one value function for states with a usable ace, and one for states without a usable ace. <br>\n","The 3 axis of the plot are the card the dealer's showing, the agent's sum, and the value of that state. <br><br>\n","\n","  - The plot with usable aces is much more rough than the plot with no usable ace. <br>\n","  That's because most blackjack games do not have a usable ace so the usable ace values are estimated with far fewer samples. <br><br>\n","  \n","  - Both plots have a similar shape. <br>\n","  Looking at the values going a cross the plot, <br>\n","  It looks like the card dealer's showing doesn't impact the value function very much. <br>\n","  If we look at the agent's sum, the value function is much higher in tates where the agent has 20 or 21. <br><br>\n","\n","  Why are these values so much higher than when the agent has a sum of 19? <br>\n","  The answer has to do with the policy the agent is following. The policy always hits on sums of 19 or lower. So the agent is likely to bust when the sum is 19. On the other hand, when the sum is 20 or 21 then the agent will stick and is likely to win.\n","\n","<br>\n","\n","Now let's look at the value estimate after __500,000 episodes__ ! <br>\n","The estimates have __nearly converged__ to the __state values__. <br>\n","Notice the plots are much smoother now. <br><br>\n","\n","We see the same pattern as before, Where the values are low unless the sum is 20 or 21. <br>\n","\n","\n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4zH8hhe601Pa","colab_type":"text"},"source":["### Implications of Monte Carlo learning <br><br>\n","\n","\n","  1. We do not need to keep a __large model__ of the environment <br>\n","  Monte Carlo learns directly from experience. So there's __no__ need to keep a __large model__ of the __environment__. <br><br>\n","\n","  2. We are estimating the value of an individual state __independently__ of the value of other states <br>\n","  In __Dynamic progamming__, the value of each state __depends on__ the values of __other states__. (this is pretty big difference!) <br><br>\n","\n","  3. The __computation__ needed to __update__ the value of each state (along the way) does __not depend__ (in any way) on the __size of the MDP__ <br>\n","  Rather, it __depends__ on the __length of the episode__ !\n","\n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1NSVQKG227o7","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - We showed how to use __Monte Carlo prediction__ to learn the value function of a policy <br><br>\n","\n","  - We talked about how Monte Carlo learning is __computationally efficient__ <br>\n","  $\\rightarrow \\quad$ Monte Carlo learning __doesn't__ need to __sweep__ over the __whole MDP__ ! \n","\n","\n","<br><br><br><br><br>\n","\n","\n"]}]}
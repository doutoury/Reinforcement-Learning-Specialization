{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2. Week_2-2_Monte Carlo for Control & Exploration methods for Monte Carlo","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyM/cYclminvUbrMPIFrnUBf"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"39ARKswlLNZg","colab_type":"text"},"source":["## __2. Monte Carlo for Control__ <br><br>\n","\n","  - Using Monte Carlo for action values <br><br>\n","\n","  - Using Monte Carlo methods for generalized policy iteration\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"F_a_K3Mb5Klq","colab_type":"text"},"source":["## $\\cdot$ Using Monte Carlo for action values <br><br>\n","\n","\n","  - Estimate __action-value function__ using __Monte Carlo__ <br><br>\n","\n","  - Understand the importance of __maintaining exploration__ in Monte Carlo algoritms \n","\n","<br><br>\n","\n","\n","So far, we've talked about using Monte Carlo to learn state-value functions ($v$) __for a fixed policy__ (?). <br>\n","This time we'll focus on learning action-values ($q$)\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HXQJbGrY_Qyx","colab_type":"text"},"source":["### __Monte-Carlo methods for Policy Improvement__ <br><br>\n","\n","$\\Rightarrow \\quad$ __Action-values__ are useful __for learning a policy__ <br>\n","$\\Rightarrow \\quad$ Evaluate action-value with MC, then Improve policy with greedication <br><br><br>\n","\n","\n","\n","#### __\" Why do we care about learning action values at all ? \"__ <br>\n","$\\;$ Action values are useful for learning a policy ! <br><br>\n","\n","$\\rightarrow \\quad$ They allow us to compare different actions in the same state. <br>\n","$\\qquad$ Then we can swtich to a better action if one is available. ( $\\rightarrow \\;$ learning policy ) <br>\n","$\\qquad$ This is only possible if we have an estimate of the values of the other actions. <br><br>\n","\n","> $\\quad$ But this can be tricky. \n","\n","\n","<br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1jL5--cx6okW","colab_type":"text"},"source":["### Monte Carlo methods in R.L. <br><br>\n","\n","\n","Learning action values is almost exactly the same process as learning state values. <br><br>\n","\n","\n","state-value | action-value\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1DbN7Z6lJxasKEPLruSIlWYIJ01IU4drm\" alt=\"2-01\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1rthABX3-Bgxi0cpTViaj7T-hZgo-f9HI\" alt=\"2-02\" width=\"500\">\n","\n","<br>\n","\n","Recall that we learned the value of a state __by averaging sample returns__ from that state. <br>\n","The same process works for action values. <br><br>\n","\n","$\\rightarrow \\quad$ We collect __returns__ following a policy from __state-action pair__, and then take their __average__.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XIs3U1rzAzJ_","colab_type":"text"},"source":["### __Exploration__ for accurate __Monte Carlo estimation__ <br>\n","#### \" There are __Exploration problems__ \" <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1RDb8wr0S0gam_zaf_6zrcGv4V4mFZVk8\" alt=\"2-03\">\n","\n","<br>\n","\n","Let's consider <br>\n","$\\qquad$ learning the action-value function for a __deterministic policy__(?). ( 정해진 것만 고르는 놈 )<br><br>\n","\n","Imagine (on) <br>\n","$\\qquad$ an __action__ that is __never selected__ by that policy. <br>\n","$\\qquad$ The agent will __never observe__ __returns__ corresponding to __that action__. <br>\n","$\\qquad$ We won't be able to form an accurate Monte-Carlo estimate. <br><br>\n","\n","\n","$\\rightarrow \\quad$ The agent __must__ try __all the actions__ in aech state in order to learn their values. <br>\n","$\\qquad$ This is the problem of maintaining __exploration__ in reinforcement learning. <br><br><br>\n","\n","\n",">Let's think about a real-world example. <br>\n",">Imagine walking home along the road you usually take. Recently a new road was built nearby. <br>\n",">If we don't ever try the new way, then we couldn't know if it was actually better. \n","\n","\n","<br><br><br>\n"]},{"cell_type":"markdown","metadata":{"id":"pD0ipDGTqeBu","colab_type":"text"},"source":["### Monte-Carlo 사용 시 Exploration 문제 발생 <br><br>\n","\n","모든 state 에 모든 value 구해가면서 update 하는 prediction 과 control 과정을 거치면 optimal policy 는 언젠가 나온다 ! <br><br>\n","\n","근데 현실에서의 문제는 그 양이 너무 많아서 불가능하기 때문에 그 대안으로 sampling 통한 optimal policy 찾기를 한다 ! <br><br>\n","\n","근데 sampling 잘못 하면 optimal 이 아닌 값을 최적값이라 생각하고 prediction & control 해갈 수 있으니까 '가능한 모든 starts state 에 대해 radom sampling 해서 편향 가능성을 막음' (?) <br><br>\n","\n","근데 Exploring starts 는 가능한 starts state 전체에 대해 시도해 볼 수 있는 상황에서만 가능한 방법 ! 역시 현실에서 불가능한 경우가 많다. <br><br>\n","\n","따라서 그 대안으로 Epsilon-soft policy를 통해서, sub-optimal 이지만 stochstic policy 를 통한 prediction & control 통해서 sub-optimal policy 찾기를 한다 ! 어짜피 확률적으로(stochastic) 접근하기 때문에 random start state 를 통해서 시작점 $\\rightarrow$ 골인점 사이의 exploration 업데이트를 할 필요가 없다 ! <br><br>\n","\n","왜 Epsilon-soft policy 는 sub-optimal policy 밖에 못 찾을까 ? 태생적으로 각 state 에서 deterministic 하게 최고 action 만 고르는 것이 아니고, 확률적으로 안 최고인 actio 도 고르기 때문에 ! deterministic 하게 optimal value 만드는 action 만 취하는 prediction & control 보다 sub-optimal 할 가능성이 항상 존재한다 ! <br><br>\n","\n","근데 sub-optimal 말고 Optimal policy 를 찾고 싶다 ! <br>\n","2. Week_1-3 에서 배울 Off-policy 를 통해 Behavior policy 와 Target policy 를 나누어, stochstic action choice 를 통한 sampling ~ ... 2. Week_1-3 마지막 강의 보고 마저 필기 ...\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lrhK7cxBaVGx","colab_type":"text"},"source":["### Deterministic policy v.s. Non-deterministic policy <br><br>\n","\n","\n","#### Note : Additional reference <br>\n","A policy can be either deterministic or stochastic (non-deterministic). <br><br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1ClD8vMDM1FCT4TMe3ahKppEJ7hW-RHDa\" alt=\"2-Deterministic_policy\" width=\"500\">\n","\n","<br><br>\n","\n","__Deterministic policy__ <br>\n",">A deterministic policy is policy that maps state to actions. You give it a state and the function returns an action to take. <br>\n",">Deterministic policies are used in deterministic environments. These are environments where the actions taken determine the outcome. There is no uncertainty. For instance, when you play chess and you move your pawn from A2 to A3, you’re sure that your pawn will move to A3.\n","\n","<br><br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1DpGN1T3pJr5lx2i362dgVDU9M1kWdbOB\" alt=\"2-Non-deterministic_policy\" width=\"500\">\n","\n","<br><br>\n","\n","__Non-deterministic policy__ <br>\n",">On the other hand, a stochastic policy outputs a probability distribution over actions. <br>\n",">It means that instead of being sure of taking action a (for instance left), there is a probability we’ll take a different one (in this case 30% that we take south). <br>\n",">The stochastic policy is used when the environment is uncertain. We call this process a Partially Observable Markov Decision Process (POMDP). <br>\n",">Most of the time we’ll use this second type of policy.\n","\n","\n","<br><br>\n","\n","\n","__source__ <br>\n","\n","https://www.freecodecamp.org/news/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f/\n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"7rnMGoA-EfsL","colab_type":"text"},"source":["### Maintain Exploration method &ensp; ( for MC ) <br><br>\n","\n","  1. Exploring starts <br><br>\n","\n","  2. Epsilon - soft policy <br>\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EQEtbM01DEwR","colab_type":"text"},"source":["### Maintain exploration method &ensp; __1. Exploring starts__ <br><br>\n","\n","\n","One way to maintain exploration $\\quad : \\;$ Exploring starts <br>\n","\n","<img src=\"https://drive.google.com/uc?id=172tENRfBEeBtUi8wx2oxBMGGmJtQcrGj\" alt=\"2-04\" width=\"500\">\n","\n","In exploring starts, <br>\n","we must __guarantee__ that episodes __start__ in __every state-action pair__. <br>\n","Afterwards, the agent simply follows it's policy. \n","\n","<br><br><br>\n","\n","\n","\n","__Example__ <br><br>\n","\n","\n","policy | exploring start sample | one episode termination\n","--- | --- | ---\n","<img src=\"https://drive.google.com/uc?id=1CqaOKxXPyhTtrxjmZDa7L6pb9KEiI71-\" alt=\"2-05\" width=\"400\"> | <img src=\"https://drive.google.com/uc?id=1BxfeaEa2L7_7aRBGFxmZ4kQ9WJY2UMlO\" alt=\"2-06\" width=\"400\"> | <img src=\"https://drive.google.com/uc?id=1yYibuspCbi2VFHAfmJ5SPP6r84fNSeTZ\" alt=\"2-07\" width=\"400\">\n","\n","\n","<br><br>\n","\n","Consider the grid world. <br>\n","Let's see how __Exploring starts__ works with a policy shown in red (arrows). <br><br>\n","\n","Exploring starts will __randomly sample__ a state and action __at the start__ of an __episode__ shown in blue arrow. <br>\n","(In this case, the blue action is not the same as the red one) <br><br>\n","\n","After the initial action, the agent will follow the red policy until the episode ends. <br>\n","We must be able to set the start state in this way to evaluate a __deterministic policy__. <br>\n","(like we can in this grid world) <br><br><br>\n","\n","\n",">This may not be always be possible. <br>\n",">__Other exploration strategies__ like __epsilon-greedy__ can be used to evaluate __stochastic policies__. <br>\n",">(We'll talk more about the later, on the part 3)\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3Tzqn_02IOOR","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","   - We described a Monte Carlo algorithm for estimating __action-values__ <br><br>\n","\n","   - Discussed the importance of maintaining __exploration__\n","\n","\n","\n","<br><br><br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RATfoicXIz4X","colab_type":"text"},"source":["## $\\cdot$ Using __Monte Carlo__ methods for __Generalized policy iteration__ <br><br>\n","\n","  - Understand how to use Monte-Carlo methods to implement a __Generlized Policy Iteration__ algorithm \n","\n","\n","<br><br><br>\n","\n","\n","Now that we can estimate action values using Monte Carlo, <br>\n","the next step is to build a generalized policy iteration algorithm <br>\n","\n",">GPI algorithm $\\quad : \\quad$ Generalized Policy Iteration algorithm\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Ajr6TXUWKEZ1","colab_type":"text"},"source":["### Monte-Carlo Generalized Policy Iteration <br><br>\n","\n","Iteration | Convergence\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1uglTt2DC_ACQEUZKO2ex9xsL0SjRzxYf\" alt=\"2-08\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=18t0jRzTicIVOZ2Bd3Ni0ac0IPMR3PUW1\" alt=\"2-09\" width=\"500\">\n","\n","<br>\n","\n","\n","GPI includes a policy evaluation and a policy improvement step. <br>\n","GPI algorithms produce sequences of policies that are at least as good as the policies before them. <br><br>\n","\n","  - For the policy improvement step, <br>\n","  We can make the __policy greedy__ with respect to the agent's current action value estimates. <br>\n","  ($\\epsilon \\text{ - greedy}$ 와는 다르다 ?) <br><br>\n","\n","  - For the policy evaluation step, <br>\n","  We'll use a Monte Carlo method to estimate the action values. <br><br><br>\n","\n","\n",">Remember that in the GPI framework, the value estimates need only improve a little, not all the way to the current action values. All that is required for convergence is that the estimates continue to improve.\n","\n",">Monte Carlo control methods combined policy improvement and policy evaluation on an episode-by-episode basis. \n","\n","<br><br><br>\n","\n","\n","### Monte-Carlo Exploring Starts __algorithm__ <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1q6XUWz3fMdtfobvQHbwL3b_bKRUpiSa-\" alt=\"2-10\">\n","\n","\n","__Generalized policy iteration + Exploring starts__ <br>\n","Let's put these two together in one algorithm ! <br><br>\n","\n","Let's start with our Monte Carlo method for learning action values. <br><br>\n","\n","You use exploring starts so that each episode begins with the randomly selected sate and action. <br>\n","Then, the agent generates an episode by following his policy, keeping track of the states, actions, and rewards along the way. <br><br>\n","\n","Once the episode is complete, it computes each return starting from the end of the episode. <br>\n","Then, it adds the return to appropriate list. <br><br>\n","\n","The list of returns are then averaged to update the action value estimates for each state-action pair. <br>\n","This completes the policy evaluation step. <br><br>\n","\n","After policy evaluation, it's time to do policy improvement. <br>\n","We simply updaate the policy to take the greedy action with repect to our updated action values. <br><br>\n","\n","We do this in every state observed during the episode.\n","\n","<br><br><br>\n","\n","Isn't this so __much easier than__ the way we did the step in __Dynamic programming__ ? That's it ! <br>\n","This is the __Generalized Policy Iteration__ algorithm, <br>\n","__Monte Carlo__ with __exploring starts__.\n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9ArkbJr4XvSb","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - We described how to use Monte-Carlo methods to create __GPI__ algorithms <br><br>\n","\n","  - We showed an example algorithm, __Monte Carlo__ with __Exploring Starts__\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hFGIDrEiYQT9","colab_type":"text"},"source":["## $\\cdot$ Solving the Blackjack example <br><br>\n","\n","\n","  - Apply __Monte Carlo__ with __Exploring Starts__ to solve an example __MDP__\n","\n","<br><br><br>\n","\n","\n","Now that we have a Generalized Policy Iteration algorithm for Monte Carlo control. <br>\n","Let's use it in an example and see how it works.\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DLc-hfzHYdm4","colab_type":"text"},"source":["### Apply __Monte Carlo__ with __Exploring Starts__ <br><br>\n","\n","### Example <br><br>\n","\n","\n","We will train our agent to play blackjack using Monte-Carlo exploring starts. <br>\n","Exploring starts requires that episodes begin with a random state and action. <br><br>\n","\n","\n","Luckily for us, our version of blackjack naturally starts in random states. <br>\n","Then, all we have to do is to randomly select the first action in each episode. That is the agent ignores what it thinks is the best action in the first state, and randomly choose to hit or stick. <br><br>\n","\n","The initial policy is hit when the agent's sum is less than 20, and stick when the sum is 20 or 21.\n","\n","<br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UH_uEZL0fn29","colab_type":"text"},"source":["#### __One episode__ <br><br>\n","\n","\n","One episode | first state | second state\n","--- | --- | ---\n","agent | <img src=\"https://drive.google.com/uc?id=1zCFOk0wb5iFVEYUYuA00UIwIF5m7pH8J\" alt=\"2-11\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1GfC7PSEtGQQ3yBOebzk_dIwayHAZcnfs\" alt=\"2-12\" width=\"500\">\n","dealer | <img src=\"https://drive.google.com/uc?id=1aaz14SnMFwjafXgESUO5_CRu2TIVNFDH\" alt=\"2-13\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=14neQnPprgdYEKddDLYeu1WHZpgaOBaga\" alt=\"2-14\" width=\"500\">\n","\n","<br>\n","\n","First state. <br>\n","Suppose the agent's cards show a total of 13 with no usable ace and the dealer's visible card is an 8. <br>\n","According to the randomly sampled first action, the agent hits. <br><br>\n","\n","\n","Second state. <br>\n","The agent gets a 7 moving the sum to 20. <br>\n","On the next step, the agent chooses the action according to it's policy. So it sticks. <br><br>\n","\n","\n","\n","Now, it's dealer's turn. <br>\n","The dealer draws a 9 and goes over 21 losing the game <br>\n","and resulting in $R = +1$ reward for the agent. \n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Fz4GVgUMfZuh","colab_type":"text"},"source":["#### __Backward process__ <br><br>\n","\n","\n","Now let's look at the __state-action pairs__ we saw during the game, <br>\n","starting __from the end__ of the episode and __working backwards__. <br><br>\n","\n","\n","state | state-action pairs & policy\n","--- | ---\n","one step prior state | <img src=\"https://drive.google.com/uc?id=1V7mO5seLBQoMvth4bRv96b3ymnS2ocOn\" alt=\"2-15\" width=\"500\">\n","two step prior state | <img src=\"https://drive.google.com/uc?id=16sFodscQU7DsIvZxV2cBz0hhKpFxPWoi\" alt=\"2-16\" width=\"500\">\n","\n","<br>\n","\n","1. One step prior state <br>\n","In the last non-terminal state, <br><br>\n","\n","$\\qquad \\text{State } X \\qquad \\quad : \\;$ the agent had a sum of 20, no usable ace, and the dealer had an 8 <br><br>\n","\n","$\\qquad \\text{Returns}(S,A) \\; : \\;$ From that state, the agent choose to stick $\\qquad ...$ by deterministic policy (?) <br>\n","$\\qquad \\qquad \\qquad \\qquad \\quad \\;$ The agent +1 to it's list of returns corresponding to that state-action pair <br>\n","$\\qquad Q(S,A) \\qquad \\quad : \\;$ The estimated value for the action stick in this state is simply 1. <br><br>\n","\n","\n","\n","$\\qquad$ Let's look at the value of the two actions in this state. <br>\n","$\\qquad$ The agent never tried to hit action, so it's value is 0. <br>\n","$\\qquad$ The value of stick action is 1. We just computed that. <br><br>\n","\n","$\\qquad \\rightarrow \\quad$ So the greedy action with respect to Q, in this state, is to stick. <br>\n","$\\qquad \\quad \\quad$ ( It has the highest edtimated value )\n","\n","<br><br><br>\n","\n","\n","\n","\n","2. Two step prior state <br>\n","In the second last non-terminal state, <br>\n","(Let's move back in time one more step to the start state) <br><br>\n","\n","$\\qquad \\text{State } Y \\qquad \\quad : \\;$ the agent had a sum of 13, no usable ace, and the dealer had an 8 <br><br>\n","\n","$\\qquad \\text{Returns}(S,A) \\; : \\;$ The randomly choosen action was to hit. $\\qquad ...$ by deterministic policy (?) <br>\n","$\\qquad \\qquad \\qquad \\qquad \\quad \\;$ The agent +1 to it's list of returns following that state-action pair <br>\n","$\\qquad \\qquad \\qquad \\qquad \\quad \\;$ The average of the list forms the estimate of the action value <br>\n","$\\qquad Q(S,A) \\qquad \\quad : \\;$ The estimated value for the action stick in this state is simply 1. <br><br>\n","\n","$\\qquad$ Finally the policy is updated in this state to be greedy to the action value estimates. <br>\n","$\\qquad$ In this state, we had never tried to stick action and it's value is 0. <br>\n","$\\qquad$ But the hit action resulted in a return of 1. <br><br>\n","\n","$\\qquad \\rightarrow \\quad$ So the __greedy action__ is to hit, <br>\n","$\\qquad \\qquad$ and the policy is updated to reflect this. \n","\n","<br><br><br>\n","\n","\n","#### __Repeat over many episodes__ <br><br>\n","\n","We then __repeat__ this process over __many episodes__. <br>\n","The action values and the policy will __approach__ their __optimal values__. \n","\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ijWP9loognQ_","colab_type":"text"},"source":["#### __Optimal policy__ <br>\n","\n","<img src=\"https://drive.google.com/uc?id=1WaLAPBm4SDl3jcr0RqZZUZn2suBGQUAW\" alt=\"2-17\" width=\"500\"> \n","\n","<br>\n","\n","Let's look at the __optimal policy__ <br>\n","that the agent found after we ran it for a __really long time__. <br><br>\n","\n","\n","Notice how the __agent__ plays __depending__ on the __usable ace__. <br>\n","For most dealer cards, agent hits until it has the sum near 19. <br><br><br>\n","\n","\n","With a usable ace, &ensp;( left-side picture ) <br><br>\n","$\\qquad$ the agent hasd a lot of flexibility in calculating the sum of it's cards. <br>\n","$\\rightarrow \\quad$ So the __policy__ is much more __aggressive__. <br><br><br>\n","\n","\n","Without a usable ase, &ensp;( right-side picture ) <br><br>\n","\n","$\\qquad$ the policy depends a lot more on the cards the dealer is showing. <br>\n","$\\qquad$ the agent sticks when it's sum is 13 or greater and the dealer has a low card, like a 2 or 3. <br>\n","$\\rightarrow \\quad$ This is __optimal__ even though it __seems__ like the chance of winning will __be low__ in this situation. \n","\n","\n","<br><br><br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_SGXJng7moAB","colab_type":"text"},"source":["### Summary <br><br>\n","\n","  - We showed how to apply __Monte Carlo__ with __Exploring starts__ to solve an example __MDP__ \n","\n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZMphBnyPnhmm"},"source":["## __3. Exploration Methods for Monte Carlo__ <br><br>\n","\n","  - Epsilon-soft policies\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ifbSPEj6nhmn"},"source":["## $\\cdot$ Epsilon-soft policies <br><br>\n","\n","\n","  - Understand why __Exploring starts__ can be __problematic__ in __real problems__ <br><br>\n","\n","  - Describe an alternative method (__Epsilon-soft policies__) to maintain __exploration__ in __Monte Carlo control__ <br>\n","  ( Limit to only deterministic policy ??? )\n","\n","<br><br>\n","\n",">We've hinted how the assumptions underlying __Exploring starts__ might __limit__ the applicability of __Monte Carlo control__. <br><br>\n",">\n",">In this video, we descirbe how to combine __Epsilon greedy expiration__ with __Monte Carlo__ to learn near __optimal policies__. \n","\n","<br><br>\n","\n","#### 내 생각\n",">Exploring starts 에서 <br>\n",">Deterministic 한 policy 를 통해 value function 을 업데이트 해갸며 optimal policy 를 찾아가며 '문제'를 해결하던 것을 <br>\n",">Epsilon-soft policy 는 <br>\n",">Stochastic 한 policy 를 통해 value function 을 업데이트 해가여 sub-optimal policy 를 찾아가며 '문제'를 해결하게 해준다 !\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"p030odhYpy9q","colab_type":"text"},"source":["### Problem of Exploring starts (?) <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1Wj9PlUjufLw516GwKbfURUe4XifL6O9e\" alt=\"3-01\" width=\"500\">\n","\n","<br>\n","\n","\n","Some situations <br>\n","where we can __not__ use __Exploring starts__. <br><br>\n","\n","$... \\quad$ This algoritm (Exploring starts) __must__ be able to __start__ from __every possible state-action pair__. <br>\n","$... \\quad$ Otherwise, the agent may __not__ explore __enough__, and could converge to a __suboptimal__ solution. <br><br>\n","\n","In many problems, <br>\n","it can be __difficult__ to __randomly sample__ an __initial state-action pair__. <br>\n",">For example, how would you randomly sample the initial state-action pair for a self-driving car? How could we ensure the agent can start in all possible states? We would need to put the car in many different configurations in the middle of a busy freeway ... <br>\n","This would be dangerous and impractical.\n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"x4jSyTo5sPaF","colab_type":"text"},"source":["### Epsilon-greedy Exploration <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1fUHZVMVdJNN8qwgtfPHIc0jLVJKKpMYZ\" alt=\"3-02\" width=\"500\">\n","\n","<br>\n","\n","\" How can we learn __all__ the __action values__ without Exploring starts ? \" <br><br>\n","\n","__Epsilon-greedy Exploration__ ! <br>\n",">We discussed Epsilon-greedy Exploration, <br>\n","__Simple__ but __effective__ method in the Bandit lectures. <br>\n",">We can use it with __Monte Carlo__ too ! \n","\n","<br><br><br>\n","\n","\n","\n","#### __Epsilon-greedy polocy v.s. Epsilon-soft policy__ <br><br>\n","\n","\n","$\\qquad \\; \\epsilon \\text{ - greedy policies} \\quad \\subset \\quad \\epsilon \\text{ - soft policies}$ <br><br>\n","\n","\n",">$\\quad$ Epsilon-greedy policies are a subset of Epsilon-soft policices <br>\n",">$\\quad$ ( A subset of a larger class of policies )\n","\n","<br><br>\n","\n","\n","\n","  - __Epsilon-greedy policies__ <br>\n","  $\\qquad$ are __stochastic policies__. ( recap )<br>\n","  $\\qquad$ They usually take the __greedy action__, but occasionally take a __random action__. <br><br>\n","\n","\n","  - __Epsilon-soft policies__ are __stochastic policies__ too (???) <br>\n","  $\\qquad$ They take each action __with probability__ at least epsilon over the number of actions $\\frac{\\epsilon}{|\\; \\mathbb{A} \\;|}$ . <br><br>\n","\n","\n",">For example, <br>\n",">Both policies shown on the picture are valid Epsilon-soft policies. <br>\n",">The uniform random policy is another notable Epsilon-soft policy. \n","\n","<br><br><br>\n","\n","\n","\n","\n","#### __Property of Epsilon-greedy policies__ <br><br>\n","\n","\n","  - Epsilon-soft policies force the __agent__ to __continually explore__. <br><br>\n","  \n","  An Epsilon-soft policy assigns __non-zero probability__ to each action in __every state__. <br>\n","Because of this, Epsilon-soft agents continue to visit __all state-action pairs indefinitely__. <br>\n","( that means we can drop the Exploring starts requirement from the Monte Carlo algorithm )\n","<br><br><br>\n","\n","\n","\n","  - __Epsilon-soft policies__ are __always stochastic__. <br>\n","\n","  <img src=\"https://drive.google.com/uc?id=1gESL8MewWCtI_rNX3gMFGdBlt5mJqQtN\" alt=\"3-03\" width=\"500\"> \n","  \n","    - __Deterministic policy__ specify a single action to take in each state <br><br>\n","\n","    - __Stochastic policies__ instead specify the probability of taking action in each state. <br><br>\n","    \n","      In Epsilon-soft policies <br>\n","    __all actions__ have a __probability__ <br>\n","    of at least Epsilon over the number of actions $\\frac{\\epsilon}{| \\ \\mathbb{A} \\ |}$ . <br><br>\n","\n","    $\\rightarrow \\quad$ They will eventually __try all the actions__. \n","\n","\n","<br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rIyeBqUclmSM","colab_type":"text"},"source":["### Example : $\\epsilon$ - greedy policy <br><br>\n","\n","Grid world example <br>\n","\n","<img src=\"https://drive.google.com/uc?id=1wPq6pb0WDBUim8Bp4sxNTJfsO93_M3MO\" alt=\"3-04\" width=\"500\">\n","\n","<br>\n","\n","Left grid-world's arrows are representing the __deterministic policy__. <br><br>\n","\n","$\\qquad$ From the starts state, the agent will follow the exact same trajectory through the grid world. <br><br><br>\n","\n","\n","Right grid-world's arrows are representing __Epsilon-greedy policy__. <br><br>\n","\n","$\\qquad$ There are more arrows because every action has some small probability of being selected. <br>\n","$\\qquad$ Accordingly, the agent will probably follow a slightly different trajectory every episode. <br>\n","$\\qquad$ After enough episodes, It will taken every action at least once in every state. <br><br><br>\n","\n","\n","\n","This difference extends the solutions that we find <br>\n","by exploring with Epsilon-soft policies. <br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1XWmqUntsVb5FSARiOQHaJrptliYgm1i4\" alt=\"3-06\" width=\"500\">\n","\n","<img src=\"https://drive.google.com/uc?id=1dleakIgcaBN33dEVPpOh9mPZDLzZ8Xu8\" alt=\"3-05\" width=\"500\">\n","\n","<br>\n","\n","\n","\n","If our policy always gives at least epsilon-probability to each action, <br>\n","it's impossible to converge to a deterministic optimall policy. <br><br>\n","\n","Exploring starts can be used to find the optimal policy. <br>\n","But Epsilon-soft policies can only be used to find the optimal Epsilon-soft policy. <br>\n","(That is the policy with the highest value in each state out of all the Epsilon-soft policies) <br><br><br>\n","\n","\n","\n","  - In general, <br>\n","  The optimal Epsilon-soft policy worse than the optimal policy ! <br><br>\n","\n","  - However, <br>\n","  It often performs reasonably well, and allows us to get rid of Exploring starts.\n","\n","\n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JIMsUaiVw718","colab_type":"text"},"source":["### Pseudo Code <br><br>\n","\n","Pseudo code for __Monte-Carlo control__ <br>\n","with __Epsilon-soft policies__ <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1bsVtukpldk-suxqtxVqbUKSrITJ_8BQy\" alt=\"3-07\">\n","\n","<br>\n","\n","\n","#### __Difference from Exploring starts__ <br>\n","( red boxed area ) <br><br>\n","\n","  1. In the initial conditions <br>\n","  The __initial policy__ must be __Epsilon-soft__ <br>\n","  ( such as the Uniform random policy ) <br>\n","  ( 처음만 $\\epsilon$-soft policy 고 후에는 업데이트된 $\\epsilon$ greedy policy 따르나 ? ) <br><br>\n","\n","  2. In the policy evaluation step <br>\n","  It is about __\" how we use the policy to generate training data \"__. <br>\n","  Since the agent's policy is Epsilon-soft, we don't have to rely on exploring starts for exploration. <br>\n","  Instead, the agent simply generates an episode following it's Epsilon-soft policy. <br><br>\n","\n","  3. In the policy improvement step <br>\n","  The new policy (finally improved) is __Epsilon-greedy__ with respect to the current action value estimates. <br>\n","  >You might think that this should be an Epsilon-soft policy rather than Epsilon-greedy policy. However, this makes sense because the optimal Epsilon-soft policy is an Epsilon-greedy policy ! \n","\n","\n","<br><br>\n","\n","\n","Even though this algorithm may not find the optimal policy, <br>\n","it does find the optimal Epsilon-soft policy (Sub-optimal ?) <br><br>\n","\n","\n",">In the future videos, <br>\n",">We will discuss how to __learn__ the __optimal policy__ using a different method called __Q learning__ !\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"36LE0hX6z-dB","colab_type":"text"},"source":["### Summary <br><br>\n","\n","  - We talked about how __sampling__ an __initial state-action pair__ <br>\n","  ( for Exploring starts is now always feasible (practical) ) <br><br>\n","\n","  - We disccussed __Monte-Carlo control__ with __$\\epsilon$-soft policies__\n","\n","\n","\n","<br><br><br><br><br>\n","\n"]}]}
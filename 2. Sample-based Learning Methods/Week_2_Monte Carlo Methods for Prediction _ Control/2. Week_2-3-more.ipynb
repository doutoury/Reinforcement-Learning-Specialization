{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2. Week_2-3-more","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyN/kKcBAIpKlgGJ9Qxx0ch9"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"09hPncuVnCQF","colab_type":"text"},"source":["## Batch Reinforcement Learning &ensp; ( by Emma Brunskill ) <br><br>\n","\n","\n","~ <br><br>\n","\n","Batch reinforcement learning <br><br>\n","\n","#### Breif motivation <br><br>\n","\n","the tale of two hamburgers <br>\n",">1/4 lb burger v.s. 1/3 lb burger <br>\n",">The price of 1/3 is similar to 1/4 , even though <br>\n",">the weight of 1/3 is better thant 1/4 <br>\n",">$\\rightarrow \\quad$ people thought that they were getting less beif, because 3 is less than 4.\n","\n","<br>\n","\n","Refraction game <br>\n","challange $\\quad : \\;$ What is the sequence of activities student should do to keep them engaged and motivated and make sure they were learning. <br><br>\n","\n","for example, we can think of this as Decision Policy, where based on the sequence of interaction with the game that student has so far, what is the next activity to give them in order to maximize somehow come we care about. <br>\n","This reward function could be something like persistence or the amount that they've learned. <br><br>\n","\n","In particular in our case, <br>\n","these are optional learning environments. So we are really interested in trying to maximize student persistence. we started thinking about this about five years ago, and it made me really interested in the problem. I'm gonna tell you about breiflly today. <br><br>\n","\n","\n","But I wanana highlight here about doing reinforcement learning for applications that could hopefully benefit society. <br><br>\n","\n","Really since the very start of reinfocement learing, which we can trace back to thing like multi-armed bandit, as you've heard about recently from the 1940s. There's been a parallel legacy of thinking about reinforcement learning to try to benefit humans. starting withi things like teaching machines and then moving on to things like adaptive clinical trials. <br>\n","Now I'm think this is both really exciting in terms of it's societal impact, but it also introduces a lot of foundational questions or reinforcement learning. <br><br>\n","\n","In particular, if we think about some of the extraordinary successes in reinforcement learning right now, they have tended to be in areas like robotics or game-palying, and in thse scenarios we typically have access to a simulator, where we can try things out and even if our agent fails to learn for a long time, as long as it learns eventually, we're happy. <br><br>\n","\n","In contrast, if we think about doains that involve interacting with people, we unfortunately do not get to have great simulator of how people learn or behave, which means that we have to rely on real data. But, this is challenging, because that real data involves interacting real humans which means that we need to be caraful about the data we collect. <br><br><br>\n","\n","\n","[ 03:30 ] <br>\n","\n","One of the key question that I think about and together with my student and my lab is how do we develop reinforcement learning techniques to try to minimize the amount of data an agent would need to learn, and what ara the fundamental limits of how much informatoin any agent would need to learn to make good decisions. \n","\n","<br><br>\n","\n","Basic MDP <br>\n","~ <br><br>\n","\n","Counterfactual case <br>\n","\n","과겨의 경험적 데이터로부터 미래를 예측하는 일이 가능한가? 더욱이 그렇게 작동하는 general 한 모델을 만들 수 있는가 ? <br><br>\n","\n","-> 다른 여러 분야에서 casual reasoning (infference) or counterfactual reasoning (infference) 등의 이름으로 많이 접근되었다. <br><br>\n","\n","이를 머신러닝 method 와 결합하려 시도하였고 그 결과로 Counterfactual R.L. or Batch R.L. 등장. <br><br>\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"D1hYc3KnXWR8","colab_type":"text"},"source":["#### Pros and Cons of Importance Sampling <br><br>\n","\n","\n","There are some challenges with using Importance Sampling to do policy evaluation. <br><br>\n","\n","Importance Sampling provides us with an __unbiased estimator__, <br>\n","but it typically can be __very high variance__. <br>\n","( the variance can scale sometimes exponentially with the horizon ) <br><br>\n","\n","That means if we don't have a lot of data or the horizon is long, <br>\n","these estimators may be very poor approximators of how good our new policy might do in the future. \n","\n","<br><br><br>\n","\n","\n","\n","#### Alternative to Importance Sampling's high variance <br><br>\n","\n","An alternaative is to use things like parametric models which you've heard about a little bit in terms of the MDP planning sections. <br><br>\n","\n","These models can have bias <br> \n","because sometimes the function approximators we're using or the model specification we've chosen is not correct. <br>\n","It doesn't match really how the role generates data. <br><br>\n","\n","But they can have very low variance in return for some bias. \n","\n","\n","<br><br><br>\n","\n","\n","\n","#### ~ <br><br>\n","\n","\n","One of the exciting innovations <br>\n","the idea of doubly robusst estimators, <br>\n","An idea that comes from statistics but has been ported over to R.L. (abundance?) for the last 10 years <br><br>\n","\n","The idea is that we can combine bettween the best of these two worlds. Parametric models of dynamics, rewards or values fit to data and Importance Sampling (with) correct mismatch of state-action distribution. Combining model-based estimators with Importance Sampling in order to get lower variance and lower bias. \n","\n","\n","<br><br><br>\n","\n","\n","#### Off-policy policy gradient with State distribution correction <br><br>\n","\n","~ \n","\n","\n","<brr><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"g829Dk8eq9i_","colab_type":"text"},"source":["## __Week 1 Summary__ <br><br>\n","\n","\n","This week was all about estimating value functions and finding optimal policy using only experience from the environment. <br><br>\n","\n","We saw On-policy and Off-policy Monte Carlo, <br>\n","and revisited the exploration problem from bandits. <br><br>\n","\n","( Let's review the materials so far ) <br><br><br>\n","\n","\n","#### Monte-Carlo algorithms <br><br>\n","\n","It is sample-based method ! <br>\n","It can be used when the model is unavailable or hard to write down. <br>\n","Monte-Carlo algorithms estimate value functions by averaging over multiple observed returns. <br>\n","They wait for the full return before updating their values. <br>\n","Therefore, we use Monte-Carlo only for episodic MDPs. \n","\n","<br><br><br>\n","\n","\n","#### Monte-Carlo ES (exploring starts) <br><br>\n","\n","We discussed how Monte-Carlo can be used inside generalized policy iteration. <br>\n","This led to our first sample-based control algorithm, Monte-Carlo with Exploring Starts. <br><br>\n","\n","Monte-Carlo algorithms don't do sweeps over the state-action space lile dynamic programming, <br>\n","So they need an exploration mechanism to ensure that they learn about every state-action pair. <br><br>\n","\n","First, we considered exploring starts. <br>\n","Exporing Starts requires that the first state and action are chosen eandomly at the beginning of every epsidoe. <br>\n","It is not always feasible or safe to use exploring starts. <br>\n",">( Just imagine trying to do exaploring starts with an autonomous car ) <br>\n",">This realization motivated us to investigate additional exploration methods. \n","\n","<br><br>\n","\n","\n","#### Exploration Techniques <br><br>\n","\n","Epsilon-soft policies / Off-policy <br><br>\n","\n","\n","Epsilon-soft policies <br><br>\n","\n","\n","We cover two other strategies for the exploration problem. <br>\n","Learning On-policy with Epsilon-soft policies and learning Off-policy. <br><br>\n","\n","For the first strategy &ensp; (On-policy learning + E-soft policies) <br>\n","The agent follows and learns about a stochastic policy. It usually takes the greedy-action. <br>\n","A small fraction of the time it takes a random action. <br>\n","This way the value estimates for all state-action pairs are guaranteed to continue to improve over time. <br>\n","This On-policy strategy forced us to learn a near optimal policy instead of an optimal policy. <br><br>\n","\n",">But what if we want to learn an optimal policy but stial maintain exploration ? <br>\n",">The answer lies with Off-policy learning, <br>\n",">We introduce some new definitions for Off-policy learning, let's review them.\n","\n","<br><br>\n","\n","\n","\n","Off-policy <br><br>\n","\n","A behavior policy is the policy that the agent uses to select actions. <br>\n","A target policy is the policy that the agent learns about in it's value function. <br><br>\n","\n","By sending an appropriate exploratory behavior policy, <br>\n","the agent can learn about any deterministic taget policy. <br><br>\n","\n","One way to learn about one policy while following another is to use importance sampling. <br>\n","Importance sampling allows the agent to estimate the expected return <br>\n","under the target policy from experience sampled under the behavior policy. <br><br>\n","\n","The ratio re-weights the samples. <br><br>\n","\n","It increases the importance of returns that were more likely to be seen under $\\pi$, <br>\n","it decreases the importance of returns that were unlikely to be seen under $\\pi$. <br>\n","The sample average effectively contains the right proportion of each return so that in expectation it is as if the returns had been sampled under $\\pi$.\n","\n","\n","<br><br><br>\n","\n",">Next we'll talk about one of the most fundamental concepts in R.L., <br>\n",">Temporal Difference learning ! "]}]}
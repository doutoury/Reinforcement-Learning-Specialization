{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2. Week_2-3_Off-policy Learning for Prediction","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyOl0rS83N3NcMxLWFjADJC/"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"39ARKswlLNZg","colab_type":"text"},"source":["## __4. Off-policy Learning for prediction__ <br><br>\n","\n","  - Why does off-policy learning matter ? <br><br>\n","\n","  - Importance sampling <br><br>\n","\n","  - Off-policy Monte Carlo prediction\n","\n","\n","<br><br>\n","\n","We've discussed the __exploration-exploitation trade-off__ <br>\n","The agent must occasionally take __suboptimal exploratory actions__ potentially receiving __less reward__. <br><br>\n","\n","But maybe we do __not need__ to make this __trade-off__ at all ! <br>\n","$\\rightarrow \\quad$ We'll discuss a __new way__ of __learning value functions__ called __Off-policy learning__\n","\n","<br><br>\n","\n",">Maintaining Exploration problem 의 대안 ! <br>\n",">$\\Rightarrow \\quad$ __Off-policy learning__\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"F_a_K3Mb5Klq","colab_type":"text"},"source":["## __$\\cdot$ \" Why does Off-policy learning matter ? \"__ <br><br>\n","\n","\n","  - Understand how __Off-policy learning__ can help deal with the __exploration problem__ <br><br>\n","\n","  - Produce example of __target policies__ <br><br>\n","\n","  - Produce example of __behavior policies__ \n","\n","\n","<br><br>\n","\n",">Off-policy 와 On-policy 왜 나누냐 ? <br><br>\n",">\n",">$\\epsilon$ - policy 의 한계 떄문\n",">Optimal policy 를 찾지 못한다 <br><br>\n",">\n",">$\\Rightarrow \\quad$ Off-policy 를 이용 <br>\n",">$\\qquad$ Target policy 와 Behavior policy 나눠서 학습\n","\n","<br>\n","\n",">Target policy 와 Behavior policy 왜 나누냐 ? <br><br>\n",">\n",">Policy Evaluation (Prediction) 과정과 Policy improvement (Control) 과정을 분리 <br><br>\n",">\n",">Prediction 과정에 대해 stochastic 한 behavior policy 를 쓰고, Control 과정에 대해 deterministic 함 target policy 를 씀으로써 control (policy improvement) 를 통해 converge 되는 policy 가 최종적으로 \" optimal policy \" 가 되도록 유도 ! <br><br>\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7NNPR5Zx5ABY","colab_type":"text"},"source":["### __Limits of $\\epsilon$ - soft policy__ <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1a7Rs0P3QLpca9Aw1k-GSHMA_owLozF0W\" alt=\"4-01\" width=\"500\">\n","\n","<br>\n","\n","\n","  1. Values based on __suboptimal policy__ <br><br>\n","\n","  2. Actions based on __suboptimal policy__ <br><br><br>\n","\n","\n","\n","\n","__Epsilon-soft policies__ <br><br>\n","\n","  - are niether optimal policies for obtaining reward, <br><br>\n","\n","  - nor are the optimal for exploring to find the best actions. <br><br>\n","  \n",">Epsilon-soft policy <br>\n",">are policies help with the continual exploration problem <br>\n","by having some small probability of exploring on each time step. <br><br>\n",">\n",">The __disadvantage__ of Epsilon-soft policies is that <br>\n","they are __suboptimal__ for both __acting__ and __learning__.\n","\n","<br><br>\n","\n","\n","__On-policy learning__ <br>\n",">We've implicitly been discussing __On-policy learning__, <br>\n",">(though we've not yet made a big deal of that name)\n","\n","\n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5ixTUUcW6_oZ","colab_type":"text"},"source":["### __On-policy__ &nbsp; v.s. &nbsp; __Off-Policy__ <br><br>\n","\n","\n","  - On-policy $\\quad : \\;$ Improve and evaluate __the policy__ being used to select actions <br>\n","  >$\\qquad \\quad \\;$ In On-policy learning, the agent learns about the policy <br>\n","  >$\\qquad \\quad \\;$ used __to generate the data__.\n","\n","  <br>\n","\n","  - Off-policy $\\quad : \\;$ Improve and evaluate a __different policy__ from the one used to select actions <br>\n","  >$\\qquad \\quad \\;$ In Off-policy learning, the agent learns about a policy <br>\n","  >$\\qquad \\quad \\;$ from data __generated by following a different policy__. \n","  \n","  <br><br>\n","  \n","  $\\quad$ \" What does it mean to __learn about a policy__ ? \" <br><br>\n","\n","  >__In prediction (policy evaluation)__, <br>\n","  >$\\qquad \\qquad \\quad \\;\\;$ it means __learning__ the __value function__ $v$\n","\n","  >__In control (policy improvement)__, <br>\n","  >$\\qquad \\qquad \\quad \\;\\;$ it means __learning__ the __opitmal policy $\\pi$__\n","\n","\n","<br><br>\n","\n","#### __Off-policy__ <br>\n","$\\rightarrow \\quad$ That is the policy that we are __learning__ ! <br>\n","$\\rightarrow \\quad$ We are using it for __action selection__ ! <br><br>\n","\n","\n",">For example, <br>\n",">you could learn the optimal policy while following a totally random policy !\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XW9jxiEk_X46","colab_type":"text"},"source":["### __Target policy__ &nbsp; & &nbsp; __Behavior policy__ <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=18bdcwe6Qm48YopuZ8ngKQwF__BwMGF72\" alt=\"4-02\" width=\"500\">\n","\n","<br>\n","\n","__Target policy__ $\\quad : \\;$ the policy that the agent is learning <br>\n","$\\qquad \\qquad \\qquad \\;\\;$ ( because it is the target of the agent's learning ) <br><br>\n","\n","Target policy is usually denoted by $\\pi$. <br>\n","The __value function $v$__ that the agent is learning is __based on__ the __target policy__. <br>\n",">One exmple of a target policy is the optimal policy. ( deterministic ? )\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=10KhJWLYw2jfEPQbUQNADTB93h8V0q6BU\" alt=\"4-03\" width=\"500\">\n","\n","<br>\n","\n","__Behavior policy__ $\\quad : \\;$ the policy that the agent is using to select actions <br>\n","$\\qquad \\qquad \\qquad \\quad \\;$ ( because it defines our agent's behavior ) <br><br>\n","\n","Behavior policy is usually denoted by $b$. <br>\n","The __behavior policy__ is in charge of __selecting actions__ for the agent. <br>\n",">One example of a behavior policy is the uniform random policy, like here. ( stochastic ? )\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IsFaEBPmC1oZ","colab_type":"text"},"source":["### \" Why __decoupling__ &nbsp; Behavior policy &nbsp; from &nbsp; Target policy ? \" <br><br>\n","\n","\n","#### Maintain Exploring <br>\n","$\\qquad$ Because it provides __another strategy__ <br>\n","$\\qquad$ for __' continual exploration '__ \n","\n","<br><br><br>\n","\n","\n","### Maintain exploration method __2. Off-policy learning__ <br><br>\n","\n","Another way to maintain exploration $\\quad : \\;$ Off-policy learning <br>\n","$\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad$ (by decoupling target and behavior policies) <br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1iKNCqYPj3aR2VzNcQdIjDMdPR2uqrYoQ\" alt=\"4-04\" width=\"500\">\n","\n","<br>\n","\n","  - If our agent behave according to __the target policy__, <br>\n","  it might only experience a samll number of states. <br><br>\n","\n","  - If our agent can bahave according to a policy that favors exploration, (__behavior policy__), <br>\n","  it can experience a much larger number of state. <br><br>\n","\n",">There are actually a few other useful applications of Off-policy learning <br>\n",">such as learning from demonstration and parallel learning.\n","\n","<br>\n","\n","$... \\quad$ __Facilitating exploration__ is one of the main motivators ! \n","\n","<br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SZlkdfKt3AEI","colab_type":"text"},"source":["### Key rule of Off-policy learning <br><br>\n","\n","\n","Off-policy | On-policy\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1akk-cQwOPZmOdQzCpizK2X9Jl0XxdBZZ\" alt=\"4-04\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1JVLu3fA8i_IUj_BREgAAfgaCbmLWHF_R\" alt=\"4-04\" width=\"500\">\n","\n","<br>\n","\n","$\\text{Target policy} \\; \\pi \\; \\subset \\; \\text{Behavior policy} \\; b$ <br><br>\n","\n","$\\qquad$ The behavior policy must cover The target policy ! <br><br><br>\n","\n","\n","\n","$\\qquad \\; \\pi(a|s) > 0 \\quad \\text{where} \\quad b(a|s) > 0$ <br><br>\n","\n","$\\qquad$ In other words, <br>\n","$\\qquad$ if the target policy $\\pi$ says the probability of selecting an action $a$ given state $s$ is greater than 0, <br>\n","$\\qquad$  then the behavior policy $b$ must say the probability of selecting that action $a$ in that state $s$ is greater than 0. \n","\n","<br><br><br>\n","\n","\n","__Mathemaical reason__ <br><br>\n","\n","There's a key mathematical reason for this ! <br>\n","We'll discuss in an upcoming video. <br><br><br>\n","\n","\n","\n","__Intuitive reason__ <br><br>\n","\n","There's an intuitive reason. <br>\n","Consider the red box state with the behavior policy always goes up, but the target policy goes to the right. <br>\n","The agent cannot learn the correct action value for that state, because it never observes samples of what would happen if it goes right. <br>\n",">(Maybe it gets reward of +1million ... but it would never know that...) \n","\n","\n","<br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CA1iO0UP6dc4","colab_type":"text"},"source":["### Off-policy learning $\\quad : \\;$ __Generalization__ of On-policy learning ! <br><br>\n","\n","\n","It's worth noting that <br>\n","__Off-policy learning__ is a strict __generalization of On-policy learning__ ! <br>\n",">__On-policies__ are the __specific case__ where <br>\n","$\\text{Target policy} \\; = \\; \\text{Behavior policy}$\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"iWcAaw8JOHbv","colab_type":"text"},"source":["### Summary <br><br>\n","\n","  - __Off-policy learning__ allows learning an __optimal policy__ from __suboptimal behavior__ <br>\n","  ( Off-policy learning is another way to obtain continual exploration ) <br><br>\n","\n","  - The policy that we are learning is the __Target policy__ <br><br>\n","\n","  - The policy that we are choosing actions from is the __Behavior policy__\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XvnEK8X-O9qX"},"source":["## __$\\cdot$ Importance Sampling__ <br><br>\n","\n","\n","  - Use __Importance Sampling__ <br>\n","  to estimate the expected value of a __(target) distribution__ <br>\n","  using samples from a __different (behavior?) distribution__ \n","\n","<br><br>\n","\n","In this video, <br>\n",">We describe \" How importance sampling works \" \n","\n","<br>\n","\n","In later video, <br>\n",">__Importance sampling__ allows us to do __Off-policy learning__, <br>\n",">learning with one policy while following another.\n","\n","<br><br>\n","\n","\n","\n",">Importance sampling 이 <br>\n",">Off-policy learning applying 예시 ???\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JCkwSfyqP3mk","colab_type":"text"},"source":["### Derivation of Importance Sampling <br><br>\n","\n","__Stating the problem__ <br>\n","that importance sampling solves <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1Ee2QdDreKcjFim9g6Xu4jKUOvgjghM8a\" alt=\"4-07\" width=\"500\">\n","\n","<br>\n","\n","We have some random variable $x$ <br>\n","that's being sampled from a probability distribution $b$. <br><br>\n","\n","We want to estimate the expected value of $x$ <br>\n","but with repect to the target distribution $\\pi$. <br><br>\n","\n","Because $x$ is drawn from dist. $b$ <br>\n","We cannot simply use the sample average to compute the expeatation under dist. $\\pi$. <br>\n","( The sample average will give us the expected value under $b$ instead ) <br><br>\n","\n","\n","### Note ! <br>\n","\n",">확률변수와 확률분포의 정의와 관계 Statistics 110 에서 복습정리 하고 <br>\n",">( 확률변수가 여러개의 확률분포를 갖을 수 있는지, 반대도 성립 하는지 ... ) <br>\n",">이 내용 다시 정리 ! \n","\n",">Random Variable <br>\n",">It is a function which maps some samples(set) as an input from the sample space $S$ to real line $\\mathbb{B}$ as an output of real-number.\n","\n",">Probability <br>\n",">~ \n","\n",">Distribution <br>\n",">~\n","\n",">__Non-naive definition of probability__ &emsp; [1강 후반 ~ 2강 초반] <br><br>\n",">\n",">$P(X)$ &emsp; &emsp; : mapping function from events $X$ to probabilities $P$ <br>\n",">&emsp; &emsp; &emsp; &emsp; &emsp; analogously, possible $y$ values , described as f(x) <br>\n",">$X$ &emsp; &emsp; &emsp; &ensp; : mapping function from universe events $S$ to specific events $A$ <br>\n",">&emsp; &emsp; &emsp; &emsp; &emsp; analogously, possible $x$ values \n","\n","<br>\n","\n","[ 01:08 ] <br><br>\n","\n","\n","&nbsp; | &nbsp;\n","--- | ---\n","Importance sampling ratio <br> ( weight ) | <img src=\"https://drive.google.com/uc?id=1cSLSmrgHMr0rrnblLxMu3FQP-4Yq6YvK\" alt=\"4-08\" width=\"500\">\n","Expectation following $b$ | <img src=\"https://drive.google.com/uc?id=1S5XwlFl-KBI3zL-8aRiIKEhZsfKwKJM3\" alt=\"4-09\" width=\"500\">\n","Compute with sampling <br> ( 모집단에 대한 표본의 근사 ) | <img src=\"https://drive.google.com/uc?id=1j2uKjrsauoLtx0pwaiSOuHTkWziLQ7KQ\" alt=\"4-10\" width=\"500\">\n","x ~ dist.$b$ 로 표현한 <br> x ~ dist.$\\pi$ 의 평균 | <img src=\"https://drive.google.com/uc?id=1pc_qOwLYiIkfzwU95gFOpGMtJTCYR5wj\" alt=\"4-11\" width=\"500\">\n","\n","<br>\n","\n","  1. importance sampling ratio ( weight ) <br>\n","  Do some arithmatic trick to represent this term with dist.$b$ <br>\n","  >Rho of $x$. <br>\n","  >$\\rho(x) = \\frac {\\pi(x)}{b(x)} \\quad : \\;$ A ratio between $\\pi$ and $b$ &ensp; ( called \" Importance sampling ratio \" )\n","\n","  <br>\n","\n","  2. Expectation following $b$ <br>\n","  If we treat $x \\rho(x)$ as a new random variable, <br>\n","  we can rewrite this sum as an expectation under $b$ <br>\n","  >Notice. <br>\n","  >Our expectation is now under $b$ ! instead of being under $\\pi$ ! \n","\n","  <br>\n","\n","  3. Compute with sampling <br>\n","  How do we use it to estimate the expectation from date ? <br>\n","  We just need to compute a weight sample average <br>\n","  ( with $\\rho(x)$ as the weightings ) <br>\n","  >Notice, <br>\n","  >These samples $x_i$ are drawn from $b$, not $\\pi$.\n","\n","  <br>\n","\n","  4. Compute with sampling <br>\n","  We can now estimate the expected value of $x$ under dist.$\\pi$ <br>\n","  by using the sample average, the samples drawn from dist. $b$ !\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"t2AmfE26B-mY","colab_type":"text"},"source":["### Example <br><br>\n","\n","Example of using importance sampling to estimate an expectation. <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1rST4CItDA2ounOdReU7eHlBNRkSTiJ7h\" alt=\"4-12\" width=\"500\">\n","\n","There're two rather different distribution $b(x)$ and $\\pi(x)$ <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1l-w0y4srkkDCj18VokF-sHl4dtR36YX_\" alt=\"4-13\" width=\"500\">\n","\n","We'll draw samples according to $b$ and try to estimate the expected value under $\\pi$. <br>\n","On the right, we'll keep track of our current estimate for the expeccted value under $\\pi$. <br>\n","For reference, we show the true expected value in the middle of the line. \n","\n","<br><br><br>\n","\n","\n","Let's draw a sample from $b$ ! <br><br>\n","\n","Sampling step | Sampled expected value\n","--- | ---\n","1st sampling from $b$ <br><br> $x=1$ | <img src=\"https://drive.google.com/uc?id=18Zsv61gBSGDkNv_54QuOMYVJ0KpZi51v\" alt=\"4-14\" width=\"500\">\n","2nd sampling from $b$ <br><br> $x=3$ | <img src=\"https://drive.google.com/uc?id=1Ngw9COeZnc6uVQCyRzHVbTQJJ2OrT5jN\" alt=\"4-15\" width=\"500\">\n","3rd sampling from $b$ <br><br> $x=1$ | <img src=\"https://drive.google.com/uc?id=1k9wM0biSlb6ZcX2A91amGDIP0F-Xeete\" alt=\"4-16\" width=\"500\">\n","\n","<br><br>\n","\n","We draw one sample $x = k$ from distribution $b$, which happens with probability of $b(x=k)$. <br>\n","And there're the probability $\\pi(x=k)$ too. <br>\n","Then we can get the weight \"Rho\" ! <br><br>\n","\n","To compute the sample average for $\\pi$ <br>\n","we need out importance sampling formula. <br><br>\n","\n","... <br><br>\n","\n","\n","With  samples just from $b$, <br>\n","we managed to get a pretty good estimate of the expectation under $\\pi$. <br><br>\n","\n",">The estimates we got through computing the saple averages are slightly different from the true expected value. But the more sampling we do, the more accurate the sample average is going to be ! \n","\n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UnhforY-QZqI","colab_type":"text"},"source":["### Summary <br><br>\n","\n","  - __Importance sampling__ <br>\n","  uses samples from one probability distribution <br>\n","  to estimate the expectation of a different distribution\n","\n","<br>\n","\n",">Importance sampling 을 이용해 <br>\n",">다른 분포에서 sample 된 data 를 이용해 <br>\n",">목표 분포의 Expectation (value) 를 계산 !\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XhkB_mLARaQ5","colab_type":"text"},"source":["## __$\\cdot$ Off-policy Monte Carlo prediction__ <br><br>\n","\n","  - Understand how to use __Importance sampling__ to correct returns <br><br>\n","\n","  - Understand how to modify the __Monte-Carlo prediction algorithm__ for Off-policy learning\n","\n","\n","<br><br>\n","\n",">We've discussed Off-policy learning. <br>\n",">We'll see how to do Off-policy prediction with Monte-Carlo !\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"eVd5B_4JMC4m","colab_type":"text"},"source":["### Recall : The goal of Monte Carlo estimation <br><br>\n","\n","\n","__The goal of Monte-Carlo estimation__ <br><br>\n","\n","$\\qquad v_{\\pi}(s) \\doteq \\mathbb{E} \\big[ G_t | S_t = s \\big]$\n","\n","<br><br><br>\n","\n","\n","\n","__Off-policy Monte-Carlo__ <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1Hxw7TkE8sgIqmtelAyER1ceYN3NLjzXq\" alt=\"4-18\" width=\"500\">\n","\n","<br>\n","\n","We would like to estimate the value of each state <br>\n","by computing a sample average over returns starting from that state. <br><br>\n","\n","\n","We run into an inssue <br>\n","When we try to estimate the value under target policy $\\pi$, using returns following a behavior policy $b$. <br><br>\n","\n",">If we simply average the returns, we saw from state $s$ under the behavior $b$, <br>\n","we'll not get the right answer. \n","\n","<br><br><br>\n","\n","\n","\n","__Importance sampling__ <br><br>\n","\n","\n","Off-policy learning without Importance sampling | Off-policy learning with Importance sampling\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1Dj0qfB9JS9BogIcit_vxd1sZCvtapxkX\" alt=\"4-19\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1dNZOYdXy7wuU5qSuwK8zDcYbkdtLKF9M\" alt=\"4-20\" width=\"500\">\n","\n","\n","We have to correct each return in the average. <br>\n","This is just what Importance Sampling is for. <br><br>\n","\n","\n","All we have to do is figure out the value of rho $\\rho$ for each of the sampled returns. \n","\n","<br><br><br>\n","\n","\n","\n","__Solving Off-policy Monte-Carlo with Importance Sampling__ <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1KQYQOxFnURBA_pAmBOHtKqxdxzeT6BIB\" alt=\"4-21\" width=\"500\">\n","\n","<br>\n","\n","Rho $\\rho$ is the probability of the trajectory under $\\pi$ divided by the probability of the trajectory under $b$. <br><br>\n","\n","This Rho corrects the distribution over entire trajectories, <br>\n","and so corrects the distribution over returns. <br><br>\n","\n","Using this correction, we get back what we want, <br>\n","the expectation of the return under $\\pi$ <br><br>\n","\n","To compute $\\rho$, <br>\n","we need to figure out how to compute the probability of a trajectory uder a policy. \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nHEhVXULUS82","colab_type":"text"},"source":["### Off-Policy Trajectories <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1c-pDreFkaFEyh7hmOlnU-1pyyCpu3Mz3\" alt=\"4-22\" width=\"500\">\n","\n","\n","Let's consider <br>\n","the probability distribution over trajectories. <br><br>\n","\n","We read this probability as, <br>\n","\" Given that the agent is in some state $s_t$, what is the probability that is takes action $a_t$ then ends up is state $s_{t+1}$, then it takes action $a_{t+1}$ and ends up in $s_{t+2}$, and so on, until termination at time $T$ ? \" <br><br>\n","\n","All of the actions are smapled according to behavior $b$. <br><br>\n","\n","1 | 2\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1FyYMw8yFfF1Fg_BSYTjoCPT1ovt5qbUd\" alt=\"4-23\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1VRbEbd4bsZ_ViveFmtnzDUv4hwdY7x7i\" alt=\"4-24\" width=\"500\">\n","\n","<br>\n","\n","Because of the __Markove property__, <br>\n","We can break this probability distribution into smaller chunks. <br><br>\n","\n","The first chink is the probability that the agent selects action $a_t$ in state $s_t$, times the probability that the invironment transitions $p$ into state $s_{t+1}$. <br><br>\n","\n","The second chunk gives the probability of the next step of experience, and so on. <br><br>\n","\n","We can rewrite this list of producted probabilities using the product notation ! \n","\n","\n","<br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"B2mGNFhpW6Nb","colab_type":"text"},"source":["### ~ <br><br>\n","\n","[ 02:50 ~ 03:30 ] <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=12BmsYw2TQatVRr7pU78TDzKcJU2BWRNg\" alt=\"4-25\" width=\"500\">\n","\n","Now, we've defined the probability of a trajectory under $b$. <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1G8yLQm7R_BaBLxBci8_BOPq7y4sSNqeS\" alt=\"4-26\" width=\"500\">\n","\n","<br>\n","\n","Remember where we are going. <br>\n","We would like to define Rho $\\rho$ using the probability of the trajectory under $\\pi$ and the probability of the trejectory under $b$. <br><br>\n","\n","\n","Let's plug these probabilities into our definition of Rho $\\rho$. <br>\n","As we saw in a previous video, we can take these probabilities and multiply them by the importance sampling ratio. <br>\n","The transition dynamics $p$ of the environment cancel out on each time step. <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1aB942T2i632hjXlC5DiGTLXTq6jk3CWj\" alt=\"4-27\" width=\"500\">\n","\n","This leaves us with only a product over the ratios between policies at each time step. \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"agV-s0G-eQIn","colab_type":"text"},"source":["### Off-Policy value <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1OJACfPJsXRHD-DGaSFejtiwViCnlMxw6\" alt=\"4-28\" width=\"500\">\n","\n","\n","Now, let's get back to estimating $V_{\\pi}$ off-policy. <br><br>\n","\n","The agent observes many returns, each according to the behavior policy $b$. <br>\n","We can estimate $V_{\\pi}$ using these returns by correcting each return with Rho $\\rho$. <br><br>\n","\n","Let's look at how we would implement this. "]},{"cell_type":"markdown","metadata":{"id":"gRQwiqDCgCbI","colab_type":"text"},"source":["### Algorithm <br><br>\n","\n","Recall : On-policy Monte-Carlo prediction algorithm <br><br>\n","\n","1 | 2\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=11dmD2AeXBg3jGIqO7Kz_eXSqfjmHbyxY\" alt=\"4-29\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1-jyq6PR-pF_hvwa4hDotpfCJJx9wUoqA\" alt=\"4-30\" width=\"500\">\n","\n","<br>\n","\n","There're two changes that we'll need to be made here. <br>\n","First, the epdiose will not be generated following $\\pi$, but it'll be generated following $b$. <br>\n","Second, the return will need to be corrected using the product of the importance sampling ratios. \n","\n","<br><br><br>\n","\n","\n","Recall : Off-policy Monte-Carlo prediction algorithm\n","\n","1 | 2\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1ovl0ZrkphpISIoKQpQ5N3UVB96Hmge4o\" alt=\"4-30\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1C7X5i6azSewXM9hQxJ7Ody13rtFyJ7jn\" alt=\"4-31\" width=\"500\">\n","\n","<br>\n","\n","By making these changes, we end up with Off-policy Monte-Carlo prediction algorithm. <br>\n","Notice the episode is now generated following the behavior policy $b$. <br>\n","The return is corrected by a new term $W$, <br>\n","which is the accumulated product of importance sampling ratios on each time step of the episode.\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HoDDHoUtjuqK","colab_type":"text"},"source":["### Computing $\\; \\rho_{t:T-1} \\;$ incrementally <br><br>\n","\n","\n","1 | 2\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1_JNDuscUjoB9GPnjG-cWHzKv3fXNbvl0\" alt=\"4-33\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1ZT7ewXu8-xItDA8K7n9VLknMflSJAF_C\" alt=\"4-35\" width=\"500\">\n","\n","<br>\n","\n","\n","We can compute Rho $\\rho$ from $t$ to $T-1$ incrementally. To see why, write out the product at each time step. Recall the Monte-Carlo algorithm loops over time steps backwards. <br><br>\n","\n","On the first step of the algorithm, <br>\n","$W$ is set to Rho on the last time step $\\rho_{T-1}$. <br><br>\n","\n","On the next time step, <br>\n","$W$ is the second last Rho $\\rho_{T-2}$ times the last Rho $\\rho_{T-1}$. <br><br>\n","\n","And so on ... <br><br>\n","\n","\n","Each time step adds one additional term to the product and reuses all previous terms. <br>\n","We can compute this recursively without having to store all past values of Rho. \n","\n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hyvWhY6FmaV0","colab_type":"text"},"source":["### Summary <br><br>\n","\n","  - Used __Importance sampling ratios__ to correct the returns <br>\n","  ( which generated by the havior policy ) <br><br>\n","\n","  - Modified the __On-policy__ Monte-Carlo prediction algorithm for __Off-policy__ learning\n","\n","\n","<br><br><br><br><br>\n","\n","\n"]}]}
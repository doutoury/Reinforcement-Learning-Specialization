{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2. Week_3-1_Introduction to Temporal Difference Learning","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyMiF8nnnfjFgAPCGmjg04mi"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yydQ9qjsKBRW","colab_type":"text"},"source":["# Week_3 <br>\n","\n","INDEX <br><br>\n","\n","\n","  - Introduction to Temporal Difference Learning <br>\n","    - What is Temporal Difference (TD) learning ? <br>\n","    - Rich Sutton : The importance of TD learning <br><br>\n","  \n","  - Advantages of TD <br>\n","    - The advantages of temporal difference learning <br>\n","    - Comparing TD (Temporal Difference) and MC (Monte-Carlo) <br>\n","    - Andy Barto and Bish Sutton : More on the history of R.L.\n","\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"39ARKswlLNZg","colab_type":"text"},"source":["## __1. Introduction to Temporal Difference Learning__ <br><br>\n","\n","\n","  - What is Temporal Deifference learning ? <br><br>\n","\n","  - Rich Sutton : The importance of TD learning\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"SrLl-g65LbKc","colab_type":"text"},"source":["## $\\cdot$ What is Temporal Difference learning ? <br><br>\n","\n","\n","  - Define __Temporal Deifference Learning__ <br><br>\n","\n","  - Define the __Temporal Difference Error__ <br><br>\n","\n","  - Understand the __TD(0) algorithm__\n","\n","<br><br>\n","\n","__Temporal Deifference learning__ <br>\n","$\\qquad$ One of the most fundamental ideas in R.L. <br><br>\n","\n",">$\\color{gray}{\\text{To quote from the textbook,}}$ <br>\n",">$\\color{gray}{\\text{If one had to identify one idea as central and novel to R.L.,}}$ <br>\n",">$\\color{gray}{\\text{It would be Temporal Difference learning !}}$ \n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NoGI6sW6S84N","colab_type":"text"},"source":["### Review $\\quad : \\;$ Estimating values from Returns [ Monte-Carlo ] <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1QB3mq8ei-o_YrkckTkQQJtg63Qy4tRJg\" alt=\"1-01\" width=\"500\">\n","\n","### Estimated value from Monte-Carlo <br><br>\n","\n","\n","$v_{\\pi}(s) \\doteq \\mathbb{E}_{\\pi} \\big[ G_t | S_t = s \\big]$ <br><br>\n","\n","\n","In the prediction problem, <br><br>\n","\n","$\\qquad$ our goal is to learn a value function ($\\; v_{\\pi}(s) \\;$) <br>\n","$\\qquad$ that estimates ($\\; \\mathbb{E} \\;$) the returns ($\\; G_t \\;$) starting from a given state ($\\; |S_t=s \\;$). \n","\n","<br><br><br>\n","\n","\n","\n","### Incremental update for Monte-Carlo <br><br>\n","\n","\n","><img src=\"https://drive.google.com/uc?id=1QBLy2XLlaD4n5yMjxzqxYm0HP8TlasKA\" alt=\"1-01-2\" width=\"500\"> <br>\n","\n","><img src=\"https://drive.google.com/uc?id=1JjlfRxlRPg_mN6t-gJxxpQCRfN_Ri6te\" alt=\"1-01-3\" width=\"500\"> \n","\n","<br><br>\n","\n","\n","\n","$V(S_{t}) = \\mathbb{E}_{\\pi} \\big[ G_t | S_t = s \\big]$ <br><br>\n","\n","$V(S_t) \\; \\leftarrow \\; V(S_t) + \\alpha \\big[ G_t - V(S_t) \\big]$ <br><br><br>\n","\n","\n","Small modification &ensp; ( for being incremental ? ) <br>\n","to our Monte-Carlo policy evaluation method. <br><br>\n","\n","$\\qquad$ We can use this formula to __incrementally update__ our estimated value. <br><br>\n","\n","$\\qquad$ Notice that this uses a __constatnt step size ($\\alpha$)__ like our Bandit learning algorithms. <br>\n","$\\qquad$ That means this algorithm can form a __Monte-Carlo__ estimate __without saving lists of returns !__ <br>\n","$\\qquad$ ( Not to save all returns, just check one return right after the end of episode and update estimated value )<br><br>\n","\n",">To compute the return ($G_t$), <br>\n",">we have to take samples of full trajectories. ( Not learning during the episode )\n","\n",">But we want to be able to learn incrementally __before the end of the episode !__ <br>\n",">We must come up with a __new update target__ to achieve this goal. ( instead of return $G_t$ )\n","\n","<br><br>\n","\n","\n",">$\\color{gray}{\\text{1. Week_1-1 최정구님 발표자료 참고}}$ <br>\n",">$\\color{gray}{\\quad \\text{( 강화학습_싸이그래머_200413 )}}$ <br><br>\n",">\n",">Sample-based Bandit problem 에서는 <br>\n",">Bandit 머신을 한번 당길 때 마다 받는 reward $R$ 에 대한 incremental update 를  통해 <br>\n",">한번의 게임이 끝날 때까지 기다리지 않고 value 를 update 한다면, (episode 가 없다 ??) <br><br>\n",">\n",">Sample-based Monte-Carlo problem 에서는 <br>\n",">Episode 를 한번 끝낼 때 마다 받는 Return $G$ 에 대한 incremental update 를 통해 <br>\n",">전체 episode 들을 다 돌 때싸지 기다리지 않고 value 를 update 한다 ??? \n"," \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XYrgfoR8Nk-v","colab_type":"text"},"source":["### Recall $\\quad : \\;$ Incremental updating value from sample average [ Bandit Problem ] <br><br>\n","\n","\n","#### Estimate value ($\\; V_n = \\mathbb{E} [R] \\; ? \\;$) in Bandit problem <br><br>\n","\n","\n","  - __State-value function__ <br><br>\n","\n","  When we know __the environment dynamics $p$__, <br>\n","  We just do $\\displaystyle \\sum_r p(r|s) \\; r$ __to estimate the value__. <br><br><br>\n","\n","\n","  - __Sample-average method__ <br><br>\n","\n","  When we have __no environment dynamics $p$__, <br>\n","  We instead do $\\displaystyle \\frac {\\sum_{i=1}^n R_i} {n}$ __to estimate the value__. <br>\n","  ( Naive definition of Estimation : Sample Mean (표본평균) (by average?) ) <br><br>\n","\n","  >We should __do sampling__ to __approximate environment dynamics ($p$) and estimation ($\\mathbb{E}$)__\n","\n","<br><br><br>\n","\n","\n","\n","#### Incremental update rule in Bendit problem <br><br>\n","\n","\n","  - __incremental update rule__ using Sample-average method <br><br>\n","\n","  $\\begin{align} V_{n+1} &= \\frac{1}{n} \\displaystyle \\sum_{i=1}^n R_i \\\\ \n","  &= V_n + \\frac{1}{n} (R_n - V_n) \\qquad \\qquad \\text{... another representation of Sample Mean} \\end{align}$ <br><br><br>\n","\n","\n","\n","  - __Estimating State-value incrementally__ <br>\n","  by Incremental update rule ( with sampling ) <br><br>\n","\n","  $\\begin{align} \\text{NewEstimate} \\quad &\\leftarrow \\quad \\text{OldEstimate + Stepsize [ Target - OldEstimate ]} \\\\ \\\\\n","  V_n \\quad &\\;= \\quad V_n + \\frac{1}{n} (R_n - V_n) \\\\ \n","  &\\; \\Downarrow \\\\ \n","  V_n \\quad &\\;= \\quad V_n + a_n (R_n - V_n) \\qquad \\qquad , \\quad a_n=[0,1] \\end{align}$\n","\n","\n","<br><br><br>\n","\n","\n",">It is a contents on &ensp; [ 1. Week_1 Bandit problem ] <br>\n",">여기서 n 은 Bandit machine 의 한번 당기는 시행 횟수 <br>\n",">후에 한 episode 의 trajectory 안에서 policy 를 따라 한 번 시행하는 단계 t 와 같은 의미 ?\n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"KeSZpiUqdA2M","colab_type":"text"},"source":["### Notation Check <br><br>\n","\n","subscript notation <br><br>\n","\n","  - $\\pi$ <br>\n","  - $t$ <br>\n","  - $n$ <br>\n","\n","<br><br>\n","\n","에피소드단위, trajectory 단위\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rRg_uLEBch9p","colab_type":"text"},"source":["### Bootstrapping <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1v6eOaxPDfK-9NgNf169arREsxGsHXHG-\" alt=\"1-02\" width=\"500\"> \n","\n","### Recursive expression of Return <br><br>\n","\n","$G_t = R_{t+1} + \\gamma G_{t+1}$ <br><br>\n","\n","Recall : <br>\n","the definition of the discounted return. &ensp; ( For the continual (non-episodic) problem ? ) <br><br>\n","\n","$\\rightarrow \\quad$ __Reward $G_t$__ can be written __recursively__ with $\\gamma$ !\n","\n","<br><br><br>\n","\n","\n","### Recursive expression of Value function <br><br>\n","\n","$v_{\\pi}(s) = R_{t+1} + \\gamma v_{\\pi}(S_{t+1})$ <br><br>\n","\n","The value of a state at time $\\; t \\;$ is the expected return at time $\\; t \\;$. <br>\n","We can __replace__ the return inside this expectation __with a recursive definition__. <br><br>\n","\n","\n","We can further split up this equation __by the linearity of expectation__. <br>\n","We then get the expectation of the return on the next step $\\mathbb{E}_{\\pi} \\big[ G_{t+1} | S_t=s \\big]$. <br><br>\n","\n","It is just the value of the next state ($v_{\\pi}(S_{t+1})$). <br>\n","Now we have written the __value function recursively__ as well. \n","\n","<br>\n","\n","$\\rightarrow \\quad$ __Value function $v_{\\pi}$__ can be written __recursively__ with $\\gamma$ ! <br>\n",">R.L. 81p\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"eHDNG2OHhA6j","colab_type":"text"},"source":["### Temporal Difference <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1DqSN09niYahVn8NMNfYjTYJrtBwLqw3M\" alt=\"1-03\" width=\"500\"> \n","\n","\n","Let's go back to our __incremental Monte-Carlo update rule__. <br>\n","We want to update toward the return, <br>\n","but we don't want to wait for it. <br><br><br>\n","\n","\n","\n","$G_t \\; \\approx \\; R_{t+1} + \\gamma V(S_{t+1})$ $\\qquad \\qquad \\qquad$ ... <br><br>\n","\n",">의미적으로 접근 <br><br>\n",">\n",">실제 trial 의 수형도에서 <br>\n",">$G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{T-1} R_{T} \\qquad \\qquad \\text{... T : terminal}$ <br>\n",">$G_t \\approx R_{t+1} + \\gamma V(S_{t+1})$ <br><br>\n",">\n",">Monte-Carlo 는 한 trial 에서 $R_{t+1}$ 부터 $R_{T}$ 까지 전부 확인 <br>\n",">Temporal Difference 는 한 trial 에서 $R_{t+1}$ 까지만 확인 <br>\n",">$\\quad \\rightarrow \\quad$ 따라서 $R_{t+2} \\; \\ldots \\; R_{T}$ 는 확인 불가 <br>\n",">$\\quad \\rightarrow \\quad$ 따라서 이 부분을 예측(estimate)값인 $V(S_{t+1})$ 으로 대체 !\n","\n","<br>\n","\n","We can replace the return ($G_t$) at time $\\; t \\;$ <br>\n","with the reward ($R_{t+1}$) $\\; + \\;$ the estimate of the return ($V_{t+1}$) in the next state. <br>\n","( We can think of the value of the next state as a stand-in for the return until the end of the episode ) <br>\n","\n",">$G_{t+1}$ 을 $V(S_{t+1})$ 으로 퉁 치는 과정에서 편향(bias)이 생기고 계속 누적나?\n","\n",">$V(S_t)$ 추측을, $R_t + V(S_{t+1})$ 다음 스텝의 추측으로 표현된 식으로 추측 !\n","\n",">Monte-Carlo 에서는 한 episode 의 시도를 통해 관찰된 sample return $G_t$ 로 current value $V(S_t)$ 를 추측하지만, <br>\n",">Temporal Difference 에서는 한 episode 의 시도도 하지 않고 다음 스텝의 successive value $V(S_{t+1})$ 로 current value $V(S_t)$ 를 추측하려 시도 !\n","\n","<br><br>\n","\n","\n","So we don't have to wait until the end of the episode, <br>\n","but we still have to wait to the next step. <br><br>\n","\n","$t \\;$ target <br>\n","We call this this $R_{t+1} + \\gamma V(S_{t+1}) \\quad$ \" the $\\; t \\;$ target \"\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"O1VYwwJfikgd","colab_type":"text"},"source":["### Error <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1DqSN09niYahVn8NMNfYjTYJrtBwLqw3M\" alt=\"1-03\" width=\"500\"> \n","\n","__TD error__ <br>\n","The term inside the brackets resemble an error. <br><br>\n","\n","$\\big[ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\big]$ <br><br>\n","\n","  - It is called \" the TD error \". <br>\n","  - the TD error is denoted by $\\; \\delta_t \\;$.\n","\n","<br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_ieAHaHqswjD","colab_type":"text"},"source":["### Updating from a Prediction <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1zlym-goLb83o56Tlf7SOynaMpW4Zj4WQ\" alt=\"1-05\" width=\"500\"> \n","\n","\n","The target of our update might seem a little strange at first. <br>\n","TD updates the value of one state toward its own estimate of the value in the next state. <br>\n","As the estimated value for the next state improves, so does our target. <br><br>\n","\n","\n",">In fact, we've done something like this before in Dynamic Programming. <br><br>\n",">\n","><img src=\"https://drive.google.com/uc?id=1NU0tQQy6qpldIbSD9iLfK8GgRLpVpuu2\" alt=\"1-06\" width=\"400\"> <br>\n",">In DP, we update it toward the value of all possible next states. <br>\n","The primary difference is in DP, we use an expectation over all possible next states. <br>\n",">We need a model of the environment to compute this expectation, <br>\n",">In TD, we only need the next state. <br>\n",">We can get that directly from the environment without a modle. \n","\n","<br>\n","\n","step | convergence\n","--- | ---\n","1 | <img src=\"https://drive.google.com/uc?id=1Qll-KvHblsybkR26fsJxA-qfWxu1E_Gb\" alt=\"1-07\" width=\"300\">\n","2 | <img src=\"https://drive.google.com/uc?id=1MOnJjXm7nhY6uvRRXiBcySmgsHlTs5F3\" alt=\"1-08\" width=\"300\">\n","3 | <img src=\"https://drive.google.com/uc?id=1XK91zeD1hGeQhmP9V8-oypiu_tEzFvCs\" alt=\"1-09\" width=\"300\">\n","4 | <img src=\"https://drive.google.com/uc?id=1GXFBsPFGuNPTBc8KWlwe5AlH42bSUpFu\" alt=\"1-10\" width=\"300\">\n","5 | <img src=\"https://drive.google.com/uc?id=1PfKNy69HHBNRLf_jWsdE9QJAFgKpkJ24\" alt=\"1-11\" width=\"300\">\n","6 | <img src=\"https://drive.google.com/uc?id=1cOcpwfnDwRM3VUik7MSB6bWD_qCIZ8yR\" alt=\"1-12\" width=\"300\">\n","\n","\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"URzqT7wBnEyd","colab_type":"text"},"source":["### How we get that next state directly from the env. ? <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1HOWb8e-6OYHG-KtFKmXYtbEg2abd-Quk\" alt=\"1-13\">\n","\n","#### Reference from DP <br><br>\n","\n","How we get that next state directly from the environment ? <br><br>\n","\n","Think of time $\\; t+1 \\;$ as the current time step, <br>\n","Think of time $\\; t \\;$ as the previous time step. <br><br>\n","\n","We simply store the state from the previous time step <br>\n","in order to make our TD updates. <br><br><br>\n","\n","\n","$s$ and $a$ from previous time step | $s'$ and $r$ for current time step\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1xyjaU_6cmyHFdZ6wIq2Ds6u_EPYwKCGJ\" alt=\"1-14\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1BAXCYV3_5wnofGBYfY6UpRgYnjDudNIX\" alt=\"1-15\" width=\"500\">\n","\n","<br>\n","\n","We see a stream of experience : <br>\n","state $\\; \\rightarrow \\;$ action $\\; \\rightarrow \\;$ reward $\\; \\rightarrow \\;$ next state $\\; \\rightarrow \\quad ... \\;$ <br><br>\n","\n","From the state of time $\\; t \\;$, we can take an action, and observe the next state at time $\\; t+1 \\;$. <br>\n","Only then can we update the value of the previouse state. <br><br>\n","\n","[ ~ 03:50 ] <br>\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xQg26NqfvfMy","colab_type":"text"},"source":["### Algorithm <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=15N0dXIr7xrwrUYCetwJ6LFIxPzkFuW6n\" alt=\"1-16\">\n","\n","We can now fully describe the tabular TD(0) algoriths. <br><br>\n","\n","TD takes the policy ($\\pi$) to evalate as input. <br>\n","It also requires a step size parameter ($\\alpha$) and an initial estimate of the value function ($V(s)$). <br><br>\n","\n","Every episode begins in some initial state $S$, <br>\n","and from there the agent takes actions according to it's policy until it reaches the terminal state. <br><br>\n","\n","On each step of the episode, <br>\n","we update the values with the TD learning rule. We onlt need to keep track of the previous state to make the update. <br><br><br>\n","\n","\n","\n","This is TD(0). <br>\n","Many algorithms and R.L. are based on TD. \n","\n","<br><br><br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"42f1cZw8xVb6","colab_type":"text"},"source":["### Summary <br><br>\n","\n","  - Temporal Difference Learning <br>\n","  : A way to incrementally estimate the return through bootstrapping <br><br>\n","\n","  - TD-error <br>\n","  : $\\delta_t \\; = \\; R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$ <br><br>\n","\n","  - TD(0) algorithm\n","\n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HUlMJN4_QZjc","colab_type":"text"},"source":["### Comparison among DP / MC / TD update rules ! <br><br><br>\n","\n","\n","  1. Dynamic Programming <br>\n","  $V'(s) \\quad \\leftarrow \\quad \\displaystyle \\sum_a \\pi(a|s) \\sum_{s'} \\sum_r p(s',r|s,a) \\big[ r + \\gamma V(s') \\big]$ <br><br>\n","\n","  When we know dynamic environment $p$, <br>\n","  Update every estimate variable value $v_{k+1}(s)$ Using Bellman Equation <br>\n","  which is the euqation about the relationship between $v_{k}(s_t)$ and $v_{k}(s_{t+1})$ <br>\n","  for a sweep of all state values step by step <br><br><br>\n","\n","\n","  2. Monte-Carlo <br>\n","  $V(S_t) \\quad \\leftarrow \\quad V(S_t) + \\alpha \\big[ G_t - V(S_t) \\big]$ <br><br>\n","\n","  When we don't know $p$, <br>\n","  Update estimate R.V. value $V(S_t)$ using Incremental update rule with sampling <br>\n","  which is the accumulative update rule each time the successive observation of $G_t$ comes up <br>\n","  for a episode of all whole trial trajectory <br><br><br>\n","\n","\n","  3. Temporal Difference <br>\n","  $V(S_t) \\quad \\leftarrow \\quad V(S_t) + \\alpha \\big[ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\big]$ <br><br>\n","  \n","  When we don't know $p$, <br>\n","  Update estimate R.V. value $V(S_t)$ using incremental update rule with sampling <br>\n","  which is the accumulative update rule each time the successive observation of $R_{t+1}$ comes up <br>\n","  for a trial of one step <br><br><br>\n","\n","\n","~ <br><br><br>\n","\n","\n"]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2. Week_3-2_Advantages of TD","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyMWIS7QpV8bv/lGCFJ1RwJx"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"39ARKswlLNZg","colab_type":"text"},"source":["## __2. Advantages of TD__ <br><br>\n","\n","\n","  - The advantages of Temporal Difference learning <br><br>\n","\n","  - Comparing TD and Monte-Carlo\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"SrLl-g65LbKc","colab_type":"text"},"source":["## $\\cdot$ The advantages of Temporal Difference learning <br><br>\n","\n","\n","  - Understand the benefit of __learning on-line__ with TD <br><br>\n","\n","  - Identify key advantages of TD methods over DP and MC methods \n","\n","\n","<br><br><br>\n","\n","\n","\n","#### __Temporal Difference learning__ <br>\n","TD elegantly combines key ideas from Dynamic Programming and Monte-Carlo methods. <br><br>\n","\n","  - Like Dynamic Programming, TD methods __bootstrap__. <br><br>\n","\n","  - Like Monte-Carlo methods, TD can learn directly __from experience (samples)__. \n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gqlZ2CgHjUd9","colab_type":"text"},"source":["### __Exmple__ <br><br>\n","\n","#### Driving Home from work <br><br>\n","\n","Each day, you predict how long it will take to get home. <br>\n","You base your predictions on the time, the day of the week, the weather, and the aother factors. <br><br>\n","\n","Imagine that you have driven home many times before, <br>\n","so you have a good estimate of how long it takes to get home from various situations you might encounter on the way. \n","\n","<br><br><br>\n","\n","\n","\n","\n","#### One trip home <br><br>\n","\n","\n","One trip scenario | specifying rewards for the problem\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1UFHGuAPg3H3OlT4aTfSsWwohAVni-QCt\" alt=\"2-01\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1WRfcyOz3vtwsovywJJHoVBxk5_5WUxBV\" alt=\"2-02\" width=\"500\">\n","\n","<br>\n","\n","__One trip scenario__ <br><br>\n","\n","\n","Predictions of remaining driving rime <br><br>\n","\n","One evening, <br><br>\n","\n","$\\begin{align}\n","&\\text{As you leave the office,} &&\\text{you predict it'll take 30 minutes to get home.} \\\\ \\\\\n","&\\text{In about 5 minutes,} &&\\text{you exit the parking lot and notice that it is raining. Traffic is slower ing the rain,} \\\\\n","& &&\\text{so you estimate that it'll take 35 minutes then on to get home.} \\\\\n","&\\text{15 minutes later,} &&\\text{you exit the hightway earlier than you anticipated,} \\\\\n","& &&\\text{and you estimate that it'll take now take you 15 minutes to get home.} \\\\\n","&\\text{30 minutes in your journey,} &&\\text{you get stuck behind a slow truck} \\\\\n","& &&\\text{and predict that it'll take you an additional 10 minutes to get home.} \\\\\n","&\\text{10 minutes later,} &&\\text{you enter the home street} \\\\\n","& &&\\text{from where it usually takes 3 minutes to arrive home.} \\\\\n","&\\text{While in 3 minutes later} &&\\text{you are home.}\n","\\end{align}$\n","\n","<br><br>\n","\n","\n","\n","<br><br><br>\n","\n","\n","__Specifying rewards for the problem__ <br><br>\n","\n","The numbers in the circles represent our predictions of remaining driving time. <br>\n","\" How can we __improve__ these __predictions__ ? \" <br><br>\n","\n","To undrestand how to update our predictions, <br>\n","$\\qquad$ We need to specify the reward. <br>\n","$\\rightarrow \\quad$ Let's use the amount of time take <br><br>\n","\n","\n","Reward labeling <br>\n","$R_1 = 5 \\qquad : \\;$ It took you 5 minutes to exit the parking lot <br>\n","$R_2 = 15 \\quad \\;\\; : \\;$ it took you 15 minutes to exit the highway <br>\n","$R_3 = 10 \\quad \\;\\; : \\;$ <br>\n","$R_4 = 10 \\quad \\;\\; : \\;$ <br>\n","$R_5 = 3 \\qquad : \\;$ <br>\n","\n","\n","<br><br>\n","\n","Prediction value and True value <br>\n",">You predicted that it would take you 30 minutes to get home, <br>\n",">but it actually took you 43 minutes. <br>\n",">( Perhaps you can learn from this experience )\n","\n","\n","\n","<br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-af9UioPp4ZR","colab_type":"text"},"source":["### __Monte-Carlo method__ for example <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1R_iSXuRBo2Qv4OQ0cUeqtVjo5csMaR1K\" alt=\"2-03\" width=\"500\">\n","\n","\n","Let's first look at a Monte-Carlo method. <br>\n","In particular, we will use the constant alpha Monte-Carlo method, with $\\; \\alpha = 1 \\;$. <br><br>\n","\n","\n","[ 02:50 ] <br><br>\n","\n","\n","\n","In Monte-Carlo methods, <br>\n","You update your estimates towards the return $G_t$ <br>\n","which is only available at the end of the episode. <br>\n",">( We can only update the estimates for each of the states once we arrive home ) \n","\n","<br><br><br>\n","\n","\n","1 | 2\n","--- | ---\n","leave | <img src=\"https://drive.google.com/uc?id=1ciPA_hR6dTnHgEugVee6-tYxMXn95nGT\" alt=\"2-04\" width=\"500\">\n","exit | <img src=\"https://drive.google.com/uc?id=1XidGCgWUKAmZg-P8sqWVYHMGB_o5tFvC\" alt=\"2-05\" width=\"500\">\n","exit highway | <img src=\"https://drive.google.com/uc?id=1YaiWzPG86Xtv4V6FKzoKOofBrJRVNbe8\" alt=\"2-06\" width=\"500\">\n","secondary road | <img src=\"https://drive.google.com/uc?id=1PnzrfWK71EDl6DXLXEvKGnK1Zs22vliy\" alt=\"2-07\" width=\"500\">\n","enter home street | <img src=\"https://drive.google.com/uc?id=1Mcun14HBaXh6IAz4WrZ802VqLm75j8lB\" alt=\"2-08\" width=\"500\">\n","\n","<br>\n","\n","We can start from when we leave the office. <br><br>\n","\n","$G_0 = 43$ <br>\n","From the office, it took you a toal of 43 minutes to get home. <br>\n","Therefore, the return at time 0 ($G_0$) is 43. <br><br>\n","\n",">As $\\alpha = 1$, <br>\n",">we move our estimate completely towards the actual return ! <br>\n","\n","<br>\n","\n","$G_1 = 38$ <br>\n","From when you exit the parking lot, it took you 38 minutes to get home. <br>\n","and we update our estimate accordingly. <br><br>\n","\n","$G_2 = 23$ <br><br>\n","\n","$G_3 = 13$ <br><br>\n","\n","$G_4 = 3$ <br><br>\n","\n","We update the estimates for the remaining states in the same way ... \n","\n","<br><br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1vr2kXgjqzzj447m4BrZnfSuBxb5nHHzq\" alt=\"2-09\" width=\"500\">\n","\n","<br>\n","\n","But is it really necessary to wait until the final outcome is known before learning can begin. \n","\n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"h6YdajdYzwXF","colab_type":"text"},"source":["### __Temporal Difference method__ for example <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1SUYboZKtoiGpZVk8ompsAfAOCXZi2UlL\" alt=\"2-10\" width=\"500\">\n","\n","\" How we would make updates with TD ? \" <br><br>\n","\n","\n","1 | 2\n","--- | ---\n","leave | <img src=\"https://drive.google.com/uc?id=1GKVrB8q8JpTB50pshF0nBoOjZQHmfVv2\" alt=\"2-11\" width=\"500\">\n","exit | <img src=\"https://drive.google.com/uc?id=1Gi0bOAK9Gf0ftnvQ_ElDoc-4a_h9yOsU\" alt=\"2-12\" width=\"500\">\n","exit highway | <img src=\"https://drive.google.com/uc?id=12fI8tH61vBddaWYono0FTuEKXOCNJRlB\" alt=\"2-13\" width=\"500\">\n","secondary road | <img src=\"https://drive.google.com/uc?id=13bTdgzXaNluRgOMpOVgzhFJ-JFsnMe32\" alt=\"2-14\" width=\"500\">\n","enter home street | <img src=\"https://drive.google.com/uc?id=16n2OkXu3X2mCP-6ALI_TEW7IQIu1u0br\" alt=\"2-15\" width=\"500\">\n","\n","<br>\n","\n","$S_1 \\quad$ (leave state) <br><br>\n","\n","From the office, it took you 5 minutes to exit the parking lot <br>\n","from where you made a prediction of 35 minutes. <br>\n","From the exit state, you can update the estimate for office state. <br>\n","Your current estimate from the office state is 30 minutes. You get a reward $R_1$ of 5. the value of the next state $V(\\text{exit})$ is 35. <br>\n","So we update our estimate to 40 minutes. <br><br>\n","\n","\n","$S_2 \\quad$ (exit state) <br><br>\n","\n","Your current estimate is 35 minutes, it took you 15 minutes from the parking lot to exit the highway. <br>\n","The estimated time to get home from when you exit the highway is also 15 minutes. <br>\n","So we reduce the estimated time to go from when we exit the parking lot to 30 minutes. <br><br>\n","\n",">This makes sense because we exited the highway earlier than expected. \n","\n","<br>\n","\n","$S_3 \\quad$ (secondary road) <br><br>\n","\n","$S_4 \\quad$ (enter home street) <br><br>\n","\n","$S_5 \\quad$ (arrive home) <br><br>\n","\n","[ 04:20 ~ ] <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1YwXeP_tBDWcjtzH4RRjx2r8RzBX2EaS-\" alt=\"2-16\" width=\"500\">\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lM2zhcJL__Ft","colab_type":"text"},"source":["### $\\cdot$ Comparing TD and Monte-Carlo <br><br>\n","\n","  - Identify the empirical benefits of Temporal-Difference learning <br><br>\n","\n","\n","<br>\n","\n",">In the last few video, we introduced TD and discussed it's advantages over Dynamic Programming and Monte-Carlo. <br>\n",">In this video, we'll dive deeper and compare TD and MC in a carefully constructed scientific experiment.\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EjvCTkPrA7fU","colab_type":"text"},"source":["### Example : Random Walk <br><br>\n","\n","Problem | Labeling true values of states\n","--- | ---\n","<img src= \"https://drive.google.com/uc?id=1qgL2_1-4TD9BfnTwtvUd-TBYwyO3kb9o\" alt=\"2-01\" width=\"500\"> | <img src= \"https://drive.google.com/uc?id=1i8r7UIV148axj5UGNBBBQ-7aiVMQMdXq\" alt=\"2-02\" width=\"500\">\n","\n","<br>\n","\n","Consider the simple MDP : Random Walk <br><br>\n","\n","5 non-terminal state $(A, B, ... , E)$ <br>\n","2 terminal state <br>\n","2 deterministic actions $(\\text{left, right})$ <br>\n","policy : uniform random distribution <br>\n","Reward : $+0$ on all transitions, except $+1$ for terminating on the right <br>\n","discount factor : $\\gamma = 1$ <br><br>\n","\n","\n","All episodes start in state $C$, <br>\n","episodes terminate either on the left or on the right. <br><br>\n","\n","We want to estimate it's value function <br><br><br>\n","\n","\n","\n",">In this problem, <br>\n",">the value has an intuitive meaning. The value of each state is the probability terminating on the right when starting from each state. The value of the start state at $C$ is 0.5, that means the probability of terminating from the center is a half (1/2). Randomly wandering from the center has a 50-50 chance of terminating on either side. That makes sense. \n","\n","\n","\n","<br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CmswYUb8rTLi","colab_type":"text"},"source":["### Comparing True-values / TD estimates / MC estimates <br><br>\n","\n","\n","We have labeled the MDP with the true values at the top of the picture. <br>\n","The agent estimates the value function using TD at the middle <br>\n","And he estimates the value function using MC at the bottom <br><br>\n","\n","\n","Let's initialize the approximate value function for both agent to 0.5 (for all states ?) <br>\n","\n","<br><br><br>\n","\n","\n","\n","#### One-episode comparison (of performance) <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1umukoXNNg_tUpnnxPNjWy3qSfxRb9V6C\" alt=\"01\" width=\"1000\">\n","\n","<br><br>\n","\n","  - Notice, <br>\n","  the TD agent only updated the value of state $E$. <br>\n","\n","  >To understand this, <br>\n","  >consider the transition from state $C$ to $D$. <br>\n","  >Because of the reward $R_{t+1}$ is 0, The TD error is $\\big[ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\big] = 0 + 0.5 - 0.5$. <br>\n","  >So we make no change to the estimate for $C$. The same thing happens on every step except the transition into this terminal state from state $E$. \n","\n","  <br>\n","\n","  - In contrast, <br>\n","  the MC agent updated thr values of all the states the robot saw during the episode.\n","\n","\n","<br><br><br>\n","\n","\n","\n","\n","#### next episode comparison <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=13dHI--zinZ6MV1sbCuo3kOt6ZVWxQTrO\" alt=\"02\" width=\"1000\">\n","\n","<br><br>\n","\n","Pay attention to how the values are updated during the episdoe. <br><br>\n","\n","  - Eventually, <br>\n","  the TD agent makes updates to the values on every step. <br><br>\n","\n","  - In contrast, <br>\n","  the MC agent waited until the end of the episode before it made it's updates. \n","\n","\n","<br><br><br>\n","\n","\n","\n","#### A few more episodes comparison <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1ql8Aox_UgfSdHNzdzOcx26xQuGVRo3ea\" alt=\"03\" width=\"1000\">\n","\n","<br><br>\n","\n","The value estimates of the TD agent seemed to be moving towards the true values. \n","\n","\n","<br><br><br>\n","\n","\n","\n","#### MC v.s. TD <br><br>\n","\n","\n","Comparing TD and MC <br>\n","on one episode result <br>\n","\n","TD | MC\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=17Wqkgf-m2BRMJYxAGwW6HG5PfHNR7j2h\" alt=\"2-03\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1Lv_rJtbQ2HvoKVzH_tYsXqKMiGufVnfh\" alt=\"2-04\" width=\"500\">\n","\n","\n","<br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Smrj1LxpH7P0","colab_type":"text"},"source":["### Performance by volumes (and step size) <br><br>\n","\n","\n","Let's jump ahead and <br>\n","evaluate the asymptotic performance. <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1uJx-9FvZuLV0h0brpeDG2_pwrhUcbyWX\" alt=\"2-05\" width=\"500\">\n","\n","<br><br>\n","\n","\n","Let's plot the estimated values at several points during learning. <br><br>\n","\n","\n","x-axis $\\quad : \\;$ represents each state in the MDP <br>\n","y-axis $\\quad : \\;$ represents the estimated values <br><br><br>\n","\n","\n","\n","__volume of episodes__ <br><br>\n","\n","\n","True value's are seemed to be linear, <br>\n","the initial value estimates are seemed to be horizontal. <br>\n","( recall, we initialized them to 0.5 ) <br><br>\n","\n","following the agent learns the episodes more and more, \n","from the values learned after the first episode <br>\n","to the values learned after 100 episodes, <br>\n","the estimated values approximate to the true values ! <br><br>\n","\n",">By the end of the first episode, TD only learned the value of a single state. <br>\n",">The estimates after a 100 episodes are about as good as they're ever going to get.\n","\n","<br><br>\n","\n","\n","\n","__Step size__ <br><br>\n","\n","We're using a constant step size of 0.1. This means the values will fluctuate in response to the outcomes of the most recent epsides. <br>\n","If you use a samller learning rate or better yet a decaying learning rate, we might get a better estimate. \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"t_UQpAgVLqXu","colab_type":"text"},"source":["### Performance by MC and TD <br><br>\n","\n","\n","\" Does TD learn faster than MC ? \" <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1RTaJgKwYWt4tkNUkb2RPiN9CIKwH_V52\" alt=\"2-06\" width=\"500\"> \n","\n","<br><br>\n","\n","\n","__Comparing the performance of MC and TD__ <br><br>\n","\n","\n","x-axis $\\quad : \\;$ represents the number of episodes <br>\n","y-axis $\\quad : \\;$ represents the root mean squared error between the value function and the learned estimates <br><br>\n","\n","Each curve is averaged over a 100 independent runs. <br><br>\n","\n","Let's look at TD's performance. <br>\n","We see that TD perform consistently better than MC. <br><br>\n","\n","Let's look a little more closely. <br>\n","Notice, the error reduces faster with a learning rate of $\\alpha=0.15$, but ultimately, results in higher final error. <br>\n","With a smaller learning rate $\\alpha=0.5%$, TD learns more slowly but achieves lower final error. \n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OvkhwTi_OiG2","colab_type":"text"},"source":["### Summary <br><br>\n","\n","  - We rans a careful experiment comparing TD and MC <br><br>\n","\n","  - The results suggest that TD converges faster to a lower final error in this problem\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Dl1QfVfrXseP","colab_type":"text"},"source":["### Andy Barto and Rich Sutton &ensp; :  More on the History of RL <br><br>\n","\n","\n","  1. Difference of R.L. to supervised learning <br>\n","  It's different with Error correction what the Trial and Error says <br><br>\n","\n","\n","  2. Temporal Difference learning from behaviorism <br>\n","  Not just exactly comes from Behavior, But inspired by Behavior's things <br>\n","  ( TD is more about the head (brain?) )\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FXvpZWhBRDO1","colab_type":"text"},"source":["## $\\cdot$ Week 2 Summary <br><br>\n","\n","  - TD with bootstrap\n","  - Tabular $TD(0)$ algorithm\n","  - Advantages of TD\n","\n","\n","<br><br><br><br><br>\n","\n"]}]}
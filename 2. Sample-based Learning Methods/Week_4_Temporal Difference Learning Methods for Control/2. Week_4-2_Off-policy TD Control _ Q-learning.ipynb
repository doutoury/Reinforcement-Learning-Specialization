{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2. Week_4-2_Off-policy TD Control : Q-learning","provenance":[],"private_outputs":true,"collapsed_sections":["39ARKswlLNZg","RGRmEpIu2BSu","jWWPNjwv_ebP","L6tbsSmTDpV2","jR2l0KRhEcyJ","I6LC9AETO_aA"],"authorship_tag":"ABX9TyNqkeZ/CEsjcgOuCz1KdmMU"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"39ARKswlLNZg","colab_type":"text"},"source":["## __2. Off-policy TD Control $\\quad : \\;$ Q-learning__ <br><br>\n","\n","  - What is Q-learning ? <br><br>\n","\n","  - Q-learning in the Windy Grid World <br><br>\n","\n","  - How is Q-learning Off-policy ?\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"SrLl-g65LbKc","colab_type":"text"},"source":["## $\\cdot$ What is Q-learning ? <br><br>\n","\n","\n","  - Describe the Q-learning algorithm <br><br>\n","\n","  - Explain the relationship between Q-laerning and the Bellman optimality equations <br><br><br>\n","\n","\n","Q-learning <br>\n",">Several reason applications of R.L have been built on a single algorithm. <br>\n",">( learning to play Atari games, control traffic signals, or even automatically configuring web systems ) <br>\n",">This algorithm is called Q-learning. \n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RGRmEpIu2BSu","colab_type":"text"},"source":["### __The Q-learning algorithm__ <br><br>\n","\n","Q-learning was developed in 1989 <br>\n","It is one of the first major On-line R.L. algorithms. \n","\n","<br><br><br>\n","\n","\n","### Pseudocode of Q-learning <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1fkOO2VRplVwikOxKnvcXoBTloMlLutQE\" alt=\"2-01\" width=\"500\">\n","\n","<br>\n","\n","\n","Almost like SARSA Control <br><br>\n","\n",">We've seen this format before. <br>\n",">The agent chooses an action in a state (using policy), <br>\n",">takes the action and observes the reward and the next state. <br>\n",">Then the agent does an update. And the cycle repeats. \n","\n","<br><br>\n","\n","\n","What's new about Q-learning ? <br><br>\n","\n","$\\Rightarrow \\quad \\text{Target !} \\quad$ ( the action value update ) <br><br>\n","\n",">The new element in Q-laerning is the action-value update. <br>\n",">The target is the reward + discounted maximum action-value in the following state. <br>\n",">target $\\quad : \\;$ $\\big[ R_{t+1} + \\gamma \\max_a' Q(S', a') - Q(S, A) \\big]$ \n","\n",">This differs from SARSA, <br>\n",">which uses the value of the next state-action pair in it's target <br>\n",">target $\\quad : \\;$ $\\color{gray}{\\big[ R_{t+1} + \\gamma Q(S', A') - Q(S, A) \\big]}$\n","\n","\n","<br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jWWPNjwv_ebP","colab_type":"text"},"source":["### __Revisiting Bellman Equations__ <br><br>\n","\n","\n","Why does Q-learning use a max instead of the next state-action pair ? <br>\n",">This connection goes way back to the Dynamic Programming material \n","\n","<br><br><br>\n","\n","\n","#### SARSA v.s. Q-learning <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1U04mf9FfG1sy9Fll93KgKW6KPj6DPJMu\" alt=\"2-02\" width=\"500\">\n","\n","<br><br>\n","\n","\n","  - __SARSA__ <br><br>\n","\n","  The __update equation__ for SARSA <br>\n","It's suspiciously similar to the Bellman Equation for action-values. <br>\n","In fact, SARSA is a __sample-based algorithm__ to solve the __Bellman Equation for action values__. <br>\n","( using given policy ) <br><br><br>\n","\n","\n","\n","  - __Q-learning__ <br><br>\n","\n","  Q-learning also solve the Bellman Equation __using samples__ from the environment, <br>\n","But, instead of using the standard Bellman Equation, <br>\n","It uses the __Bellman ' OPTIMALITY ' Equation for action values__. <br>\n","( using optimal policy ) <br><br><br>\n","\n","  $\\Rightarrow \\quad$ The Optimality Equations enable Q-learning to directly learn $q_*$ <br>\n","  $\\qquad$ instead of switching between policy improvement and policy evaluation steps. \n","\n","<br><br><br>\n","\n","\n","\n","#### Policy Iteration v.s. Value Iteration\n","\n","<img src=\"https://drive.google.com/uc?id=1sog5-0gGjKaPhrt76YN5ZB_S358vdnu1\" alt=\"2-03\" width=\"500\">\n","\n","<br>\n","\n","Even though SARSA and Q-learning are both based on Bellman Equations, <br>\n","they're based on very different Bellman Equations. <br><br>\n","\n","  - __SARSA__ is sample-based version of __Policy Iteration__ <br>\n","  which uses Bellman Equations for action-values, that each depend on a fixed policy. <br><br>\n","\n","  - __Q-learning__ is a sample-based version of __Value Iteration__ <br>\n","  which iteratively applies the Bellman Optimality Equation. \n","\n","<br><br><br>\n","\n","\n","\n","Applying the Bellman Optimality Equation strictly improves the value function (, unless it is already optimal). <br><br>\n","\n","So value iteration continually improves it's value function estimate $V(S)$, <br>\n","which eventually converges to the optimal solution. <br><br>\n","\n","For the same reason, Q-learning also converges to the optimal value function <br>\n","as long as the agent continues to explore and samples all areas of the state-action space.\n","\n","<br><br><br>\n","\n","\n","Question ...\n",">__Policy Iteration__ <br>\n",">SARSA update 의 경우, policy evaluation 과정 중 전 단계에서 결정된 specific policy 를 적용한 뒤 <br>\n",">policy improvement 과정에 $\\epsilon$-greedy policy 를 적용 <br><br>\n",">\n",">__Value Iteration__ <br>\n",">Q-learning 의 경우, policy evaluation 과정 중 현 단계에서 가능한 optimal policy 를 적용한 뒤 <br>\n",">policy improvement 과정에 $\\epsilon$-greedy policy 를 적용 (???) <br>\n",">( 아니면 아예 policy improvement 과정을 대체하여 생략하나 ? )\n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"L6tbsSmTDpV2","colab_type":"text"},"source":["### Summary <br><br>\n","\n","  - Introduced the Q-learning algorithm <br><br>\n","\n","  - Described the connection to the Bellman Optimality Equations \n","\n","\n","<br><br><br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1Ypho79fD_wO","colab_type":"text"},"source":["## $\\cdot$ Q-learning in the Windy Grid World <br><br>\n","\n","  - Understand how Q-learning operates (performs) in an example MDP <br><br>\n","\n","  - Gain experience (insight) comparing the performance of multiple learning algorithm on a simple MDP <br>\n","  ( especially between Q-learning and SARSA ) <br><br><br>\n","\n","\n",">Intuition for why Q-learning works well \n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n"," "]},{"cell_type":"markdown","metadata":{"id":"jR2l0KRhEcyJ","colab_type":"text"},"source":["### Comparison <br><br>\n","\n","Windy Grid World <br><br>\n","\n","It's an undiscounted grid world with wind that blows upwards as it moves through the windy states \n","\n","<br><br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1KS0l_Z6Zh6sHALOeIDI7yYORaKw64EpJ\" alt=\"2-04\" width=\"500\">\n","\n","Recall SARSA learns slowly for the first couple of episodes. <br>\n","Then, it improved exponentially before plateauing. \n","\n","<br><br><br>\n","\n","\n","\n","Using SARSA learning <br><br>\n","\n",">It's value function takes into account it's Epsilon-greedy behavior. \n","\n","<br><br>\n","\n","\n","Using Q-learning <br><br>\n","\n",">It derectly learns the optimal policie's value function.\n","\n","<br><br><br>\n","\n","\n","\n","The results of comparison <br><br>\n","\n","In the beginning, the two algorithms learn at a similar pace. <br>\n","Towards the end, Q-learning seems to learn a better final policy. \n","\n","<br><br><br>\n","\n","\n","\n","Why is Q-learning doing so much better here ? <br>\n",">Without conducting more target experiments, <br>\n","we cannot know for sure. Perhaps the update target of Q-learning is more stable.\n","\n","<br>\n","\n","Q-learning takes the max over next action values. <br>\n","So it only changes when the agent learns that one action is better than another. <br><br>\n","\n","In contrast, SARSA uses the estimate of the next action value in it's target. <br>\n","This changes every time the agent takes an exploratory action. \n","\n","<br><br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1rDkTDsORaEIyTRMGCYRJFfd_P0NMGAwx\" alt=\"2-05\" width=\"500\">\n","\n","\n","Perhaps, you could design an experiment to get to the bottom of this. (진짜 이유를 알아내기 위해)\n","\n","<br><br>\n","\n","\n","How can we make SARSA perform better ? <br>\n","A step size parameter value of 0.5 might be a bit high for this experiment. <br>\n","These large updates might be causing SARSA trouble when the agent takes exploratory actions. <br>\n","SARSA's final policy certainly isn't optimal. <br><br>\n","\n","\n","Let's run the experiment a bit longer, <br>\n","with the alpha of 0.1 <br><br>\n","\n",">We expect SARSA to learn more slowly with the smaller alpha, <br>\n",">but find a better policy. \n","\n","<br>\n","\n","SARSA learns the same final policy as Q-learning ! <br>\n","but more slowly. <br><br>\n","\n","We know both algorithms have to converged to the same policy <br>\n","because the slopes of the lines are equal. <br>\n","Equal slopes mean that both agents are completing episodes at the same rate. <br><br><br>\n","\n","\n","This experiment also highlights the impact of parameter choices in R.L. <br>\n","$\\alpha, \\; \\epsilon, \\; \\text{initial values}$, and the length of the experiment can all influence the final result. \n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"I6LC9AETO_aA","colab_type":"text"},"source":["### Summary <br><br>\n","\n","  - Showed an example of Q-learning in an MDP <br><br>\n","\n","  - Campared Q-learning and SARSA to understand their behaviour during the experiment <br>\n","  ( Gain insight into the differences between two )\n","\n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"19FmDyDzK5vw"},"source":["## $\\cdot$ How is Q-learning Off-policy ? <br><br>\n","\n","  - Understand how Q-learning can be off-policy without using __importance sampling__ <br><br>\n","\n","  - Describe how learning on-policy or off-policy might affect performance in control \n","\n","<br><br>\n","\n","\n","Q-learning is an __off-policy__ algorithm. <br><br>\n","\n","We've only seen off-policy algorithms that use __importance sampling__. <br>\n","How can Q-learning be off-policy __without__ using __importance sampling__ ?\n","\n","<br><br><br><br><br>\n","\n","\n"," "]},{"cell_type":"markdown","metadata":{"id":"jvA3FTY2kn7N","colab_type":"text"},"source":["### On-policy v.s. Off-policy $\\quad$ ( SARSA v.s. Q-learning ) <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1kFwxMo-6r5u9HRByAnTvkg3p3D_CboUw\" alt=\"2-06\" width=\"500\">\n","\n","<br>\n","\n","#### Recall <br>\n","$\\qquad$ an agent __estimates__ it's value function __according to__ expected returns under their __target policy__. <br>\n","$\\qquad$ They actually __behave__ (doing action) __according to__ their __behavior policy__. <br><br>\n","\n","\n","$\\qquad$ $\\text{Target policy} = \\text{Behavior policy} \\quad : \\;$ The agent is learning __on-policy__ <br>\n","$\\qquad$ $\\text{Target policy} \\not= \\text{Behavior policy} \\quad : \\;$ The agent is learning __off-policy__ \n","\n","<br><br><br>\n","\n","\n","\n","  - __SARSA__ is an __on-policy algorithm__ <br>\n","  The agent bootstraps off of the value of the action it's going to take next, <br>\n","  which is sampled from it's behavior policy <br><br>\n","\n","\n","  - __Q-learning__ is an __off-policy algorithm__ <br>\n","  The agent bootstraps off of the largest action value in it's next state <br>\n","  This is like sampling an action under an estimate of the optimal policy (rather than behavior policy)\n","\n","<br><br>\n","\n","\n",">Since Q-learning learns about the best action it could possibly take rather than the actions it actually takes, <br>\n",">it is learning off-policy !\n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"ETe_mR9cCxg4","colab_type":"text"},"source":["#### __\" What are the target policy and behavior policy ? \"__ <br>\n",">It's a natural question to ask whenever you see a R.L. algorithm \n","\n","<br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=18Zi8W7Bor7v13h7QKmuUdeCEeA-bLGqp\" alt=\"2-07\" width=\"500\">\n","\n","<br>\n","\n","Q-learning's <br>\n","$\\qquad$ target policy is always greedy with respect to it's current values. <br>\n","$\\qquad$ behvior policy can be anything that continues to visit all state-action pairs during learning. ( one possible policy is $\\epsilon - greedy$ ) \n","\n","<br>\n","\n","$\\Rightarrow \\quad$ The difference here between the target policy and behavior policy confirms that Q-learning is off-policy !\n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"wkiRul5dE8P_","colab_type":"text"},"source":["#### \" Why don't we see any importance sampling ratio ( on Q-learning ) ? \" <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=18OV1cIBLci4cy8z7h2ZMvdLLstdwhHFR\" alt=\"2-08\" width=\"500\">\n","\n","<br>\n","\n","In general, <br>\n","Off-policy learning need an __importance sampling ratio__ ! <br><br>\n","\n","In Q-learning, <Br>\n","Off-policy learning need __no__ importance sampling ratio ! \n","\n","<br><br><br>\n","\n","\n","\n","It is because the agent is estimating action values with unknown policy. <br>\n","It does not need importance sampling ratios to correct for the difference in action selection.\n","\n","<br><br><br>\n","\n","\n","The action value function $Q(S_{t+1}, a')$ represents the returns ($-3, 4, 6$) following each action $\\sim \\pi(a'|S_{t+1})$ in a given state $S_{t+1}$. <br>\n","The agent's target policy $\\pi(a'|S_{t+1})$ represents the probability of taking each action in a given state. \n","\n","<br><br>\n","\n","Putting these two elements together, <br>\n","the agent can calculate the expected return under it's target policy from any given state ( in particular, the next state $S_{t+1}$ ) <br><br>\n","\n","$\\rightarrow \\quad$ $\\displaystyle \\sum_{a'} \\pi(a'|S_{t+1}) Q(S_{t+1}, a') = \\mathbb{E}_{\\pi} [G_{t+1}|S_{t+1}]$\n","\n","<br><br><br>\n","\n","\n","Q-learning uses exactly this technique to learn off-policy. <br><br>\n","\n","Since the agent's target policy is greedy with repect to it's action values, <br>\n","all non-maximum actions have probability 0 ! <br><br>\n","\n","As a result, <br>\n","the expected return from that state = a maximal action value from that state ! \n","\n","\n","<br><br><br>\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pg0v7YzwM6I2","colab_type":"text"},"source":["### Subtleties of Q-learning <br><br>\n","\n","#### __\" Q-learning doesn't iterate between policy evaluation and policy improvement ! \"__ <br>\n","$\\Rightarrow \\quad$ __It learns the optimal values directly !__  <br><br>\n","\n","\n","$\\qquad$ Directly learning the optimal value function and policy sounds great, <br>\n","$\\qquad$ But there are some subtleties that makes it less desirable in specific situations ! <br><br><br>\n","\n","\n","\n",">let's look at an example\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qehoVRwqJZI1","colab_type":"text"},"source":["### Example $\\quad : \\;$ The cliff walking environment <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=18yXC7kYRcRUigAyMN7J19l5jO-c3jSrp\" alt=\"2-09\" width=\"500\">\n","\n","<br>\n","\n","\n","\n","\\\n","\n","\n","$\\begin{align} &\\gamma &&: 1 \\\\\n","&\\text{Reward on almost grids} &&: -1 \\\\\n","&\\text{Reward on the cliff} &&: -100 \\\\\n","&\\epsilon &&: 0.1 \\end{align}$ \n","\n","<br><br><br>\n","\n","\n","\n","#### Compare Q-learning and SARSA $\\quad$ ( with $\\epsilon = 0.1$ ) <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1U5-bt_yKLPRPEMD-Rq0JmwQ-iD7e8XLE\" alt=\"2-10\" width=\"500\">\n","\n","<br>\n","\n","  - The average performance of Q-learning <br><br>\n","\n","  Since Q-learning leans the optimal value function, <br>\n","it quickly learns that an optimal policy travels right alongside the cliff. <br><br>\n","\n","  However, since his actions are $\\epsilon - greedy$, <br>\n","traveling alongside the cliff accasionally results in falling off of the cliff ! <br><br><br>\n","\n","\n","\n","  - The average performance of SARSA <br><br>\n","\n","  SARSA learns about his current policy, <br>\n","  taking into account the effect of $\\epsilon - greedy$ action selection ! <br><br>\n","\n","  Accounting for occasional exploratory actions, it learns to take the longer but more reliable path. <br>\n","  They usually avoids randomly falling into the cliff ! <br><br>\n","\n","\n","$\\rightarrow \\quad$ Because of it's safer path, SARSA is able to reach the goal more reliably ! $\\quad$ ( in this case )\n","\n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zK3ltvsHRIEZ","colab_type":"text"},"source":["### Summary <br><br>\n","\n","  - Q-learning is off-policy without using importance sampling <br><br>\n","  \n","  - Learning on-policy or off-policy may perform differently in control ( depending on the task ! )\n","\n","\n","<br><br><br>\n","\n","\n"]}]}
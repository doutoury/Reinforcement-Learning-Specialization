{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2. Week_4-1_TD for Control","provenance":[],"private_outputs":true,"collapsed_sections":["39ARKswlLNZg","cX4b85nuOaL4","1CzDuS1PVZua","H059xP8he-Vi","xpGCcGoNnIZv","tlKnx3Efzfz7"],"authorship_tag":"ABX9TyNLHmSwxcU819DJW6injVzO"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yydQ9qjsKBRW","colab_type":"text"},"source":["# Week_4 <br>\n","\n","INDEX <br><br>\n","\n","\n","  1. TD for Control <br>\n","    - SARSA $\\quad : \\;$ GPI (Generalized Policy Iteration) with TD <br>\n","    - SARSA in the Windy Grid World <br><br>\n","  \n","  2. Off-policy TD Control $\\quad : \\;$ Q-learning <br>\n","    - What is Q-learning ? <br>\n","    - Q-learning in the Windy Grid World <br>\n","    - How is Q-learning Off-policy ? <br><br>\n","  \n","  3. Expected SARSA <br>\n","    - Expected SARSA <br>\n","    - Expected SARSA in the Cliff World <br>\n","    - Generality of Expected SARSA\n","\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"39ARKswlLNZg","colab_type":"text"},"source":["## __1. TD for Control__ <br><br>\n","\n","\n","  - SARSA $\\quad : \\;$ GPI (Generalized Policy Iteration) with TD <br><br>\n","\n","  - SARSA in the Windy Grid World\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"SrLl-g65LbKc","colab_type":"text"},"source":["## $\\cdot$ SARSA $\\quad : \\;$ GPI (Generalized Policy Iteration) with TD <br><br>\n","\n","\n","  - Explain ' how __GPI__ can be used __with TD__ to find __improved policies__ ' <br>\n","  ( as well as describe the SARSA control algorithm )\n","<br><br><br>\n","\n","\n","\n",">In previous videos, we talked about using Generalized Policy Iteration (GPI) to find an optimal policy. <br>\n",">We've also talked about using Temporal Difference Learning (TD) to estimate value functions. <br><br>\n",">\n",">\" What would it look like if we use __TD__ to do the __policy evaluation step__ in __GPI__ (Generalized Policy Iteration) ? \"\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cX4b85nuOaL4","colab_type":"text"},"source":["### Recall $\\quad : \\;$ __Generalized Policy Iteration__ <br><br>\n","\n","\n","Generalized Policy Iteration combines 2 parts <br><br>\n","\n","  - Policy evaluation <br>\n","  $\\downarrow \\qquad \\qquad \\quad \\uparrow$\n","  - Policy improvement\n","\n","<br><br><br>\n","\n","\n","\n","\n","### GPI comparison <br><br>\n","\n","\n","  1. #### __GPI $\\quad$ ( with DP ? )__ $\\qquad : \\;$ update after whole one game $\\quad$ ( or after convergence ) <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1JE20fftXMkhsTlXZW92gaC1NANfASQZ3\" alt=\"1-01\" width=\"500\">\n","\n","<br>\n","\n","Policy Iteration $\\quad : \\quad$ First form of GPI we saw <br>\n","It runs policy evaluation to convergence before greedifying the policy. <br>\n","Then it runs policy improvement with greedification. <br><br>\n","\n","\n","( 내 정리 ) <br>\n","GPI with DP, <br>\n","it evaluates all state-value in full trajectories of every possible episode, <br>\n","then greedifies the policy with choosing the actions to transfer agent to the highest value's state in the possible actions. \n","\n","<br><br><br>\n","\n","\n","\n","  2. #### __GPI with MC__ $\\qquad : \\;$ update after whole one episode <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1jNj1CYnigvBlAqekUHKn7nODkgRY5o0A\" alt=\"1-02\" width=\"500\">\n","\n","<br>\n","\n","Iteration in each episode <br>\n","It performs a cycle of policy evaluation and policy improvement every episode. <br><br>\n","\n","example <br>\n",">The mouse starts out knowing nothing and follows a random policy. <br>\n",">Eventually the mouse will stumble into the cheese (goal) just by moving randomly. <br>\n",">At that point, the mouse updates it's action-values. <br>\n",">Then it improves it's policy by greedifying with repext to it's action-values. <br>\n",">As the process repeats, it will eventually learn the optimal policy. \n","\n","<br>\n","\n","__Notice__ <br>\n","GPI with MC does not perform a full policy evaluation step before improvement. <br>\n","$\\rightarrow \\quad$ Rather, it evaluates and improves __after each episode__\n","\n","\n","<br><br><br>\n","\n","\n","  3. #### __GPI with TD__ $\\quad$ ( __SARSA prediction__ ) $\\qquad : \\;$ update after just one step <br><br>\n","\n","\n","transition from state to state <br> and learn the value of each state | transition from state-action pair to state-action pair <br> and learn the value of each state-action pair\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1A79VozxTPB3qDdyFKztTNRoLfO2S63qE\" alt=\"1-03\" width=\"400\"> | <img src=\"https://drive.google.com/uc?id=1rZMkgcvy3z_gnHalAkoVqjAwU_tyY_-8\" alt=\"1-04\" width=\"400\">\n","\n","\n","<br>\n","\n","Iteration in each step <br>\n","With TD, going even further, <br>\n","we could improve the policy __after just one policy evaluation step__. <br><br>\n","\n","To use TD within GPI, <br>\n","we need to learn an action-value function. <br><br>\n","\n","So we need to look at slightly different version of TD than you've seen in the past. <br>\n","Instead of looking at transitions from state $s_t$ to state $s_{t+1}$, and learning the value of each state ($v(s)$), <br>\n","let's look at transitions from state-action pair ($(s_t, a_t)$) to state-action pair ($(s_{t+1}, a_{t+1})$), and learning the value of each pair ($q(s, a)$). <br><br>\n","\n","\n","This algorithm is called __\" SARSA prediction \"__. \n","\n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"1CzDuS1PVZua","colab_type":"text"},"source":["### __SARSA algorithm__ <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1mpyvPI7TDoBeNnlAR27vy4yeWs5_2URC\" alt=\"1-05\" width=\"400\">\n","\n","<br><br>\n","\n","\n","The SARSA acronym describes the data used in the updates <br>\n","$S_t, \\; A_t, \\; R_{t+1}, \\; S_{t+1}, \\; A_{t+1}$ <br><br>\n","\n","SARSA makes predictions about the values of state-action pairs. <br><br>\n","\n"," \n","$\\qquad$ | <img src=\"https://drive.google.com/uc?id=1mpyvPI7TDoBeNnlAR27vy4yeWs5_2URC\" alt=\"1-05\" width=\"300\">\n","---| ---\n","The agent chooses an action , in the initial state to creat the first state-action pair $(S_t, A_t)$ | <img src=\"https://drive.google.com/uc?id=114cP_1tuVhRv-gegiGH2cL8MCJdcMm8s\" alt=\"1-06\" width=\"400\"> \n","Next, <br> it takes that action in the current state, and observes the reward $R_{t+1}$ and the next state $S_{t+1}$ | <img src=\"https://drive.google.com/uc?id=1xwA-qMbecKRITdiolRoxeucIkQ698eT6\" alt=\"1-07\" width=\"400\"> \n","In SARSA, <br> the agent needs to know it's next state-action pair $(S_{t+1}, A_{t+1})$ before updating it's value estimates. <br> That means it has to commit to it's next action $A_{t+1} \\sim \\pi$ before the update ( following the policy $\\pi$ ) | <img src=\"https://drive.google.com/uc?id=17_uQGD_ENcopR_GBBlHe2dx_qOzXrKqg\" alt=\"1-08-2\" width=\"400\">\n","\n","<br><br>\n","\n","\n","$\\quad A_{t+1} \\sim \\pi$ <br><br>\n","\n","$\\quad$ Since our agent is learning action values for a specific policy, <br>\n","$\\quad$ it uses that policy $\\pi$ to sample the next action $A_{t+1}$. \n","\n","<br><br><br>\n","\n","\n","\n","\n","\n","#### Update equation of SARSA algorithm <br><br>\n","\n","\n","Here's the full update equation. <br><br>\n","\n","\n","$Q(S_t, A_t) \\quad \\leftarrow \\quad Q(S_t, A_t) + \\alpha \\big( R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\big)$ <br><br><br>\n","\n","\n",">It actually looks quite similar to the TD update for state-values. <br>\n",">( With state-values $V(S_t)$ replaced by action-values $Q(S_t)$ ) <br><br>\n",">\n",">$\\color{gray}{V(S_t) \\quad \\leftarrow \\quad V(S_t) + \\alpha \\big( R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\big)}$\n","\n","<br><br><br>\n","\n","\n","\n","\n","evaluation | improvement\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1lAZjmydJt4LlHc5NpLuHCosqcjaBIEIe\" alt=\"1-09\" width=\"400\"> | <img src=\"https://drive.google.com/uc?id=1tmUdwHAuTuDbBsoOdEx8V9W3D2Wbugag\" alt=\"1-10\" width=\"400\">\n","\n","<br><br>\n","\n","\n","#### Policy Evaluation with SARSA <br><br>\n","\n","This algorithm is for policy evaluation. <br>\n","It learns action values $A_{t+1}$ for a specific fixed policy $\\pi$. <br><br><br>\n","\n","\n","\n","#### Policy Improvement with SARSA <br><br>\n","\n","Thanks the GPI framework, <br>\n","It can be turned into a control algorithm. ( with following $\\epsilon - greedy$ )\n","\n","<br><br><br>\n","\n","\n","This completes the description of SARSA. <br>\n","The GPI algorithm that uses TD for policy evaluation. <br>\n",">This time, we did improve the policy __every time step__ (with TD) <br>\n",">rather than after an episode like MC (or after convergence like DP)\n","\n","<br><br><br><br><br>\n"]},{"cell_type":"markdown","metadata":{"id":"H059xP8he-Vi","colab_type":"text"},"source":["### Summary <br><br>\n","\n","  - We can combine Generalized Policy Iteration with TD learning to find Improved policies <br><br>\n","\n","  - SARSA is an action-value form of TD which combines these ideas <br>\n","  ( SARSA control is an example of GPI with TD learning )\n","\n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NxXEOlUpjixX"},"source":["## $\\cdot$ SARSA in the Windy Grid World <br><br>\n","\n","\n","  - Understand how the SARSA control algorithm operates in an example MDP <br><br>\n","\n","  - Gain experience analyzing the performance of a learning algorithm\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"V-pdGZ9PkAca","colab_type":"text"},"source":["### Example of SARSA control $\\quad : \\;$ the Windy Grid World <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=14xS3-bYTHlqpbSopFRX-S4QrAd2i_R5x\" alt=\"1-11\" width=\"500\">\n","\n","<br>\n","\n","The Windy Grid World <br>\n","A gridworld with a twist by the wind blowing the agent upwards when moving out of certain states <br>\n","Single start state and terminal state <br><br>\n","\n","Agent $\\qquad \\;\\;\\; : \\;$ moving in the usual 4 direction <br>\n","Reward $\\qquad \\; : \\;$ $-1$ per step $\\qquad$ ( this motivates the agent to escape as fast as possible ) <br>\n","Gamma $\\qquad : \\;$ discount factor $\\gamma = 1$ $\\qquad$ ( since it's an episodic task ) <br>\n","Condition $\\quad \\; : \\;$ the wind strength in each state depends on the column <br><br>\n","\n","(Moving into a boundary does nothing)\n","\n","<br><br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1Ne8LwGY5JAwv3DKdz45miWg_fpBbBGWR\" alt=\"1-12\" width=\"500\"> <br>\n","<img src=\"https://drive.google.com/uc?id=1URFOvDg2vzFS3CK468nBneLYsl-nL3Sf\" alt=\"1-13\" width=\"500\">\n","\n","<br>\n","\n","Applying SARSA in to the Windy Grid World task <br>\n","using $\\epsilon - greedy$ action selection <br><br>\n","\n","\n","$\\begin{align} &\\epsilon &&= 0.1 &&\\text{( we take a random action 1 in every 10 steps )}\\\\\n","&\\alpha &&= 0.5 \\\\\n","&v_0 &&= 0 &&\\text{( initialized value ... being optimistic encouraging systematic exploration )} \\end{align}$ <br><br>\n","\n","This plot show the total number of episodes completed after each time step. <br>\n",">The results are averaged over a hundred runs.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xpGCcGoNnIZv","colab_type":"text"},"source":["\n","\n","\n","\n","\n","  1. #### Applying SARSA <br><br>\n","\n","Notice that the first few episodes take a coupe thousand steps to complete. <br>\n","The curve gradually steeper indicating that episodes are completed more quickly. <br><br>\n","\n","Notice the episode completion rate stops increasing. <br>\n","$\\rightarrow \\quad$ Around 7,000 steps, the greedy policy stops improving <br>\n","$\\rightarrow \\quad$ This means the agent's policy hovers around the optimal policy <br>\n","$\\rightarrow \\quad$ It woun't be exactly optimal, because of exploration \n","\n","<br><br><br>\n","\n","\n","\n","  2. #### Appplying Monte-Carlo $\\quad$ ( compared to SARSA ) <br><br>\n","\n","\n","Notice that Monte-Carlo methods would not be a great fit here. <br>\n","$\\rightarrow \\quad$ This is because many policies do not lead to termination. <br>\n","$\\rightarrow \\quad$ Monte-Carlo methods only learn when an episode terminates. <br>\n","$\\rightarrow \\quad$ So a deterministic policy might get trapped and never learn a good policy in this grid world. <br>\n","\n",">For example, <br>\n",">if the policy took the left action in the start state, it would never terminate. \n","\n","<br><br><br>\n","\n","\n","\n","#### __SARSA avoids this trap !__ <br><br>\n","\n","$\\Rightarrow$ Becaues it would learn such policies are bad during the episode <br>\n","$\\Rightarrow$ So it switch to another policy and not get stuck \n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tlKnx3Efzfz7","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - Showed an example of the SARSA control algorithm in an MDP <br><br>\n","\n","  - Analyzed SARSA's performance to understand the behavior (learning process) during the experience \n","\n","\n","<br><br><br><br><br>\n","\n","\n"]}]}
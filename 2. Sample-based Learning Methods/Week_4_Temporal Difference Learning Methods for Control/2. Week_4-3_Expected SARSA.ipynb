{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2. Week_4-3_Expected SARSA","provenance":[],"private_outputs":true,"collapsed_sections":["39ARKswlLNZg","mo3qHeRq5PUV","_eZhY1jS-pHJ","arloqB7Ei3R0","SpcxDhjCkPQC","xWpqg_82t0ta","B877r80kJZVX","9by25MX5LINg","365ZEt4YN_Te","x9JI04iCTevS","bWSTdsGYT5MW"],"authorship_tag":"ABX9TyOdkumUeZGo9Y/xEI0mzlbn"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"39ARKswlLNZg","colab_type":"text"},"source":["## __3. Expected SARSA__ <br><br>\n","\n","  - Expected SARSA <br><br>\n","  \n","  - Expected SARSA in the Cliff World <br><br>\n","  \n","  - Generality of Expected SARSA\n","\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"SrLl-g65LbKc","colab_type":"text"},"source":["## $\\cdot$ Expected SARSA <br><br>\n","\n","\n","  - Explain the expected SARSA algorithm\n","\n","\n","<br><br>\n","\n",">Two TD control method <br>\n",">  1. SARSA <br>\n",">  2. Q-learning <br>\n",">  3. \" Expected SARSA \" $\\qquad \\leftarrow \\;$ talking this lecture \n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mo3qHeRq5PUV","colab_type":"text"},"source":["### The Expected SARSA algorithm <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1s9B6rdYgwS8c5MhqjaNUYHGkB8Dj6Kvk\" alt=\"3-01\" width=\"500\">\n","\n","<br>\n","\n","Recall <br>\n","Bellman equation for action-values <br><br>\n","\n","Here you can see the expectation over values of possible next state-action pairs. <br><Br>\n","\n","\n","Breaking this expectation apart, <br>\n","We see a sum over possible next states as well as next actions. \n","\n","<br><br><br>\n","\n","\n","\n","#### SARSA <br><br>\n","\n","Target value : $\\qquad Q(S_{t+1}, A_{t+1})$ <br><br>\n","\n","$\\rightarrow \\quad$ SARSA estimates this expectation by sampling the next state from the environment and the next action from it's policy. <br><br>\n","\n","$\\qquad S_{t+1} \\sim p(s',r|s,a) \\qquad , A_{t+1} \\sim \\pi(a'|s')$ <br><br>\n","\n","$... \\;\\;\\;$ the agent already knows it's policy $\\pi$ ! <br>\n","$\\qquad$ So Why should it have to sample it's next action ? <br>\n","$\\qquad$ Do not have to sample it's next action from $\\pi$ <br>\n","$\\qquad$ ( Instead just compute the expectation directly ! )\n","\n","\n","<br><br><br>\n","\n","\n","\n","#### Expected SARSA <br><br>\n","\n","Target value : $\\qquad \\displaystyle \\sum_{a'} \\pi (a'|S_{t+1}) Q(S_{t+1}, a')$ <br><br>\n","\n","$\\rightarrow \\quad$ the agent already knows it's policy $\\pi$ ! <br>\n","$\\qquad$ So just compute the expectation directly ! <br>\n","$\\qquad$ Instead of sampling it's next action from $\\pi$ <br>\n","$\\qquad$ ( like the bellman equation for action-value ! ) <br><br>\n","\n","$\\rightarrow \\quad$ In this case, <br>\n","$\\qquad$ take a weighted sum of the values of all possible next actions ( for $\\mathbb{E}$ ). <br>\n","$\\qquad$ The weights are the probability of taking each action under the agent's policy $\\pi$.\n","\n","\n","<br><br><br>\n","\n","\n","\n",">SARSA <br>\n",">policy $\\pi$ 를 모를 경우, sampling 을 통해 estimate 하므로 Random variable $A_{t+1}$ 로 표기하지만, <br><br>\n",">\n",">Expected SARSA <br>\n",">policy $\\pi$ 를 아는 경우, sampling 을 할 필요 없이 distribution $\\pi$ 의 probability 를 따르는 argument $a'$ 로 표기하여 expectation $\\mathbb{E}$ 을 계산한다 (?) $\\qquad ...$ action-value 의 Bellman equation 처럼 ! \n","\n","\n","\n","<br><br><br>\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_eZhY1jS-pHJ","colab_type":"text"},"source":["### Compare SARSA and Expected SARSA <br><br>\n","\n","\" Explicitly computing the expectation over next action is the main idea behind the Expected SARSA algorithm ! \" \n","\n","<br><br><br>\n","\n","\n","\n","#### SARSA's learning update <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1nAriyWPKo8k2DGo_CFgcvfcvPoBpl9yF\" alt=\"3-02\" width=\"500\">\n","\n","<br>\n","\n","Since expected SARSA is still based on the Bellman equation for action-values, <br>\n","it uses the familiar form of learning update. <br><br>\n","\n","The algorithm is nearly identical to SARSA, <br>\n","But instead of a sample of the naxt action-value $\\; Q(S_{t+1}, A_{t+1})$ <br>\n","the TD error uses the expected estimate of the next action-value $\\; \\displaystyle \\sum_{a'} \\pi (a'|S_{t+1}) Q(S_{t+1}, a')$ <br><br>\n","\n","That means <br>\n","on every time step, the agent has to average the next state's action values according to how likely they are under the policy ($\\pi$). <br><br>\n","\n",">For example, <br>\n",">with the following values and policy (up there), <br>\n",">expected SARSA would use a value of $1.4$ for it's estimate of the expected next action value. \n","\n","<br><br><br>\n","\n","\n","\n","#### Upside of Expected SARSA $\\qquad$ ( than SRSA ) <br><br>\n","\n","\" There's a huge upside to calculating the expectation explicitly. <br>\n","Expected SARSA has a more stable update target than SARSA. \" <br><br>\n","\n","\n","Example <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1ee9aw601ZlALDXF4ezligrRendppCkX9\" alt=\"3-03\" width=\"500\">\n","\n","<br>\n","\n","imediate reward is deterministically $1$. <br><br>\n","\n","Both SARSA and Expected SARSA <br>\n","start up with a true action values for the next state. <br>\n","\n","\n","<br><br><br>\n","\n","model | updated target value\n","--- | ---\n","SARSA | <img src=\"https://drive.google.com/uc?id=1k-7FSm7IpqZcNxOiIX5XGB-p8GfP-Sgq\" alt=\"3-04\" width=\"500\"> <br>\n","Expected SARSA | <img src=\"https://drive.google.com/uc?id=1po2ZyyRD20y6mmhoN7RkE96H3d_uliCJ\" alt=\"3-05\" width=\"300\">\n","\n","<br>\n","\n","SARSA <br>\n","Even in this idealized case, the next action sampling that SARSA does can cause it to update it's values in the wrong direction ! <br>\n","It relies on the fact that in expectation across multiple updates, the direction is correct. <br><br>\n","\n","Expected SARSA <br>\n","By contrast, Expected SARSA's update targets are exactly correct ! <br>\n","and do not change their estimated values away from the true values. \n","\n","<br><br>\n","\n","\n","In general, <br>\n","Expected SARSA's update targets are much lower variance than SARSA's ! <br>\n",">The lower variance <br>\n",">comes with a downside though !\n","\n","<br><br><br>\n","\n","\n","\n","#### Downside of Expected SARSA $\\qquad$ ( than SRSA ) <br><br>\n","\n","\n","\" The lower variance comes with a downside ! \" <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1CWH3-FWCbv_bITU4NxSQdqG2uGBnfpbL\" alt=\"3-06\" width=\"500\">\n","\n","<br>\n","\n","Computing the average $\\mathbb{E}$ over next actions <br>\n","becomes more expensive as the number of actions increases. <br><br>\n","\n",">When there are many actions, <br>\n",">computing the average might take a long time, <br>\n",">especially since the average has to be computed every time step. \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"arloqB7Ei3R0","colab_type":"text"},"source":["### Summary <br><br>\n","\n","  - The Expected SARSA algorithm explicitly computes the expectation under it's policy, <br>\n","  which is more expensive than sampling, but has lower variance than sampling\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0RgycgjAjuM2"},"source":["## $\\cdot$ Expected SARSA in the Cliff world <br><br>\n","\n","  - Describe Expected SARSA's behavior in an example MDP <br><br>\n","\n","  - Empirically compare Expected SARSA to SARSA\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SpcxDhjCkPQC","colab_type":"text"},"source":["### Example $\\quad : \\;$ the Cliff walking environment <br><br>\n","\n","\n","Cliff walking environment again <br>\n","So we can talk about Q-learning, SARSA, and Expected SARSA all at once. <br><br>\n","\n",">Undiscounted episodic grid world <br>\n",">Reward -100, when moving into the cliff <br>\n",">Reward -1, Otherwise on each time step\n","\n","<br><br><br>\n","\n","\n","\n","#### Over 50,000 independent runs case <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1DwAb-JYPGecr72M2q_CmyT_KXcGxt-Rg\" alt=\"3-07\" width=\"500\">\n","\n","<br>\n","\n","Compared Q-learning to SARSA <br><br>\n","\n","SARSA perfromed better than Q-learning on this domain <br>\n",">because SARSA's policy accounted for it's own exploration\n","\n","<br><br>\n","\n","\n","Compare SARSA to Expected SARSA $\\quad$ (with $\\epsilon = 0.1$) <br><br>\n","\n","This plot <br>\n","created by testing each agent with different values of the step size parameter $\\alpha$.\n",">We did hundred episodes and averaged everything over 50,000 independent runs <br>\n",">y-axis shows the average return per episode <br>\n",">x-axis shows the step size values\n","\n","<br><br>\n","\n","\n","  - SARSA <br>\n","  SARSA's performance increases with larger step size values, but only up to a point. <br><br><br>\n","\n","\n","  - Expected SARSA <br>\n","  Expected SARSA outperformed SARSA for almost all values of $\\alpha$. <br>\n","  Expected SARSA is able to use larger $\\alpha$ values more effectively. <br><br>\n","\n","$\\rightarrow \\quad$ This is because it explicitly averages over the randomness due to it's own policy <br>\n","$\\rightarrow \\quad$ This environment is deterministic, so there are no other sources of randomness to account for. <br>\n","$\\qquad$ This means Expected SARSA's updates are deterministic for a given state and action !\n","\n","<br><br>\n","\n","on the other hand <br>\n","SARSA's updates can vary significantly depending on the next action. \n","\n","<br><br><br>\n","\n","\n","\n","#### Over 100,000 independent runs case <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1lHIQwwnFDIOcoDqXDA_EijNAHt4ecWmY\" alt=\"3-08\" width=\"500\">\n","\n","<br>\n","\n",">Let's look at the average return per episode after 100,000 episodes. <br>\n",">At this point, each algorithm has learned everything it's going to learn. \n","\n","<br><br>\n","\n","\n","  - Expected SARSA <br>\n","  Expected SARSA's long-term behavior is unaffected by $\\alpha$ ! <br><br>\n","\n","$\\qquad$ It's updates are deterministic in this exmample. <br>\n","$\\rightarrow \\quad$ Therefore the step size only determines how quickly the estimates approach their target values. \n","\n","<br><br>\n","\n","  - SARSA <br>\n","  SARSA behaves quite differently here, <br>\n","  it even fails to converge for larger values of $\\alpha \\quad$ ( affected by $\\alpha$ ! ) <br><br>\n","\n","$\\qquad$ As $\\alpha$ decreases, SARSA's long-run performance approaches Expected SARSA's.\n","\n","\n","\n","\n","<br><br><br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xWpqg_82t0ta","colab_type":"text"},"source":["### Summary <br><br>\n","\n","  - Expected SARSA was able to quickly learn a good policy in the cliff-world <br><br>\n","\n","  - Expected SARSA is more robust to large step-size than SARSA\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TEbODPpZH27l"},"source":["## $\\cdot$ Generality of Expected SARSA <br><br>\n","\n","\n","  - Understand how Expected SARSA can do off-policy without using importance sampling <br><br>\n","\n","  - Explain how Expected SARSA generalizes Q-learning <br><br>\n","\n","\n","\n","<br><br>\n","\n","\n","We've seen three TD algorithms for control <br>\n",">Q-learning / SARSA / Expected SARSA <br><br>\n",">\n",">\n",">SARSA & Expected SARSA $\\quad \\rightarrow \\quad$ both approximate the same Bellman equation. <br><br>\n",">\n",">Q-learning & Expected SARSA $\\quad \\rightarrow \\quad$ How they are related to each other ?\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"B877r80kJZVX","colab_type":"text"},"source":["### Off-policy Expected SARSA <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1IN5HCUXZ_ZizLbZVEzBqZ24wN38TH2C0\" alt=\"3-09\" width=\"500\">\n","\n","<br>\n","\n","\n",">Let's start with the on-policy case, <br>\n",">where the target policy $\\pi$ and the behavior policy $b$ are equal. \n","\n","<br><br>\n","\n","\n","#### the Expected SARSA update <br><br>\n","\n","$A_{t+1} \\quad \\sim \\quad \\pi(a'|S_{t+1}) \\; \\not= \\; b(a'|S_{t+1})$ <br><br>\n","\n","The next action $A_{t+1}$ is sampled from $\\pi$ in this case. <br><br>\n","\n","$\\rightarrow \\quad$ However, $\\quad$ the expectation over actions is \"computed independently\" of the action actually selected in the next state ! <br>\n","$\\rightarrow \\quad$ In fact, $\\quad \\;\\;\\;$ target policy $\\pi$ need not to be equal to the behavior policy $b$ ! <br><br>\n","\n","\n","> ????????????????????????? <br>\n","> 이 부분 조금 더 생각해보자\n","\n","<br><br>\n","\n","This means <br><br>\n","\n","$\\Rightarrow \\quad$ Expected SARSA can be used to learn off-policy without importance sampling ! $\\quad$ ( like Q-learning ) \n","\n","\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9by25MX5LINg","colab_type":"text"},"source":["### Greedy Expected SARSA <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=14gQMt0S0TfexEpxVF3xaKUkT2dRJPWx0\" alt=\"3-10\" width=\"500\">\n","\n","<br>\n","\n","\n","$\\pi \\; = \\; \\pi^*$ <br><br>\n","\n","$\\qquad$ \" What happens if the target policy is greedy <br>\n","$\\qquad$ with respect to it's action value estimates ? \" \n","\n","<br><br><br>\n","\n","\n","\n","$\\displaystyle \\sum_{a'} \\pi(a'|S_{t+1}) Q(S_{t+1},a') \\; = \\; max_{a'} Q(S_{t+1},a')$ <br><br>\n","\n","$\\qquad$ Only the highest value action is considered in the expectation <br>\n","$= \\quad$ Computing the maximum over actions in the next state $\\quad$ ( just like in Q-learning ) <br><br>\n","\n","\n","\n","$\\Rightarrow \\quad$ Q-learning is a special case of Expected SARSA ! $\\quad$ ( In otherwords ) <br><br>\n","\n",">Q-learning $\\quad \\in \\quad $ Expected SARSA\n","\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"365ZEt4YN_Te","colab_type":"text"},"source":["### Summary <br><br>\n","\n","  - Expected SARSA uses the same technique as Q-learning to learn off-policy withou importance sampling <br>\n","  >Both use the expectation over their target policies in their update targets. <br>\n","  >This allows them to learn off-policy without importance sampling. \n","\n","  <br>\n","  \n","  - Expected SARSA with a target policy that's greedy with respect to it's action-values is exactly Q-learning\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"x9JI04iCTevS","colab_type":"text"},"source":["### Week 3 (4?) Summary <br><br>\n","\n","  - This week we learned how to use TD methods for control \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bWSTdsGYT5MW","colab_type":"text"},"source":["### TD control and Bellman equations <br><br>\n","\n","\n","the TD control algorithms are based on Bellman equations. <br>\n",">We learned about three of them\n","\n","<br>\n","\n","<img src=\"https://drive.google.com/uc?id=1jqwzplxYuVrD8hnRDSiuSbK_euT4ta4X\" alt=\"3-12\" width=\"500\">\n","\n","<br>\n","\n","  - SARSA <br>\n","  SARSA uses a sample based version of the Bellman equation <br>\n","  It learns $Q_{\\pi}$ <br>\n","  $\\rightarrow \\quad R_{t+1} + \\gamma \\; Q(S_{t+1}, A_{t+1})$ <br><br>\n","\n","\n","  - Q-learning <br>\n","  Q-learning uses the Bellman optimality equation. <br>\n","  It learn $Q^{*}$ <br>\n","  $\\rightarrow \\quad$ $R_{t+1} + \\gamma \\; max_a Q(S_{t+1}, a')$ <br><br>\n","\n","\n","  - Expected SARSA uses the same Bellman equation as SARSA, but samples it differently ! <br>\n","  It takes an expectation over the next action values. <br>\n","  $\\rightarrow \\quad$ $R_{t+1} + \\gamma \\; \\displaystyle \\sum_{a'} \\pi(a'|S_{t+1}) Q(S_{t+1},a')$\n","\n","\n","<br><br><br>\n","\n","\n","\n","### The story with On-policy and Off-policy learning <br><br>\n","\n","\n","\n","  - SARSA <br>\n","  SARSA is a on-policy algorithm that learns the action values for the policy it's currently following. <br><br>\n","\n","\n","  - Q-learning <br>\n","  Q-learning is an off-policy algorithm that learns the optimal action values. <br><br>\n","\n","  - Expected SARSA <br>\n","  Expected SARSA is both an on-policy and an off-policy algorithm that can learn the action values for any policy.\n","\n","<br><br><br>\n","\n","\n","\n","\n","### SARSA v.s. Q-learning <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1Eegx24QfidGe1uobYhNEwh7IRAKsXDBz\" alt=\"3-13\" width=\"500\">\n","\n","\n","<br>\n","\n","SARSA can do better than Q-learning when performance is measured online. <br>\n","Because on-policy control methods account for their own exploration. <br><br>\n","\n","\n","In the cliff world we saw that q-learning frequently fell off the cliff becaue of it's exploratory actions. <br>\n","SARSA learned the longer but safer path that rarely fell off the cliff, this resulted in higher reward.\n","\n","<br><br><br>\n","\n","\n","\n","#### Improvement over SARSA called Expected SARSA <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1_LfEapmpXsHLXK_BROaKED2bFcCQzsxm\" alt=\"3-14\" width=\"500\">\n","\n","<br>\n","\n","\n","Expected SARSA outperformed SARSA for all the step size parameter values we tested. <br>\n","This is becuse Expected SARSA mitigates the variance due to it's own policy. <br><br>\n","\n","Expected SARSA takes the expectation over the next action. $\\quad$ ( like the name suggests )\n","\n","\n","<br><br><br>\n","\n","\n",">Next <br>\n",">We'll learn how to combine PLANNIG / LEARNING / ACTING ! \n","\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n"]}]}
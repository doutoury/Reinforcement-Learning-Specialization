{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1. Week_3-1_Policies and Value functions","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyPz5jRAZ2LNUxTMnLfcmvpF"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1Lv7uvopbn9Y","colab_type":"text"},"source":["# Week_3 <br>\n","\n","INDEX <br><br>\n","\n","\n","  - Policies and Value functions <br>\n","    - Specifying polocies <br>\n","    - Value functions <br>\n","  \n","  - Bellman equations <br>\n","    - Bellman equation <br>\n","    - Why Bellman equations ? <br>\n","\n","  - Optiality <br>\n","    - Optimal policies <br>\n","    - Optimal value functions <br>\n","    - Using optimal balue functions to get optimal policies <br>\n","\n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NocPSXO6b6qZ","colab_type":"text"},"source":["## 1. Policies and Value functions <br><br>\n","\n","\n","  - Specifying policies <br><br>\n","\n","  - Value functions\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"DLDk_wsHbr0H","colab_type":"text"},"source":["## $\\cdot$ Specifying Policies <br><br>\n","\n","\n","__Reinforcement learning__ is a problem formulation <br>\n","for __sequential decision making__ under __uncertainty__. <br><br>\n","\n","Earlier, we learned that the agent's role in this interaction is to choose an action on each time step. The choice of action has an immediate impact on both the immediate reward and the next state. <br><br><br>\n","\n","\n","### __Policy__ <br>\n","\n","__\" How an agent selects actions. \"__ <br><br>\n","\n","  - A policy is a distribution over actions for each state <br>\n","  - Discribe the similarities and differences between stochastic and deterministic policies <br>\n","  - Generate valid policies for a given MDP (markov decision process) \n","\n","<br><br><br>\n"]},{"cell_type":"markdown","metadata":{"id":"z1rm_sh0ifpM","colab_type":"text"},"source":["### __Deterministic policy__ notation <br><br>\n","\n","#### $\\pi(s) = a$ <br><br>\n","\n","In the simplest case, <br>\n","It's called the deterministic policy. <br>\n","A policy __maps each state to a single action__. <br><br>\n","\n","It represents the action selected in state $S$ by the policy $\\pi$. <br><br>\n","\n","(It's __just a function__. for mapping constant(?)) \n","\n","<br><br><br>\n","\n","\n","### Example of policy <br><br>\n","\n","![1-1](https://drive.google.com/uc?id=1l4sgP037rZoYCY0tbwhiCAOae3OkiJrH)\n","\n","길찾기 문제. <br>\n","state : 2차원 격자상의 위치 <br>\n","action : 2차원 격자상의 한칸 이동 <br><br>\n","\n","$\\rightarrow$ 눈에 안 보이는 차원과 움직임의 문제도 표현 가능 ?! <br> \n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"Ckfc99VjkYZv","colab_type":"text"},"source":["### __Stochastic policy__ notation <br><br>\n","\n","#### $\\pi(a|s)$ <br><br>\n","\n","in general, <br>\n","It's called stochastic policy. <br>\n","A policy assigns __probabilities to each action__ in __each state__. <br><br>\n","\n","It represents the probability of selecting action $a$ in a state $s$. <br><br>\n","\n","\n","  - A stochastic policy is one where __multiple actions__ may be selected with non-zero probability. <br>\n","(한 상태 $s_k$ 에서 동시에 여러 행동 action 이 서로 다른 확률로 선택될 수 있다! 확률분포 distribution에 따라서!) <br>\n","  - $\\pi$ specifies __a seperate distribution__ over actions __for each state__ ! <br> \n","(각 상태 $s_k$ 마다 서로 다른 action의 확률분포 distribution을 갖을 수 있다!)\n","  - __The set of available actions__ can be __different in each state__. <br>\n","  (보통 행동set가 다른 경우가 많지는 않지만, 다를 수 있다는 사실은 중요!) \n","\n","<br><br>\n","\n","(It's a __probability distribution__. for mapping probabilities)\n","\n","<br><br><br>\n","\n","![1-2](https://drive.google.com/uc?id=16EpSa_hd6GrlQGwRltCU3f5HQ7l57aJq) <br>\n","\n","\n","\n","#### Basic rule of stochastic policy <br>\n","\n","&emsp; &emsp; We have to follow some basic rules ( 확률통계 규칙 ? )<br><br>\n","\n","  - $\\displaystyle \\sum_{a \\in A(s)} \\pi(a|s) = 1$ &emsp; &emsp; : The sum over all action probabilities must be $1$ for each state <br><br>\n","  \n","  - $\\pi(a|s) \\geq 0$ &emsp; &emsp; &emsp; &emsp; &nbsp; : Each action probability must be non-negative <br><br>\n","\n","\n","![1-3](https://drive.google.com/uc?id=13m1-tRlrJwu0pQBkdd4ynEtk8qAwXKgz)\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MdoVkJP0QezS","colab_type":"text"},"source":["Previously we discussed how a stochastic policy, like Epsilon greedy, can be useful for exploration. The same kind of exploration-exploitation trade-off exists in MDP's. <br><br>\n","\n","(Let's talk more about that later)\n","\n","<br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xlB53d_ZsyDO","colab_type":"text"},"source":["### Valid and invalid policies <br><br>\n","\n","It's important that <br>\n","__Policies depend only on the current state !__ <br>\n","__Not on other things__ like time or previous states ! <br><br>\n","\n","$\\to \\quad$ __\" The state defines all the information used to select the current action. \"__ <br><br><br>\n","\n","\n","#### Example of valid and invalid policy <br><br>\n","\n","![1-4](https://drive.google.com/uc?id=1EInKEvC_RHQw-8I02Nz7kbSs9ZBxtSH6) <br>\n","\n","\n","  - state &emsp; : $s_k$ <br>\n","  - action &ensp; : $a_{left}$ , $a_{right}$ <br><br>\n","\n","  1. Valid policy <br>\n","  In this MDP example, we can define a policy that chooses to go either left or right (action) with equal probability. <br><br>\n","\n","  2. Invalid policy <br>\n","  We might also want to define a policy that chooses the opposite of what it did last, alternating between left and right actions. <br>\n","  $\\rightarrow$ That would __not be a valid policy__ because this is __conditional on the last action__. <br>\n","  $\\rightarrow$ That means the action __depends on something other than the state__. <br>\n","\n","<br><br>\n","\n","It is better to think of this as __a requirement on the state__, not a limiation on the agent. \n","\n","<br>\n","\n",">__In MDP__, we assume that __the state includes all the information__ required __for decision-making__. <br><br>\n",">\n",">__If alternating__ between left and right would yield a higher return, <br>\n",">then __the last action should be included in the state__. \n","\n","<br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8cr7Amrqv8vM","colab_type":"text"},"source":["### Summary <br><br>\n","\n","  - An agent behavior is specifies by a policy <br>\n","  - A policy maps the current state onto a set of probabilities for taking each action <br>\n","  ( A policy maps the state to a probability distribution over actions )\n","  - Policies can only depend on the current state <br>\n","  ( Not other things like time or previous states ) \n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Y0oWRvAecwzk","colab_type":"text"},"source":["## $\\cdot$ Value functions <br><br><br>\n","\n","\n","### Short-term reward v.s. Long-term reward <br><br>\n","\n","Many problems involve some amount of __delayed reward__. <br>\n",">For example, A store manager could lower their prices and sell off their entire inventory to maximize short term gain. <br>\n",">But they would do better in the long run by maintaining to sell when demand is high. \n","\n","<br>\n","\n","  - __In reinforcement learning__, <br>\n","  &emsp; &emsp; __\" Reward captures the notion of short-term gain \"__ . <br><br>\n","\n","  - __The objective__, however, is to learn <br>\n","  &emsp; &emsp; __\" a Policy that achieves the most reward in the long run \"__. <br><br><br>\n","\n","\n","\n","#### $\\Rightarrow$ &ensp; __Value functions__ formalize what ' this ' means <br>\n","&emsp; &emsp; ( ' this ' &ensp; Long-term reward ??? )\n","\n","<br><br><br>\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PRB8HXCq2QEm","colab_type":"text"},"source":["### Value functions <br><br>\n","\n","  - Describe __the role__ of __state-value functions__ and __action-value functions__ in RL <br>\n","  - Describe __the relationship__ between __value functions__ and __policies__ <br> \n","  - Create example of __valid value functions__ for a given __MDP__ \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9A2DMFUKgLQp","colab_type":"text"},"source":["### __State-value functions__ <br><br>\n","\n","$\\begin{align} v_{\\pi}(s) \\doteq \\mathbb{E}_{\\pi} \\; [G_t | S_t = s] \\qquad \\qquad \\qquad \\qquad \\qquad &_{Recall \\;\\; 'return \\; G_t'} \\\\ \n","& G_t = \\displaystyle \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\end{align}$ <br><br>\n","\n","\n","Roughly speaking, a state-value function is __the future reward__ an agent can __expect to recieve__ starting from a particular state. <br><br>\n","\n","More precisely, <br>\n","  - the state-value function is __the expected return__ <br>\n","  from __a given state__. <br>\n","  (It is the expected sum of future rewards) <br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LVIrrhLI8haD","colab_type":"text"},"source":[">\\* &ensp; The agent's behavior will also determine how much total reward it can expecct. So value function is defined with respect to a given policy. <br>\n","\n",">\\* &ensp; In value function formula, it would be indicated by subscription $\\pi$ that the value function is contingent on the agent selecting actions according to $\\pi$. \n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"5RijJB_tmjr8","colab_type":"text"},"source":["### __Action-value functions__ <br><br>\n","\n","$\\begin{align} q_{\\pi}(s,a) \\doteq \\mathbb{E}_{\\pi} \\; [G_t | S_t = s , \\; A_t = a] \\qquad \\qquad \\qquad &_{Recall \\;\\; 'return \\; G_t'} \\\\ & G_t = \\displaystyle \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\end{align}$ <br><br>\n","\n","\n","Action value describes __what happens__ when __the agent first selects a particular action__. <br><br>\n","\n","More formally, <br>\n","  - the action value of a state is __the expected return__ <br>\n","  if __1. the agent selects action $a$__, and then __2. follows policy $\\pi$__.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Tf7q3wtsoDex","colab_type":"text"},"source":["### Value functions with respect to the policy $\\pi$ <br><br>\n","\n","\n","\\* &ensp; __The agent's behavior__ will also determine __how much total reward__ it can expect. <br>\n","So __value function__ is defined with respect to __a given policy__. <br><br><br>\n","\n","\n","\n","#### __Policy $\\pi$__ &ensp; (distribution) <br><br>\n","\n","\n","\\* &ensp; The subscript $\\pi$ indicates the __value function__ is contingent on __the agent__ selecting actions __according to (policy) $\\pi$__. <br><br>\n","\n",">The subscipt $\\pi$ on the expection $\\mathbb{E}$ indicates that the expectation is computed __with respect to the policy $\\pi$__. \n","\n","<br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WWSBRpB4-w3g","colab_type":"text"},"source":["### Benefit of value functions <br>\n","\n","\n","![1-5](https://drive.google.com/uc?id=1E5nx_KoUNVRkGnLuvXh4xXhuy6_-SwkW)<br><br>\n","\n","\n","#### __\" Value functions predict rewards into the future \"__ <br><br>\n","\n","__Value functions__ are crucial in reinforcement learning. <br>\n","Value functions allow an agent to __query the quality__ of its __current situation__ <br>\n","(instead of waiting to observe the long-term outcome !) <br><br><br>\n","\n","\n","\n","#### The benefit is twofold <br><br>\n","\n","\n","  1. Value fuctions __summarize all the possible futures__ by averaging over returns. <br>\n","\n","  >(Reinforce the fact that) __The return__ is __not immediately available__ <br><br>\n","  >\n","  >(return 은 reward 의 합이므로 short-term benefit 만 반영하므로 ?) <br>\n","  (randomness 때문에 average 로만 활용 가능한가 ?)\n","\n","\n","  <br>\n","\n","  2. Value functions enable us to __judge the quality of different policies__. <br>\n","  (Ultimately we care most of __learning a good policy__) <br>\n","  \n","  >(Reinforce the fact that) __The return__ may be __random__ <br><br>\n","  >\n","  >(due to __stochasticity (무질서도)__ in both __the policy $\\pi$__ and __environment dynamics $p(\\;)$__)\n","  \n","  <br><br>\n","  \n","  >여기서 The return 은 Value function $V_{\\pi} (s)$ 의 return 인가 ? <br>\n","  >The return &ensp; $G_t = \\displaystyle \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$ 인듯 ?\n","\n","\n","<br><br><br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5GPRGXOgH4TY","colab_type":"text"},"source":["### __Value function v.s. Return__ (위에 대한 내 정리 !) <br><br>\n","\n","\n","  - Value function <br>\n","  Policy 를 반영한 Value function 을 통해서는 all possible futures 에 대한 the most reward 를 찾고 <br>\n","  ( The most reward 가 아니라 모든 possible rewards 의 평균을 찾는 듯 (?) ) <br><br>\n","\n","  - Return <br>\n","  discount rate $\\gamma$ 만 반영한 Return 을 통해서는 one possible future (?) 에 대한 reward 를 찾음 <br>\n","  ( Week2 에서 정의됨. Return 의 정의는 미래 Reward 들의 가중합 )\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qo0i--mkZAJK","colab_type":"text"},"source":["### __Dynamics v.s. Policy__ (위에 대한 내 정리 !) <br><br>\n","\n","\n","  - Dynamics of MDP $\\quad : \\quad p(s', r | s, a)$\n","  특정상태 s 에서 특정행동 a 를 했을 때, 보상 r 과 함께 가능한 다음상태 possible s' 들 중 하나가 결과로 나타날 확률 (?) <br><br>\n","\n","  - Policy $\\quad : \\quad \\pi$ <br>\n","  특정상태 s 에서 agent가 특정 행동 a 를 실행할 확률 (?) <br>\n","  $\\rightarrow \\quad$ policy $\\pi$ 는 확률분포를 말하는게 아니라, \"어떤 확률분포를 따라 행동action (event) 을 결정하는 행위function\" ? <br>\n","  $\\rightarrow \\quad$ 뒷 부분에 $\\pi_{*} = \\; ...$ 부분 보면 agent 가 할 action 을 결정해주는 알고리즘 같은데 ?\n","\n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GTfQnD3YBCZZ","colab_type":"text"},"source":["### Example of value function <br>\n","(just for intuition ..) <br><br><br>\n","\n","\n","\n","#### 1. Exmaple of episodic MDP case <br><br>\n","\n","\n","![1-6](https://drive.google.com/uc?id=14L6QNbKQfO4HwZKPnnZxwOW99NvKV4GC) <br><br>\n","\n","\n","__Problem__ <br>\n","Agent playing the game of chess. <br>\n","Chess has an episodic MDP. <br>\n","The state is given by the positions of all the pieces on the board. <br>\n","The actions are the legal moves. <br>\n","Termination occurs when the game ends in either a win, loss, or draw. <br><br>\n","\n","Define the reward as $+1$ for winning and $+0$ for all the other moves. <br><br><br>\n","\n","\n","\n","\n","__\" How well the agent is playing ? \"__ <br><br>\n","\n","\n","  - $\\text{Reward} = \\begin{cases} +1 & \\text{for winning} \\\\ +0 & \\text{otherwise} \\end{cases}$ <br><br>\n","\n","  $\\rightarrow$ The reward (only itself) does not tell us much <br>\n","  about \" how well the agent is playing during the match \". <br>\n","  >We'll have to wait until the end of game to see any non-zero reward. \n","  \n","  <br>\n","\n","  - $\\text{State-value} \\;\\; V_{\\pi}(s) = P(\\text{win})$ <br><br>\n","  \n","  $\\rightarrow$ The value function tells us much more. <br>\n","  The state-value is equal to the expected sum of future rewards. <br>\n","  >Since the only possible non-zero reward is $+1$ for winning, <br>\n","  >the state-value, the expected sum of future rewards, is simply the probability of winning, $P(win)$, if we follow the current policy $\\pi$. <br>\n","  > ... ???\n","\n","<br><br>\n","\n","\n","\n","![1-7](https://drive.google.com/uc?id=1_LX9v2Vo9c9ONl29RqZYk4q-xjcf7C8e) <br><br>\n","\n","\n","\n","__State transition__ <br><br>\n","\n","$\\begin{align} P(\\text{win}) = & \\; V_{\\pi}(s) &= 0.34 \\\\ & \\; V_{\\pi}(s') &= 0.31 \\end{align}$ <br><br>\n","\n","In this two player game, <br>\n","the opponent's move is part of the state transition. <br>\n","The environment moves both the agent's piece and the opponent's piece. <br>\n","This put the board into a new state $S'$. <br><br>\n","\n","State transition from former state to new state , $S \\rightarrow S'$ , is done after the agent player's move and the opponent player's move. <br><br>\n","\n",">Note the value of state $S'$ is lower than the value of state $S$. <br>\n",">This means we are less likely to win the game from this new state assuming we continue following policy $\\pi$. \n","\n","<br><br>\n","\n","\n","__Action transition__ <br><br>\n","\n","An action-vlue function would allow us to assess the probability of winnig __for each possible move__ given we follow the policy $\\pi$ for the rest of the game. <br>\n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BuEpgpXbH5vm","colab_type":"text"},"source":["#### 2. Example of continuing MDP case <br><br>\n","\n","\n","![1-8](https://drive.google.com/uc?id=1S4Ze31DmgbQbmn17qeGXpCpvs54qrmy-) <br><br>\n","\n","__Problem__ <br>\n","The states are defined by the locations on the grid. <br>\n","The actions move the agent up, down, left, or right. <br>\n","The reward of $-1$ is generated by bumping to left or right side. <br>\n","no reward is yielded by most other actions. <br><br>\n","\n","However, <br>\n","Two special states are there, labeled $A$ and $B$. <br>\n","$+10$ reward is yielded from every action in state $A$, <br>\n","$+5$ reward is yielded from every action in state $B$. <br>\n","Every action in state $A$ and $B$ transitions the agents to state $A'$ and $B'$ respectively. <br><br>\n","\n","$A$ and $B$ are the only source of positive reward in this MDP. \n","\n","<br><br><br>\n","\n","\n","\n","__Policy__ <br><br>\n","\n","\n","We must specify the policy before we can figure out what the value function is. <br>\n","\n","![1-9](https://drive.google.com/uc?id=1lqJ2CBlmJK-dZmUdeteMOUZD3ZyVdx4p) <br>\n","\n","$\\text{Policy} \\quad : \\quad \\text{Uniform dist. } \\begin{cases} \\leftarrow & = 25\\% \\\\ \\rightarrow & = 25\\% \\\\ \\uparrow & = 25\\% \\\\ \\downarrow & = 25\\% \\end{cases} \\qquad \\qquad , \\gamma = 0.9$ <br><br>\n","\n","Let's look at the uniform random policy. <br>\n","Since this is __a continuing task__, we need to __specify $\\gamma$__. let's go with $0.9$. <br><br>\n","\n",">Later, we'll learn several ways to compute and estimate the value function. <br>\n",">But in this example, we're going to compute it for you. <br>\n",">$\\rightarrow$ &ensp; __Bellman equation__ 등 ... ?\n","\n","<br><br><br>\n","\n","\n","\n","__State-value__ <br><br>\n","\n","We've written the already computed value of each state in the table. <br>\n","let's obesrve the values. <br>\n","\n","\n","![1-10](https://drive.google.com/uc?id=1_aTM0zTrpAcPYgWsgEkaIUXvW6Y30dqe) <br>\n","![1-11](https://drive.google.com/uc?id=1OyreLi1jhPFJyP2LK6AcGXUqBvN1lSzE) <br>\n","\n","\n","  - At the bottom line : <br><br>\n","\n","  Those are negative values, so low. <br>\n","  >Because the agent is likely to bump into the lower wall before reaching the distant states $A$ and $B$. &emsp; ... (A and B are the only source of positive reward in this MDP) \n","  \n","  <br>\n","\n","  - At the state $A$ : <br><br>\n","\n","  It is the highest value what $A$ has. <br>\n","  Notice that the value is less than $10$ <br>\n","  even though every action from state $A$ generates a reward of $+10$. <br>\n","  >Why? <br>\n","  >Because every transition from $A$ moves the agent close to the lower wall. And near the lower wall, the random policy is likely to bump and get negative reward. \n","  \n","  <br>\n","\n","  - At the state $B$ : <br><br>\n","\n","  It is slightly greater than $5$ what $B$ has. <br>\n","  Notice that the value is greater than $5$ <br>\n","  even though every action from state $B$ generates a reward of $+5$. <br>\n","  >Why? <br>\n","  >Because The transition from $B$ moves the agent to the middle. in the middle, the agent is unlikely to and is close to the high-valued states $A$ and $B$. \n","\n","  <br><br>\n","  \n","  It's really quite amazing <br>\n","  how the value function compectly summarize all these possibilities !\n","\n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"PTfjc3ozZIfZ","colab_type":"text"},"source":["### Summary <br><br>\n","\n","  - __State-value functions__ represent the __expected return__ from a __given state__ under a specific policy. <br><br>\n","\n","  - __Action-value functions__ represent the __expected return__ from a __given state after taiking a specific action__, later folloiwng a specific policy. <br>\n","\n","<br><br>\n","\n",">Soon, We'll discuss how value functions can be computed. \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]}]}
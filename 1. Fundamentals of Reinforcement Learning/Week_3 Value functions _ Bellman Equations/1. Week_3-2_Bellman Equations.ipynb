{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1. Week_3-2_Bellman Equations","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyOl3KnkyiMO5gTJBYv+Gdgg"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Dd3hMoha0DWQ","colab_type":"text"},"source":["## 2. Bellman equation <br><br>\n","\n","\n","  - Bellman equation derivation <br><br>\n","  \n","  - Why Bellman equations ? \n","  \n","  <br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"T_ry0zMU_wFw","colab_type":"text"},"source":["## $\\cdot$ Bellman equation derivation <br><br>\n","\n","\n","#### __In everyday life ...__<br>\n","\n",">In everyday life, we learn a lot __without getting explicit, poisitive or negative feedback__. Imagine for example, you are riding a bike and hit a rock that sends you off balance. Let's say you recover so you don't suffer any injury. You might learn to avoid rocks in the future and perhaps react more quickly if you do hit one. How do we know that hitting a rock is bad, even when nothing bad happened this time ? <br><br>\n",">\n",">The answer is that we __recognize that the state of losing our balance is bad even without falling and hurting ourselves__. Perhaps we had similar experiences in our past when things didn't work out so nicely.\n","\n","<br>\n","\n","#### __In reinforcement learning ...__ <br>\n","\n",">In reinforcement learning a similar idea allows us to __relate the value of the current state to the value of future states without waiting to observe all the future rewards__. <br><br>\n",">\n",">$\\rightarrow$ &ensp; __Bellman equations__ <br>\n",">It is used to formalize this connection between __the value of a state__ and __its possible successors__ (successor states) ($\\approx$ future states ?).\n","\n","<br><br><br>\n","\n","\n","\n","### Bellman equation derivation <br><br>\n","\n","  - Derive the Bellman equation for state-value functions <br>\n","  - Derive the Bellman equation for action-value functions <br>\n","  - Understand how Bellman equations relate current and future values. \n","\n","\n","<br><br><br>\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Ao-Pcg-CFjZP","colab_type":"text"},"source":["### State-value Bellman equation <br><br>\n","\n","\n","The Bellman equation for the state-value function define a __relationship__ <br>\n","between __the vlaue of a state__ and __the value of his possible successor states__. <br><br><br>\n","\n","\n","####__How to derive this relationship__ <br>\n","&emsp; &emsp; from the definitions of the state-value function and return <br><br>\n","\n","\n","\n","\n","  - Recall the state-value function &ensp; $v_{\\pi}(s)$ <br>\n","  (defined as the expected return starting from the state $s$) <br>\n","\n","  - Recall the return &ensp; $G_t$<br>\n","  (defined as the discounted sum of future rewards) <br>\n","  <br>\n","\n","  $\\begin{align} v_{\\pi}(s) \\doteq \\mathbb{E}_{\\pi} \\; [G_t | S_t = s] \\qquad \\qquad \\qquad \\qquad \\qquad &_{Recall \\;\\; 'return \\; G_t'} \\\\ \n","& G_t = \\displaystyle \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\end{align}$ <br><br><br>\n","\n","\n","\n","\n","  1. Recursive expression <br><br>\n","\n","\n","  - Recall that the return $G_t$ at time $t$ can be written recursively <br>\n","  (As the immediate reward &nbsp; $R_{t+1}$ &nbsp; and the discounted return &nbsp; $\\gamma \\; G_{t+1}$ &nbsp; at time &nbsp; $t+1$) <br>\n","  <br>\n","  \n","  $\\begin{align} v_{\\pi}(s) &\\doteq \\mathbb{E}_{\\pi} \\; [G_t | S_t = s] \\\\ &= \\mathbb{E}_{\\pi} \\; [R_{t+1} + \\gamma \\; G_{t+1} | S_t = s] \\end{align}$ <br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6BTQclUseiYw","colab_type":"text"},"source":["  2. Expand $\\mathbb{E}_{\\pi}$ to weighted sum expression <br><br>\n","\n","  - Expand this expected reutrn <br>\n","  (defined as a sum of possible outcomes weighted by the probability that they occur) <br><br><br>\n","\n","  First, expand the expected return &ensp; $\\mathbb{E}_{\\pi} \\; [R_{t+1} + \\gamma \\; G_{t+1} | S_t = s]$ <br>\n","  __as a sum over possible action choices made by the agent__. <br><br>\n","\n","  $\\rightarrow \\quad \\displaystyle \\sum_a \\pi(a|s) \\qquad \\qquad \\quad \\; \\text{... by the policy selecting an action}$ <br><br><br>\n","\n","  Second, expand over __next states $s'$ and possible rewards $r$ <br>\n","  conditioned on state $s$ and anction $a$__. <br><br>\n","  \n","  $\\rightarrow \\quad \\displaystyle \\sum_{s'} \\sum_{r} p(s',r|s,a) \\qquad \\text{... dynamics function $p$} \\\\ \n","  \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\; \\text{to select the immediate reward $r$ and next state $s'$}$ <br><br>\n","\n","  >We can break it down in this order <br>\n","  >because the action choice $a$ depends only on the current state $s$, <br>\n","  >while the next state $s'$ and reward $r$ depend only on the current state and action. \n","\n","  <br><br>\n","\n","  The result is a (stochastically) __weighted sum__ of terms consisting of <br>\n","  $\\rightarrow \\quad \\displaystyle \\sum_{a} \\pi(a|s) \\sum_{s'} \\sum_{r} p(s',r|s,a) \\; * \\; \\sim$ <br><br>\n","\n","  immediate reward $r$ and (plus) epxected future returns from the next state $s'$ <br>\n","  $\\rightarrow \\quad r + \\gamma \\mathbb{E}_{\\pi} \\big[ G_{t+1}|S_{t+1}=s' \\big]$ <br><br>\n","\n","  $\\Rightarrow \\quad \\displaystyle \\sum_{a} \\pi(a|s) \\sum_{s'} \\sum_{r} p(s',r|s,a) \\; * \\; \\bigg[r + \\gamma \\mathbb{E}_{\\pi} \\big[ G_{t+1}|S_{t+1}=s' \\big] \\bigg]$ <br><br>\n","\n","  >All we have done is explicitly write the __expectation__ as it's defined, <br>\n","  >as a __sum of possible outcomes weighted by the probability that they occur__. <br><br>\n","  >\n","  >Note that $R_{t+1}$ is a random variable. <br>\n","  >While the $r$ represents each possible reward outcome.<br><br>\n","  >\n","  >The expected return depends on states and rewards <br>\n","  >infinitely far into the future. \n","\n","  <br>\n","  <br>\n","\n","  $\\begin{align} v_{\\pi}(s) &\\doteq \\mathbb{E}_{\\pi} \\; [G_t | S_t = s] \\\\ &= \\mathbb{E}_{\\pi} \\; [R_{t+1} + \\gamma \\; G_{t+1} | S_t = s] \\\\ &= \\displaystyle \\sum_{a} \\pi(a|s) \\sum_{s'} \\sum_{r} p(s',r|s,a) \\bigg[r + \\gamma \\mathbb{E}_{\\pi} \\big[ G_{t+1}|S_{t+1}=s' \\big] \\bigg] \\end{align}$ <br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"somXXBVBi_Fw","colab_type":"text"},"source":["3. Recursive expression <br><br>\n","\n","\n","  - Expand recursively this equation.\n","We can do that as many times as we want, but it could only make the expression more complicated. <br><br>\n","  \n","  $\\begin{align} v_{\\pi}(s) &\\doteq \\mathbb{E}_{\\pi} \\; [G_t | S_t = s] \\\\ \n","  &= \\mathbb{E}_{\\pi} \\; [R_{t+1} + \\gamma \\; G_{t+1} | S_t = s] \\\\ \n","  &= \\displaystyle \\sum_{a} \\pi(a|s) \\sum_{s'} \\sum_{r} p(s',r|s,a) \\bigg[r + \\gamma \\mathbb{E}_{\\pi} \\big[ G_{t+1}|S_{t+1}=s' \\big] \\bigg] \\\\ \n","  &\\color{gray}{\\; = \\displaystyle \\sum_{a} \\pi(a|s) \\sum_{s'} \\sum_{r} p(s',r|s,a) \\bigg[r + \\gamma \\displaystyle \\sum_{a'} \\pi(a'|s') \\sum_{s''} \\sum_{r'} p(s'',r'|s',a') \\bigg[r' + \\gamma \\mathbb{E}_{\\pi} \\big[ G_{t+2}|S_{s+2}=s'' \\big] \\bigg] \\bigg]} \\\\\n","  &\\color{gray}{\\; = \\quad \\vdots} \\end{align}$ <br><br><br>\n","  \n","  Instead, Notice that this ecpexted return $\\mathbb{E}_{\\pi} \\big[ G_{t+1} | S_{t+1} = s' \\big]$ is also the definition of the value function for state $s'$. The only difference is that the time index is $t+1$ instead of $t$. <br>\n","  $... \\quad$ $\\mathbb{E}_{\\pi} \\big[ G_{t+1} | S_{t+1} = s' \\big] \\quad \\Leftrightarrow \\quad v_{\\pi}(s')$<br><br>\n","  \n","  >This is not an issue because neither the policy $\\pi$ nor dynamics function $p$ depends on time. \n","  \n","  <br>\n","  \n","  $\\begin{align} v_{\\pi}(s) &\\doteq \\mathbb{E}_{\\pi} \\; [G_t | S_t = s] \\\\ &= \\mathbb{E}_{\\pi} \\; [R_{t+1} + \\gamma \\; G_{t+1} | S_t = s] \\\\ &= \\displaystyle \\sum_{a} \\pi(a|s) \\sum_{s'} \\sum_{r} p(s',r|s,a) \\bigg[r + \\gamma \\mathbb{E}_{\\pi} \\big[ G_{t+1}|S_{t+1}=s' \\big] \\bigg] \\\\ &= \\displaystyle \\sum_{a} \\pi(a|s) \\sum_{s'} \\sum_{r} p(s',r|s,a) \\bigg[r + \\gamma v_{\\pi}(s') \\bigg] \\end{align}$ <br><br><br>\n","\n","\n","\n","Making this replacement, we get the __Bellman equation__ for the state-value funcion. <br>\n","The magic of value functions is that we can use them as a stand-in (대리) <br>\n","for the average of an infinte number of possible futures. <br>\n","(infinite reculsive average expression 말하는 건가?) <br><br>\n","\n","\n","따라서 ... <br>\n","$v_{\\pi}(s) = \\displaystyle \\sum_{a} \\pi(a|s) \\sum_{s'} \\sum_{r} p(s',r|s,a) \\bigg[r + \\gamma v_{\\pi}(s') \\bigg]$\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MSGbU8plkVz-","colab_type":"text"},"source":["### Action-value Bellman equation <br><br>\n","\n","\n","We can derive similar equation for the action-value function. <br>\n","It will be a __recursive equation__ for the __value of a state-action pair__ in terms of its possible __successor's state-action pairs__. <br><br>\n","\n","$q_{\\pi}(s,a) \\doteq \\; ... \\; q_{\\pi}(s',a')$ <br><br><br>\n","\n","  1. Recursive expression <br><br>\n","\n","  $q_{\\pi}(s,a) \\doteq \\mathbb{E}_{\\pi} \\big[ G_t | S_t=s , A_t=a \\big]$ <br><br><br>\n","  \n","  2. Expand $\\mathbb{E}_{\\pi}$ to weighted sum expression <br><br>\n","\n","  - Expand this expected reutrn <br>\n","  (defined as a sum of possible outcomes weighted by the probability that they occur) <br><br>\n","\n","  In this case, the equation begin __without the policy__ selecting an action. <br>\n","  This is because __the action is already fixed__ as __part of the state-action pair__. <br>\n","  $... \\quad \\text{without} \\;\\; \\displaystyle \\sum_{a} \\pi(a|s)$ <br><br>\n","  \n","  So we skip directly to __the dynamics function $p$__ (probability function) <br>\n","  to select the immediate reward $r$ and next state $s'$. <br><br>\n","  \n","  $\\begin{align} q_{\\pi}(s,a) &\\doteq \\mathbb{E}_{\\pi} \\big[ G_t | S_t=s , A_t=a \\big] \\\\\n","&= \\displaystyle \\sum_{s'} \\sum_{r} p(s',r | s,a) \\bigg[ r + \\gamma \\mathbb{E}_{\\pi} \\big[ G_{t+1} | S_{t+1} = s' \\big] \\bigg] \\end{align}$\n"," <br><br><br>\n","\n","  3. Expand $\\mathbb{E}_{\\pi}$ to weighted sum expression <br><br>\n","  \n","  However, It is unlike the Bellman equation for the state-value function. <br>\n","  It is not a recursive equation just for the value of state. <br><br>\n","\n","  We want to recursive equation <br>\n","  for the value of one state-action pair in terms of next state-action pair. <br><br>\n","  \n","  At the moment, we have the expected return given only the next state <br>\n","$... \\quad$ $\\bigg[r + \\gamma \\mathbb{E}_{\\pi} \\big[G_{t+1} | S_{t+1} = s' \\big] \\bigg]$. <br><br>\n","  \n","  - To change this, we can express the expected return from the next state <br>\n","  as a sum of the agents possible action choices <br>\n","  ( $S_{t+1}$ 을 그 상태에서 확률분포 $\\pi(a'|s')$ 의 확률에 따라 발생 가능한 모든 액션들 $A_{t+1}$ 로 exapnd 하여 표현 ! $(S_{t+1}, A_{t+1})$ ) <br><br>\n","  \n","  $\\rightarrow \\quad$ $\\bigg[r + \\gamma \\displaystyle \\sum_{a'} \\pi(a'|s') \\mathbb{E}_{\\pi} \\big[G_{t+1} | S_{t+1} = s', A_{t+1} = a' \\big] \\bigg]$. <br><br>\n","  \n","  $\\begin{align} q_{\\pi}(s,a) &\\doteq \\mathbb{E}_{\\pi} \\big[ G_t | S_t=s , A_t=a \\big] \\\\\n","&= \\displaystyle \\sum_{s'} \\sum_{r} p(s',r | s,a) \\bigg[ r + \\gamma \\mathbb{E}_{\\pi} \\big[ G_{t+1} | S_{t+1} = s' \\big] \\bigg] \\\\ \n","&= \\displaystyle \\sum_{s'} \\sum_{r} p(s',r | s,a) \\bigg[r + \\gamma \\displaystyle \\sum_{a'} \\pi(a'|s') \\mathbb{E}_{\\pi} \\big[G_{t+1} | S_{t+1} = s', A_{t+1} = a' \\big] \\bigg] \\end{align}$ <br><br>\n","  \n","  In particular, we can change the expectation to be conditioned on both the next state and the next action, and then sum over all possible actions. Each term is weighted by the probability under $\\pi$ of selecting $a'$ in the state $s'$. <br><br><br>\n","\n","\n","  4. Recursive expression <br><br>\n","  \n","  - Expand recursively this equation. <br>\n","  ( This expected return is the same as the definition of the action value function for $(s', a')$ ) <br>\n","$... \\quad$ $\\mathbb{E}_{\\pi} \\big[G_{t+1} | S_{t+1} = s', A_{t+1} = a' \\big] \\quad \\Leftrightarrow \\quad q_{\\pi}(s', a')$ <br><br>\n","  \n","  $\\begin{align} q_{\\pi}(s,a) &\\doteq \\mathbb{E}_{\\pi} \\big[ G_t | S_t=s , A_t=a \\big] \\\\\n","&= \\displaystyle \\sum_{s'} \\sum_{r} p(s',r | s,a) \\bigg[ r + \\gamma \\mathbb{E}_{\\pi} \\big[ G_{t+1} | S_{t+1} = s' \\big] \\bigg] \\\\ \n","&= \\displaystyle \\sum_{s'} \\sum_{r} p(s',r | s,a) \\bigg[r + \\gamma \\displaystyle \\sum_{a'} \\pi(a'|s') \\mathbb{E}_{\\pi} \\big[G_{t+1} | S_{t+1} = s', A_{t+1} = a' \\big] \\bigg] \\\\\n","&= \\displaystyle \\sum_{s'} \\sum_{r} p(s',r | s,a) \\bigg[r + \\gamma \\displaystyle \\sum_{a'} \\pi(a'|s') q_{\\pi}(s',a') \\bigg] \\end{align}$ <br><br><br>\n","\n","\n","\n","Making this replacement, we get the Bellman equation for the action value function. <br>\n","따라서 ... <br>\n","\n","$q_{\\pi}(s,a) = \\displaystyle \\sum_{s'} \\sum_{r} p(s',r | s,a) \\bigg[r + \\gamma \\displaystyle \\sum_{a'} \\pi(a'|s') q_{\\pi}(s',a') \\bigg]$ <br><br><br>\n","\n","\n","\n",">수식 정리할 때 중간에 <br>\n",">policy function ? $\\pi( )$ 가 뭔지 앞에 보고 체크. the funcion of prbability 라는데 ... <br>\n",">위에서 예제에서 보여준 uniform random policy : L25% R25% U25% D25% <br>\n",">처럼 각 행동action 에 대한 확률분포 ! (결국 화률) <br>\n",">policy 정할 때 같이 정해지는 $\\gamma$ 값 0.9 or 0.7 or ... 는 discount factor ! <br>\n",">Q. 뭐에 대한 discount ? 미래 reward 값에 대한 할인 !\n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"3gmNdDgIexsb","colab_type":"text"},"source":["### Summary <br><br>\n","\n","  - We have derived the Bellman Equation for state-value function and action-value function <br><br>\n","  \n","  - The current time-step's __state/action values__ can be __written recursively__ in terms of __future state/action values__ <br>\n","  (These equations provide relationships between the value of a state and the possible next state, or state-action pair and the possible next state-action pairs) <br>\n","\n","<br><br>\n","\n","Why we care so much about these definition and relationships ? <br>\n","$\\rightarrow \\quad$ The Bellman equations capture an important structure of the reinforcement learning problem. <br><br>\n","\n",">Next we will discuss why this relationship is so fundamental and reinforcement in R.L. and how we can use it to design algorithm which efficiently estimate value functions. \n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"7JehDaeIv4Yc","colab_type":"text"},"source":["## $\\cdot$ Why Bellman equations ? <br><br>\n","\n","\n","Previously, we saw the Bellman Equations allow us to express the value of a state or state-action pair __in terms of it's possible successors__. <br><br>\n","\n","### What is the point of all this ? <br><br>\n","\n","  - Use the __Bellman Equations__ to __compute value functions__ <br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"nG10uIUFRD-I","colab_type":"text"},"source":["### Example : Gridworld <br><br>\n","\n","Small example to use Bellman equation to compute value function. <br><br>\n","\n","\n","![1-13](https://drive.google.com/uc?id=1aYXbyuvJCVG9bIoVoTCXMJ_BHSYFpyXL) <br>\n","\n","__Problem__ <br><br>\n","There is a grid consisting of just four states, labeled (as) A, B, C, and D. <br>\n","The action space consists of moving up, down, left, right, and actions which would move off the grid (instead keep the agent in place). <br>\n","The reward is 0 everywhere except for any time the agent lands in state B. <br>\n","If the agent lands in state B, it gets a reward of +5. (including starting in state B, and hitting a wall to remain there) <br>\n","Let's consider the uniform random policy. (which moves in every direction 25% of the time) <br>\n","The discount factor $\\gamma$ is 0.7. <br><br><br>\n","\n","\n","\n","![1-14](https://drive.google.com/uc?id=1wmTd5Q8JiP-MvvjIYFZCTPKseIiTIgun) <br>\n","\n","__Goal : the value of states__ <br><br>\n","\n","How can we work out the value of each of these states A, B, C, and D under this policy ? <br><br>\n","\n","Recall that the value functions is defined as the expected return under policy $\\pi$. <br>\n","This is an average over the return obtained by each sequencce of actions an agent could possibly choose, infinitely many possible futures. <br><br><br>\n","\n","\n","\n","![1-15](https://drive.google.com/uc?id=1WIWcD6UdaXJ_ixF4hD53sr8FKaYopBtq) <br>\n","\n","![1-16](https://drive.google.com/uc?id=1-nJ2ghW6B4U2i2v3uNxXLOUa5dL8r5iV) <br>\n","\n","__Express the value of state by Bellman equation__ <br><br>\n","\n","Luckily, the Bellman equatioo for the state value function provides an elegant solution. <br>\n","Using the Bellman equation, we can write down an expression for the value of state A in terms of the sum of the four possible actions and the resulting possible successors states. <br><br>\n","\n","__Simplify the expression further in this concrete case__ \n","\n","We can simplify the expression by Bellman equation further in this concrete case, because for each action there's only one possible associated next state and reward. <br>\n","That's the sum over $s'$ and $r$ reduces to a single value. <br>\n",">Note that here $s'$ and $r$ do still depend on the selected action $a$ and the current state $s$.\n","\n","<br>\n","\n","If to go right from state A, the agent lands in state B, and receive a reward of +5. <br>\n","This happes 1/4 of the time under the random policy. <br>\n","If to go down from state A, the agent lands in state C, and receive no immediate reward. <br>\n","This occurs 1/4 of the time under the policy. <br>\n","If to go up or left from state A, the agent lands back in state A again, and receive no immediate reward. <br>\n","Each of the actions occur 1/4 of the time. <br><br><br>\n","\n","\n","\n","\n","![1-17](https://drive.google.com/uc?id=1lGlcbUTvXZQS55oGNwQCPLJMPhNYDaEK) <br>\n","\n","__A system of equations for variables__ <br><br>\n","\n","We can write down a similar equation for each of the other states B, C, and D. <br>\n","Now we have a __system of 4 equation for 4 variables__. <br>\n","\n","<br><br><br>\n","\n","\n","\n","\n","![1-18](https://drive.google.com/uc?id=1k_9YckKTOq70tOxkbz-3caZ5egGDUo7T) <br>\n","\n","__The solution__ <br><br>\n","\n","The unique solution is shown here. \n","\n","<br><br><br> \n","\n","\n","\n","\n","__\" The Bellman Equation provides a powerful general relationship for MDPs. \"__ <br><br>\n","\n","The important thing to note is that the __Bellman equation__ reduced an __unmanageable infinte sum over possible futures__ to a __simple linear algebra problem__.\n","\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dUNLVCxDMISS","colab_type":"text"},"source":["### Small MDPs v.s. Complex MDPs <br>\n","\n","![1-12](https://drive.google.com/uc?id=19iP2fwBPSA_Baa-N5FgAi2cfJjy7n84m)<br>\n","\n","\n","\n","#### For MDPs of moderate size <br><br>\n","\n","  - Using the Bellman equation, you can directly write down a system of equations for the state values, and then solve the system to find the values ! <br>\n","  >This approach may be possible for MDPs of moderate size. However, in more complex problems, this won't always be practical.\n","\n","<br><br>\n","\n","\n","\n","#### For MDPs of large size  &ensp; (complex) <br><br>\n","\n","  - This won't be practical ! <br>\n","  >Consider the game of chess for example. There are around $10^{45}$ of possible states. constructing and solving the resulting system of Bellman equations would be a whole other story ...\n","\n","<br><br><br>\n","\n","\n","However, This simple chess game represents a tiny fraction of human experience, and humans can learn to do many things. Our agent should be able to learn many things too. <br><br>\n","\n",">In upcoming lessons we will discuss algorithms based on Bellman equations, that can scale up to large problems. \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3CDOOsGHb3eb","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","\" __Bellman Equations__ are so __fundamental for R.L.__ \" <br><br>\n","\n","\n","Without the Bellman equation, we might have to infinite number of possible futures. The Bellman equations exploit the structure of the MDP formulation, to reduce this infinite sum to a system of linear equations. We can then potentially solve the Bellman equation directly to find the state values. <br><br>\n","\n","\n","  - You can use the __Bellman Equations__ to solve __for a value function__ by writing a __system of linear equations__ <br><br>\n","\n","  - We can only solve __small MDPs__ directly, but Bellman Equations will factor into the solutions we see later for __large MDPs__\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]}]}
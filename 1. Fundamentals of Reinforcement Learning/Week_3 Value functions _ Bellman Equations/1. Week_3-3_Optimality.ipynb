{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week_3-3_Optimality","provenance":[],"private_outputs":true,"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOgKcd0j3H+Iswd13RA8/5S"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"OOB922yvbPRg","colab_type":"text"},"source":["## 3. Optimality <br><br>\n","\n",">Up to this point, we've generally talked about a policy as something that is given. <br><br>\n",">\n",">The __policy__ specifies __how an agent behaves__. Given this way of behiving, <br>\n",">we __then__ aim to __find the value of function__. \n","\n","<br>\n","\n","But __the goal of R.L.__ is __not__ just to __evaluate specific policies__. <br>\n","Ultimately, we want to __find a policy__ that obtains as __much reward__ as possible in the long run. \n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VT4om1eSvPwD","colab_type":"text"},"source":["## $\\cdot$ Optimal Policies <br><br>\n","\n","\n","Let's make this notion __(finding a good policy)__ precise with the idea of an __optimal policy__. <br><br><br>\n","\n","\n","\n","  - Define an __optimal policy__ <br><br>\n","\n","  - Understand how a __policy__ can be at least as __good__ as every other policy __in every state__ <br><br>\n","\n","  - Identify an __optimal policy__ for a __given MDP__\n","\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"QsU8cGlebrhn","colab_type":"text"},"source":["### Define an optimal policy <br><br>\n","\n","\n","\" What it means __for one policy to be better than the other policy__ ? \" <br><br>\n","\n","To define an optimal policy, we first have to understand it. <br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1VLTgTX_1QXjZJ1KZpCD8bHECEcW28AbQ\" alt=\"1-19\" width=\"500\"> <br><br>\n","\n","  - Which one policy is better ??? <br><br>\n","  \n","  Here are the value of two policies polotted across state. <br>\n","(illustrated as a countinuous line. discrete states case is the same as this) <br><br>\n","\n","  This plot illustrates that <br>\n","$\\pi_1$ achieves a higher value in some states, and <br>\n","  $\\pi_2$ achieves a higher value in other states. <br><br>\n","\n","  $\\rightarrow \\quad$ So it doesn't make much sense to say $\\pi_1$ is better than $\\pi_2$, or that $\\pi_2$ is better than $\\pi_1$. <br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=12PBuiKi6F2LELaoxb02S0EKr2SbvkDd8\" alt=\"1-20\" width=\"500\"> <br><br>\n","\n","  - In this diagram shown here, <br>\n","  the line visualizing the value of $\\pi_1$ is always above the line for $\\pi_2$. <br><br>\n","\n","  In this case, we will say policy $\\pi_1$ is as good as or better than policy $\\pi_2$, <br>\n","  if and only if the value under $\\pi_1$ is grater than or equal to the value under $\\pi_2$ for every state. <br><br>\n","\n","  $\\rightarrow \\quad \\pi_1 \\geq \\pi_2$ <br>\n","  >Just in this case, we denote the relationship between policies with a grater than or equal to sign.\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lfmTq6f_bNTn","colab_type":"text"},"source":["### Optimal policy <br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1BNGc-hLO6l8DAgHLCbg8ZY0IugM-enJj\" alt=\"1-21\" width=\"500\"> <br><br>\n","\n","\n","  - An __optimal policy__ will have __the highest possible value in every state__. <br>\n","  \" An optimal policy $\\pi_*$ is as good as or better than all the other policies \" <br><br>\n","\n","  Denote any optimal policy to $\\pi_*$ <br><br>\n","\n","  __There's always at least one optimal policy.__ <br>\n","  (there may be more than one) \n","\n","<br><br><br>\n","\n","\n","\n","### Why there must exist optimal policies ? <br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=12qHDx1_1qTX87jsLkudiV9BhBqhJ3WfN\" alt=\"1-22\" width=\"500\"> <br><br>\n","\n","  - There could be a situation the agent's doing well in one state requires doing badly in another (state). <br><br>\n","\n","\n","  $\\pi_1$ does well in some states while policy $\\pi_2$ does well in others. <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=10-0gm0BnkIaUJzTSfO0m32O66RtiH-CF\" alt=\"1-23\" width=\"500\"> <br><br>\n","\n","  - We could combine these two policices $\\pi_1$ and $\\pi_2$ into a third policy $\\pi_3$ ! <br><br>\n","\n","  __combined $\\pi_3$ always__ choose actions according to whichever of policy $\\pi_1$ and $\\pi_2$ has __the highest value in the current state__. ( $\\pi_3$ will necessarily have a value greater than or equal to both $\\pi_1$ and $\\pi_2$ in every state ) <br><br>\n","\n","  $\\rightarrow \\quad$ So the agent will never have a situation doing well in one state requires sacrificing value in another. <br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1sTtddMmbcgKkL_Wv0PuO5Yvffofu0pik\" alt=\"1-24\" width=\"500\"> <br><br>\n","\n","\n","  - Because of this, <br>\n","there always exists some policy which is best in every state. <br><br><br>\n","  \n","  This is only an informal argument. <br>\n","But there's a rigorous proof showing that <br>\n","  there __must always__ exist at least __one optimal ' deterministic policy '__. <br><br>\n","  \n","  >Notice <br>\n","  >that in some states $\\pi_3$ has a strictly greater value than either $\\pi_1$ or $\\pi_2$. <br><br>\n","  >\n","  >(As an exercise, try to explain how this is possible given that $\\pi_3$ simply chososes actions according to whichever of $\\pi_1$ and $\\pi_2$ has a higher value in each state)\n","\n","<br><br><br>\n","\n","\n",">Week 4 예시에서 보면 <br>\n",">Optimal policy 를 iterative 하게 찾는게 그냥 각 state 에서 가장 action-value (다음 state-value) 가 높은 action 만 고르게 하는 policy 를 가리킴 <br>\n",">즉, 유리한 action 만 고르게 하면 Optimal policy $\\pi_{*}$ ! \n","\n","\n","<br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KQX-6JTVnm1C","colab_type":"text"},"source":["### Example of Optimal policy <br><br>\n","\n","Consider two choice MDP shown here. <br>\n","The only decision to be made is in the top state labeled $X$. <br>\n","The agent can take either action $A_1$ or $A_2$. <br>\n","\n","![1-25](https://drive.google.com/uc?id=1hIejbH1xaFUS2kZdiBKN_42AsxiDw50R) <br><br>\n","\n","From state $X$, action $A_1$ takes the agent to state $Y$. <br>\n","In state $Y$, only action $A_1$ is available and it takes the agent back to state $X$. <br><br>\n","\n","From state $X$, action $A_2$ takes the agent to state $Z$. <br>\n","In state $Z$, only action $A_2$ is available and it takes the agent back to state $X$. <br><br>\n","\n","The numbers show the rewards the agent receives after each action. \n","Notice that while $A_1$ offers an immediate reward of $+1$, $A_2$ offers a larger reward of $+2$ after single-step delay. <br><br>\n","\n","\n","\n","![1-26](https://drive.google.com/uc?id=15LXMmEbKs-jFoeVRPWcw0zilj6J3VSJm) <br><br>\n","\n","\n","\n","There are only two __deterministic policies__ in this MDP, <br>\n","which are completely defined by the agent's choice in state $X$. <br>\n","(taking action $A_1$ or taking action $A_2$) <br>\n","Let's call these $\\pi_1$ and $\\pi_2$. <br><br>\n","\n","\" Which of these two policies is optimal ? \" <br>\n","The optimal policy will be the one for which the value of $X$ is highest. <br>\n","The answer depends on the discount factor $\\gamma$. <br><br>\n","\n","  - In the case that $\\gamma = 0$ <br>\n","  $\\rightarrow \\quad \\pi_1$ is the optimal policy ! <br><br>\n","\n","  the value is defined using only the immediate reward. <br>\n","    - $v_{\\pi_1}(X) = 1$ <br>\n","    - $v_{\\pi_2}(X) = 0 \\quad$ ... Cuz $+2$ reward occurs after a one-step delay <br>\n","    &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; (which does not affect the return) <br><br>\n","\n","\n","  - In the case that $\\gamma = 0.9$ <br>\n","  $\\rightarrow \\quad \\pi_2$ is the optimal policy ! <br><br>\n","\n","  the value is defined using only the discounted reward by $\\gamma$. <br>\n","  the value of $X$ under each policy is an infinite sum. <br>\n","    - $v_{\\pi_1}(X) = 1 + (0.9)*0 + (0.9)^2*1 + ... \\; \\approx \\; 5.3$ <br>\n","    - $v_{\\pi_2}(X) = 0 + (0.9)*2 + (0.9)^2*0 + ... \\; \\approx \\; 9.5$ <br><br>\n","\n","\n","\n","\n","![1-27](https://drive.google.com/uc?id=1jGLyMOJTwf9L7haGy5neU6LrQhRj5S2y) <br><br>\n","\n","\n","In these two choice MDP, finding the optimal policy was relatively straighforward. <br>\n","There were only two deterministic policies, and we simply had to compute the value functio for each of them. <br><br>\n","\n","\n","But<br>\n","In general, finding the optimal policy will note be so easy. <br>\n","Even if we limit ourselves to deterministic policies, the number of possible policies is equal to the number of possible actions to the power of the number of states. <br><br>\n","\n","We could use a brute force search where we compute the value function for every policy to find the optimal policy. But it's not hard to see this will become intractable for even moderately large MDP. <br><br><br>\n","\n","\n","Luckily, there's a better way to organize our search of the policy space. <br>\n","The solution will come in the form of yet another set of Bellman equations, <br>\n","$\\rightarrow \\quad$ __the Bellman Optimality Equations__. \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CAI0d5Er_tMm","colab_type":"text"},"source":["### Summary <br><br>\n","\n","  - An __optimal policy__ is defined as __the policy with the highest possible value function in all states__ <br><br>\n","\n","  - At least one optimal policy always exists (there may be more than one) <br><br>\n","\n","  - The exponential number of possible policies makes searching for the optimal policy by brute force intractable\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"68VnuuwI3gg7","colab_type":"text"},"source":["## $\\cdot$ Optimal value functions <br><br>\n","\n","Optimal policies achieve the goal of R.L. to optain as much reward as possible in the long run. <br><br>\n","\n","Let's describe the related notion of an __Optimal value function__, <br>\n","and introduce __associated set of Bellman equations__.  <br><br><br>\n","\n","\n","\n","  - Derive the Bellman optimality equation for state-value funtions <br><br>\n","\n","  - Derive the Bellman optimality equation for action-value functions <br><br>\n","  \n","  - Understand how the __Bellman optimality equations__ relate to the previously introduced __Bellman equations__\n"," \n","<br><br><br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pjeok8wtO0_Z","colab_type":"text"},"source":["$\n","\\DeclareMathOperator*{\\argmax}{argmax}\n","\\DeclareMathOperator*{\\argmin}{argmin}\n","$"]},{"cell_type":"markdown","metadata":{"id":"iLwtN8Y4_Ucz","colab_type":"text"},"source":["### Optimal value functions <br><br>\n","\n","\n","Bellman equation 으로부터 \n","Optimal Policy 가 Optimal value 를 갖게 만드는 Bellman equation 을 $\\displaystyle \\max_{\\pi}$ 와 $\\displaystyle \\argmax_{\\pi}$ 의 개념적 수식으로 표현\n","\n","\n","<br><br><br>\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WBbUYd5HnhlW","colab_type":"text"},"source":["#### __Recall an optimal policy__ <br><br>\n","\n","$\\pi_1 \\geq \\pi_2 \\quad \\text{if and only if} \\quad v_{\\pi_1}(s) \\geq v_{\\pi_2}(s) \\quad \\text{for all} \\quad s \\in S$ <br><br>\n","\n","An optimal policy is one which is as good as or better than every other policy, <br><br>\n","\n","__Optimal value function__ <br>\n","The value function for optimal policy thus has __the greatest value__ possible in every state. <br>\n","Express mathematically. <br>\n","$\\rightarrow \\quad \\displaystyle \\max_{\\pi}$<br><br><br>\n","\n","\n","  - __Optimal state-value function__ &ensp; $v*$ <br><br>\n","  \n","  $\\begin{align} v_{\\pi*}(s) & \\doteq \\mathbb{E}_{\\pi*} [G_t | S_t = s] \\\\ \n","& = \\displaystyle \\max_{\\pi} v_{\\pi}(s) \\quad \\quad \\quad ... \\text{for all} \\quad s \\in S \\end{align}$ <br><br>\n","  \n","  What it means\n",">$v* \\quad \\displaystyle \\max_{\\pi} v_{\\pi}(s) \\quad \\quad \\quad ... \\text{for all} \\quad s \\in S$ <br><br>\n",">\n",">Imagine we were to consider every possible policy and compute each of their values for the state s. The value of an optimal policy is defined to be the largest of all the computed values. We could repeat this for every state and the value of an optimal policy would always be the largest. <br><br>\n",">\n",">All optimal policies have this same optimal state-value function, which we denote by $v_*$. \n","\n","<br>\n","\n","  - __Optimal action-value function__ &ensp; $q*$ <br><br>\n","  \n","  $\\begin{align} q_{\\pi*}(s,a) & \\doteq \\mathbb{E}_{\\pi*} [G_t | S_t = s, A_t = a] \\\\ \n","& = \\displaystyle \\max_{\\pi} q_{\\pi}(s,a) \\quad \\quad \\quad ... \\text{for all} \\quad s \\in S \\quad and \\quad a \\in A \\end{align}$ <br><br>\n","\n","  >$q* \\quad \\displaystyle \\max_{\\pi} q_{\\pi}(s,a) \\quad \\quad \\quad ... \\text{for all} \\quad s \\in S$ <br><br>\n","  >\n","  >Optimal policies also share the same optimal action-value function. That is again the maximum possible for every state-action pair, which we denote by $q*$\n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"Uw536o5BzVRo","colab_type":"text"},"source":[">Optimal value function $v_{*}$ 과 $q_{*}$ 는 그 정의에서 이미 Optimal policy $\\pi_{*}$ 를 따른다고 가정되 있다 (?)\n","\n","<br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0jdyQo3hnkHK","colab_type":"text"},"source":["#### __Recall a Bellman equation for state-value function__ <br><br><br>\n","\n","\n","\n","$v_{\\pi}(s) = \\displaystyle \\sum_{a}{\\pi(a|s)} \\sum_{s'} \\sum_{r} p(s',r | s,a) [r + \\gamma v_{\\pi}(s')]$ <br><br>\n","\n","A Bellman equation for state-value function holds for the value function of any policy including an optimal policy. <br><br>\n","\n","$v_{*}(s) = \\displaystyle \\sum_{a}{\\pi_{*}(a|s)} \\sum_{s'} \\sum_{r} p(s',r | s,a) [r + \\gamma v_{*}(s')]$ <br><br>\n","\n","By substituting the optimal policy $\\pi_{*}$ into this Bellman equation, we get the Bellman equation for $v_{*}$ <br><br>\n","\n","$v_{*}(s) = \\displaystyle \\max_{a} \\sum_{s'} \\sum_{r} p(s',r | s,a) [r + \\gamma v_{*}(s')]$ <br><br>\n","\n","Because this is an optimal policy, we can rewrite the equation in a special form, which doesn't reference the policy iteself ! We can express this another way by replacing the sum over $\\pi_{*}$ with a max over $a$. <br><br>\n","\n","$\\quad \\quad \\displaystyle \\sum_{a}{\\pi_{*}(a|s)} \\quad \\rightarrow \\quad \\max_{a}$ <br><br>\n","\n",">Remeber, <br>\n",">There always exists an optimal deterministic policy, which selects an optimal action in every state. <br>\n",">Such a deterministic optimal policy will assign probability 1 for an action that achieves the highest value, and probability 0 fo all other actions. \n","\n","<br><br>\n","\n","\n","__Notice__ <br>\n","that __policy $\\pi_{*}$ no longer__ appears in the equation ! <br>\n","We have derived a __relationship__ that applies directly to __$v_{*}$ itself__ ! <br><br><br>\n","\n","\n","\n","__Bellman optimality equation for $v_{*}$__<br>\n","\n","We call this special form ' the Bellman optimality equation for $v_{*}$ '. <br><br>\n","\n","$v_{*}(s) = \\displaystyle \\max_{a} \\sum_{s'} \\sum_{r} p(s',r | s,a) [r + \\gamma v_{*}(s')]$ <br><br>\n","\n"," \n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"oFDQ9vVgseTd","colab_type":"text"},"source":["#### __Recall a Bellman equation for action-value function__ <br><br><br>\n","\n","\n","\n","$q_{\\pi}(s,a) = \\displaystyle \\sum_{s'} \\sum_{r} p(s',r | s,a) \\bigg[ r + \\gamma \\sum_{a'}{\\pi(a'|s')} q_{\\pi}(s',a') \\bigg]$ <br><br>\n","\n","\n","$q_{*}(s,a) = \\displaystyle \\sum_{s'} \\sum_{r} p(s',r | s,a) \\bigg[ r + \\gamma \\sum_{a'}{\\pi_{*}(a'|s')} q_{*}(s',a') \\bigg]$ <br><br>\n","\n","\n","We can make the same replacement in the Bellman equation for the action-value function. <br><br>\n","\n","\n","$q_{*}(s,a) = \\displaystyle \\sum_{s'} \\sum_{r} p(s',r | s,a) \\bigg[ r + \\gamma \\max_{a'} q_{*}(s',a') \\bigg]$ <br><br>\n","\n","\n","Here the optimal policy appears in the inner sum. <br>\n","We replace the sum over $\\pi_{*}$ with a max over $a$. <br><br><br>\n","\n","\n","\n","__Bellman optimality equation for $q_{*}$__<br>\n","\n","We call this special form ' the Bellman optimality equation for $q_{*}$ '. <br><br>\n","\n","$q_{*}(s,a) = \\displaystyle \\sum_{s'} \\sum_{r} p(s',r | s,a) \\bigg[ r + \\gamma \\max_{a'} q_{*}(s',a') \\bigg]$\n","\n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"Q584IkQSxa4b","colab_type":"text"},"source":["### __Linearity__ of policy $\\pi$ (dist.) v.s. __Non-linearity__ of $\\displaystyle \\max_a$ (function) <br><br>\n","\n","\n","![1-28](https://drive.google.com/uc?id=1dER4bti9Fvkiaoygx97YQ0R-gr9MJ7qO) <br><br>\n","\n","\n","In the simple Bellman equation problem given one specific policy, <br><br>\n","\n","  - We can find how the Bellman equations form a linear system of equations that can be solved by stadard methods. <br><br>\n","\n","  We can find the state-value function, through this linear equation solving for the Bellman equation. <br>\n","  We can get the policy $\\pi$ too. <br><br><br>\n","\n","\n","\n","In the Bellman optimality equation <br><br>\n","\n","  - We can find a similar system of equations for the optimal value. But taking the maximum over actions, $\\max_{a}$, is __not a linear operation__ ! <br><br>\n","  \n","  We can not find the optimal state-value function, through the similar linear equation solving for the optimal value ! <br>\n","  We can not get the policy $\\pi_{*}$ neither. <br><br><br>\n","\n","\n","\n","Standard techniques from linear algebra for solving linear system won't apply to non-linear operation for Optimal bellman equation with $\\max_{a}$. <br><br>\n","\n","In this course, <br>\n","For the optimal bellman equation, we will not from and solve systems of equations in the usual way. <br>\n","Instead, we will use other techniques based on the Bellman equations to compute value functions and policies. <br><br><br>\n","\n","\n","#### Why we can't use $\\pi_{*}$ in the ordinary Bellman equation to get a system of linear equations for $v_{*}$ <br>\n",">The answer is simple. <br>\n",">We don't know $\\pi_{*}$. <br><br>\n",">\n",">If we know $\\pi_{*}$, then we would have already achieved the fundamental goal of R.L.. If we can manage to solve the Bellman oprimality equation for $v_{*}$, we can use the result to obtain $\\pi_{*}$ fairly easily. We'll see how this can be done.\n","\n","\n","\n","<br><br><br>\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Aq04as6W1BsR","colab_type":"text"},"source":["#### Note <br><br>\n","\n","\n","  - 확률에 대한 분포를 나누는 확률분포 function 인 policy $\\pi$ 는 그 분포의 확률변수의 갯수만큼 equation 을 결정할 수 있는데 <br><br>\n","\n","  - 최댓값을 선택하는 함수 function 인 $\\displaystyle \\max_a$ 는 equation 으로 표현이 불가능 (?) <br>\n","  $\\rightarrow \\quad$ 따라서 iterative algorithm 으로 대소비교해서 결정 (?)\n","\n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kZXUqW3t_Xmo","colab_type":"text"},"source":["### Summary <br><br>\n","\n","  - We introduced __optimal value functions__ <br>\n","  and derived the associated __Bellman optimality eqiations__ <br><br>\n","\n","  - The __Bellman optimality equations__ relate the value of a state (or state-action pair) to it's possible successors under __any optimal policy__ <br>\n","\n","<br><br>\n","\n","\n","Next time, we'll learn how to find the optimal policy from the optimal value function. \n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NngZghBcAi43","colab_type":"text"},"source":["## $\\cdot$ Using optimal value functions to get optimal policies <br><br>\n","\n","__Optimal value functions__ and __Bellman optimality equations__ <br><br>\n","\n","Our ultimate goal is not to find the value of an optimal policy, <br>\n","Our __ultimate goal__ is but to find __the optimal policy__ itself ! <br><br>\n","\n","In this video, we will show how given an optimal value function, it is quite easy to find an __associated__ optimal policy. <br>\n","So __the two goals are almost the same !__ <br><br><br>\n","\n","\n","\n","  - Understand the __connection__ between the __optimal value functions__ and __optimal policies__ <br><br>\n","  \n","  - Verify the optimal value function for given MDPs\n","\n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9Z7pBcbGBWHG","colab_type":"text"},"source":["### Example : Optimal state-value for Grid world <br><br>\n","\n","\n","Grid world example we introduced earlier <br><br>\n","\n","All actions in state $A$ &ensp; : &ensp; transition to state $A'$ with reward of $+10$ <br>\n","All actions in state $B$ &ensp; : &ensp; transition to state $B'$ with reward of $+5$ <br>\n","All actions in else state : &ensp; transition to state beside with reward of $+0$ <br>\n","(everywhere except for bumping into the walls with reward of $-1$) <br><br>\n","\n","the discount factor &ensp; &ensp; &nbsp; : &ensp; $\\gamma = 0.9$ \n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1mVa0jA7uysYAFhR2Y70caP2xLFNMrHbc\" alt=\"1-29\" width=\"500\"> <br><br>\n","\n","Leftside here are the states and rewards, <br>\n","Rightside here are the associated __optimal state-values ($v_{*}$)__ for each state.  <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1T6S5lFiZjsv4OWoAFKgT95wzjyWCgHV7\" alt=\"1-30\" width=\"500\"> <br><br>\n","\n","\n","Notice that <br>\n","unlike with the uniform random policy (case) before, <br>\n","with optimal policies (case), the values along the bottom are not negative. <br><br>\n","\n","$\\rightarrow \\quad$ The __optimal policy__ will __not__ ever __choose to bump into the walls__. <br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1UiDe2QUoAqJt0EOiYcV9oS_IbBIfhH8F\" alt=\"1-31\" width=\"500\"> <br><br>\n","\n","\n","As a consequence, the optimal value of state $A$ is also much higher than the immadiate reward of $+10$. <br><br>\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"iJCGm6fmHL9j","colab_type":"text"},"source":["### Determining an optimal policy <br>\n","\n","Association between __optimal value for state__ and __optimal policy__ \n","\n","<br><br><br>\n","\n","\n","\n","&emsp; | picture\n","--- | ---\n","process | <img src=\"https://drive.google.com/uc?id=192o-CB4CrFzMMwFULR49dZWrK-ePT6AN\" alt=\"1-31\" width=\"500\">\n","association | <img src=\"https://drive.google.com/uc?id=1VKz0TXhYiyAded3vQDH_p5I8i4aljvas\" alt=\"1-32\" width=\"500\"> \n","\n","\n","\n","In general, <br><br>\n","\n","having $v_{*}$ makes it relatively easy to work out the optimal policy $\\pi_{*}$ <br>\n","as long as we also have access to the dynamics function $p$. <br><br>\n","\n","$\\rightarrow \\quad$ To evaluate the optimal policy $\\pi_{*}$, <br>\n","$\\quad \\quad$ we need the __optimal value__ $v_{*}$ and the __dynamics function__ $p$ <br><br>\n","\n","$... \\quad$ For the terms &ensp; $p(s',r | s,a)$ &ensp; and &ensp; $\\big[ r + \\gamma v_{*}(s') \\big]$ \n","\n","<br><br><br>\n","\n","\n","\n","because, <br><br>\n","\n","for any state $s$, <br>\n","we can look at each available actions $a$ and evaluate the boxed term <br>\n","A part of the state-value $v_{\\pi}$ with one of the part of all possible actions. <br><br>\n","\n","$... \\quad$ state-value of one-part of possible actions <br>\n","$\\qquad \\;\\; \\displaystyle \\sum_{s'} \\sum_{r} p(s',r | s,a) \\big[ r + \\gamma v_{*}(s') \\big]$ <br><br>\n","\n","\n","$\\rightarrow \\quad$ There will be some action $a$ for which this term obtains a maximum, $\\displaystyle \\argmax_{a}$. <br>\n","$\\qquad$ A deterministic policy which selects this maximizing action for each state will necessarily be optimal, <br>\n","$\\qquad$ since it obtains the highest possible value (the boxed term). \n","\n","<br><br><br>\n","\n","\n","\n","So, <br>\n","The equation shown here for $\\pi_{*}$ is thus almost the same as the Bellman optimality equation for $v_{*}$. <br><br>\n","\n","$... \\quad v_{*}(s) = \\displaystyle \\max_{a} \\displaystyle \\sum_{s'} \\sum_{r} p(s',r | s,a) \\big[ r + \\gamma v_{*}(s') \\big]$ \n","\n","<br><br><br>\n","\n","\n","\n","Optimality | &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; definition &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; | meaning\n","--- | --- | ---\n","<br><br> Optimal value <br><br> &emsp; | $\\qquad v_{*}(s) = \\displaystyle \\max_{a} \\sum_{s'} \\sum_{r} p(s',r | s,a) \\big[ r + \\gamma v_{*}(s') \\big]$ | $v_{*}$ is the maximum <br>of the boxed term over all actions.\n","<br><br> Optimal policy <br><br> &emsp; | $\\qquad \\pi_{*}(s) = \\displaystyle \\argmax_{a} \\sum_{s'} \\sum_{r} p(s',r | s,a) \\big[ r + \\gamma v_{*}(s') \\big]$ | $\\pi_{*}$ is the the argmax, <br>which is the particular action which achieves this maximum.\n","\n","<br><br>\n","\n","  - $v_{*}$ is the maximum of the boxed term over all actions. <br><br>\n","\n","  $v_{*} = \\displaystyle \\max_{a} \\sum_{s'} \\sum_{r} p(s',r | s,a) \\big[ r + \\gamma v_{*}(s') \\big]$ <br><br><br>\n","\n","\n","  - $\\pi_{*}$ is the the argmax, which is the particular action which achieves this maximum. <br><br>\n","\n","  $\\pi_{*} = \\displaystyle \\argmax_{a} \\sum_{s'} \\sum_{r} p(s',r | s,a) \\big[ r + \\gamma v_{*}(s') \\big]$\n","\n","\n","<br><br><br>\n","\n","\n","\n","&emsp; | picture\n","--- | ---\n","determining | <img src=\"https://drive.google.com/uc?id=1XhhdWYLhaqHL3GFSyHilywPTmYcuLBFg\" alt=\"1-33\" width=\"500\">\n","\n","<br><br>\n","\n","\n","for example, <br><br>\n","\n","imagine doing so for particular actions, labeled $A_1, A_2, A_3$, followd From upper state $s$ to lower state $s'$. <br>\n","with the evaluated term of $5, 10, 7$. (each state value for each action) <br><br>\n","\n","\n","$\\rightarrow \\quad$ To evaluate this term for a given action, <br>\n","$\\qquad$ we __need__ perform a __one step look ahead__ at the possible next states and rewards that follow. <br><br>\n","\n","$... \\quad$ Since we have access to $v_{*}$ and $p(\\;)$, we can then evaluate each term in the sum over $s'$ and $r$ ! <br>\n","$... \\quad$ this requires only a one step look ahead thanks to having access to $v_{s}$ ! <br><br><br>\n","\n","\n","\n","Of these three actions, <br>\n","$A_2$ maximizes the boxed term with a value of 10. <br><br>\n","\n","$\\rightarrow \\quad$ $A_2$ is the optimal action. \n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","### Stochastic optimal policy case <br><br>\n","\n","In fact, <br>\n","there could be more than one maximizing optimal action if multiple actions are tied. <br><br>\n","\n","If there are __multiple maximizing actions__, we could define a __stochastic optimal policy__ <br>\n","that __chooses between each__ of them __with some probability__. \n","\n","\n","\n","<br><br><br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"97G7H5SSg9xP","colab_type":"text"},"source":["### Example : Optimal policy for Grid world <br><br><br>\n","\n","\n","\n","#### Case 1. <br><br>\n","\n","\n","\" __How we can find the optimal policy__ for the grid world example ? \" <br><br>\n","\n","&emsp; | picture\n","--- | ---\n","Case 1 | <img src=\"https://drive.google.com/uc?id=11mhNcFA8fh9Cj-2DhYo8lKqXHxbb5Nhn\" alt=\"1-34\" width=\"500\">\n","\n","<br>\n","\n","describe. <br><br>\n","\n","  1. states ($s'$) and rewards ($r$) for each state ($s$) and action ($a$) <br>\n","  2. optimal state values for each state <br>\n","  $v_{*}(s) = \\displaystyle \\max_{a} \\sum_{s'} \\sum_{r} p(s',r | s,a) \\big[ r + \\gamma v_{*}(s') \\big]$\n","  3. optimal action choice for each state <br>\n","  $\\pi_{*}(s) = \\displaystyle \\argmax_{a} \\sum_{s'} \\sum_{r} p(s',r | s,a) \\big[ r + \\gamma v_{*}(s') \\big]$ <br><br><br>\n","\n","\n","\n","A one step look ahead considers <br>\n","each action $a$ and the __potential__ next states $s'$ and rewards $r$. <br><br>\n","\n","This is especially simple in this case, <br>\n","because each action leads us __deterministically__ to a specific next state and reward. <br><br>\n","\n","$\\rightarrow \\quad$ $p(s',r | s,a)$ dynamics for each action in some state is $1$ or $0$. <br>\n","$\\quad \\quad$ (if the highest value is multiple, it'll be uniformly distributed ... same probability distribution)\n","\n","\n","<br><br>\n","\n","\n","action | immediate reward &nbsp; & &nbsp; discounted next state-value &ensp; (after each specific action) \n","--- | ---\n","up | <img src=\"https://drive.google.com/uc?id=1ediNeQPpXTq8vAKfQceZvmUWh2uoYxs7\" alt=\"1-35\" width=\"500\">\n","right | <img src=\"https://drive.google.com/uc?id=1vfM4GKU87ok5geTJrOFmdw-HEkrKQq06\" alt=\"1-36\" width=\"500\">\n","down | <img src=\"https://drive.google.com/uc?id=1fjELcGZv2BMe0SauxMcw6iCUovFV1Din\" alt=\"1-37\" width=\"500\">\n","left | <img src=\"https://drive.google.com/uc?id=1R8RU-hHZ7wpsKh5caMyJOWXbYJR_2eiC\" alt=\"1-38\" width=\"500\">\n","\n","<br><br>\n","\n","\n","action | reward | next state value $v(s')$ | $p(s',r | s,a)$\n","--- | --- | --- | ---\n","up | $+0$ | $17.5$ | $0$\n","right | $-1$ | $16.0$ | $0$\n","down | $+0$ | $14.4$ | $0$\n","left | $+0$ | $17.8$ | $1$\n","\n","<br><br>\n","\n","\n","Of all these choices, <br>\n","__the highest value action__ is ' left ' at $16$. <br>\n","Therefore, left is __the optimal action__ in this state, and must be selected by any __optimal policy__ <br><br>\n","\n","As an aside, <br>\n","we've also verified that $v_{*}$ based on the Bellman optimality equation in this state. <br>\n","for the maximizing left action, the right side of the equation of value is to $16$, <br>\n","$\\big[ 0 + 0.9 * 17.8 \\big]$, which is indeed equal to $v_{*}$ for the state itself (Optimal state value ?). <br>\n","\n","\n","\n","<br><br><br>\n","\n","\n","\n","#### Case 2. <br><br>\n","\n","\n","&emsp; | picture\n","--- | ---\n","Case 2 | <img src=\"https://drive.google.com/uc?id=1AaQNcuLV_8hCp6k0brQFwRNPeLutT6kq\" width=\"500\"> \n","\n","<br>\n","\n","\n","In this state, <br>\n","two different actions, up and left, <br>\n","each gives the same optimal value of $\\big[ 0 + 0.9 * 19.8 = 17.8 \\big]$ ! <br><br>\n","\n","So there are two different optimal actions. <br>\n","An optimal policy is free to pick either with some probability. <br>\n","\n","<br><br><br>\n","\n","\n","\n","#### Case 3. <br><br>\n","\n","\n","&emsp; | picture\n","--- | ---\n","Case 3 | <img src=\"https://drive.google.com/uc?id=1bIuafHsYkSftDEXI9Y5jdfltT-6_fnCP\" width=\"500\">\n","\n","\n","<br>\n","\n","In this state $A$, <br>\n","regardless of the action we pick in state $A$, we transition to $A'$ with reward of $+10$. <br><br>\n","\n","This means that in state $A$ every action is optimal <br>\n","since the transitions are equivalent. <br><br>\n","\n","$v_{*}$ for state $A$ is $\\big[ 10 + \\gamma * v_{*}(A') \\big]$ <br>\n","$\\qquad \\qquad \\qquad \\; \\big[ 10 + 0.9 * 16.0 \\big]$ <br><br>\n","\n","which is indeed the recorded value for $v_{*}(A)$.\n","\n","\n","<br><br><br>\n","\n","\n","\n","\n","#### Case all. <br><br>\n","\n","\n","&emsp; | picture\n","--- | ---\n","Case all | <img src=\"https://drive.google.com/uc?id=1Q_SrktPVEIIDxxLA1LT3TQmUPJ2ZxJd-\" width=\"500\">\n","\n","<br>\n","\n","\n","We see the __optimal policy__ essentially heads toward state $A$ to obtain $+10$ reward as quickly as possible ! <br>\n","(Right side arrow-grid is an Optimal policies ! It is from mid-grid's Optimal state-values !)\n","\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"X2gK5bAgAKMe","colab_type":"text"},"source":["### __Deterministic__ environment case v.s. __Stochstic__ dynamics environment case <br><br><br>\n","\n","\n","dynamics $p$ environment | Optimal value $v_{*}$\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1NkcWuv51bsKqTH3Zzt1NG4SVq4ww-FGS\" alt=\"1-43\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1MUsXYneGZY1t-qTdwyVrWj6nm5viejex\" alt=\"1-43\" width=\"500\">\n","\n","<br><br>\n","\n","\n","  - Deterministic environment case <br><br>\n","\n","  Working out the __optimal policy $\\pi_{*}$ from $v_{*}$__ is especially simple in this grid world example. <br><br>\n","  \n","  Because each action leads us __deterministically__ to a __specific__ next state ($s'$) and reward ($r$). <br>\n","So we __only__ have to evaluate __one transition per action__. \n","\n","<br><br>\n","\n","\n","\n","  - Stochastic dynamics environment case <br><br>\n","\n","  __In general ...__ <br>\n","the __dynamics function__ $p$ can be __stochastic__, it might not be so simple. <br><br>\n","\n","  However, <br>\n","as long as we have __access to $p$__, <br>\n","we can __always__ find the __optimal action__ from $v_{*}$ __by computing__ the right-hand side of the __Bellman equation__ \n","$\\displaystyle \\sum_{s'} \\sum_{r} p(s',r | s,a) \\big[ r + \\gamma v_{*}(s') \\big]$ \n","for each action and __finding the largest value__. \n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","### Optimal policy from __Optimal action-value__ function <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1fAhyJiL-W8gt2s4269kmo2USElPY6oQU\" alt=\"1-43\" width=\"500\">\n","\n","\n","If we have access to the __optimal action value, $q_{*}$__, <br><br>\n","\n","\n","it's __even easier__ to come up with the __optial policy__. <br>\n","In this case, we do not have to do a one-step look ahead at all. <br>\n","We only have to select any action $a$, that maximizes $q_{*}(s,a)$. <br>\n","The action-value function caches the results of a one-step look ahead for each action. <br><br>\n","\n","In this sense, <br>\n","the problem of finding an optimal action-value function corresponds to the goal of finding an optimal policy ! \n","\n","\n","\n","\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WOF_wV-aWJXl","colab_type":"text"},"source":["### Summary <br><br>\n","\n","  - Once we have the optimal state value function, it's relatively easy to work out the optimal policy. <br><br>\n","  \n","  - If we have the optimal action-value function, working out the optimal policy is even easier. <br><br><br>\n","\n","\n","\n","This __correspondence__ between __optimal value functions__ and __optimal policies__ will help us to derive many of the Reinforced Learning algorithms. (we will explore that many algorithms)\n","\n","\n","\n","<br><br><br><br><br>"]}]}
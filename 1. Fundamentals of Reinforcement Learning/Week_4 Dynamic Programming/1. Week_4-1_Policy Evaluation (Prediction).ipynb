{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1. Week_4-1_Policy Evaluation (Prediction)","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyMwy6Js5ngLadi1bPDQunVa"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"l974Bq3_LhhG","colab_type":"text"},"source":["\\DeclareMathOperator*{\\argmax}{armax} <br>\n","\\DeclareMathOperator*{\\argmin}{argmin}\n","\n","$\\DeclareMathOperator*{\\argmax}{argmax}\n","\\DeclareMathOperator*{\\argmin}{argmin}$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yydQ9qjsKBRW","colab_type":"text"},"source":["# Week_4 <br>\n","\n","INDEX <br><br>\n","\n","\n","  - Policy evaluation (prediction) <br>\n","    - Policy evaluation v.s. control <br>\n","    - Iterative policy evaluation <br>\n","  \n","  - Policy Iteration (control) <br>\n","    - Policy improvement <br>\n","    - Policy iteration <br>\n","\n","  - Generalizeed policy iteration <br>\n","    - Flexibility of the policy iteration framework <br>\n","    - Efficiency of dynamic programming <br>\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"xPXvQ7ZHsJsA","colab_type":"text"},"source":["It's hard to improve our policy if we don't have a way to assess how good it is. <br><br>\n","\n","We will look at __a collection of algorithms__ called __dynamic programming__ <br>\n","for solving both __policy evaluation__ and __policy control__ problems. \n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"39ARKswlLNZg","colab_type":"text"},"source":["## __1. Policy evaluation (prediction)__ <br><br>\n","\n","\n","  - Policy evaluation v.s. control <br><br>\n","\n","  - Iterative policy evaluation\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"SrLl-g65LbKc","colab_type":"text"},"source":["## $\\cdot$ Policy evaluation v.s. control <br><br>\n","\n","\n","  - Understand the distinction between __policy evaluation__ and __policy control__ <br><br>\n","  \n","  - Explain __the setting__ in which dynamic programming can be applied, as well as its limitations\n","\n","\n","<br><br><br>\n","\n","\n","\n","\n","We often talk about two different tasks, <br><br>\n","\n","  1. policy evaluation <br>\n","  $\\rightarrow \\quad$ the task of __determining the value function__ for a specific policy <br><br>\n","\n","  2. policy control <br>\n","  $\\rightarrow \\quad$ the task of __finding a policy__ to obtain as much reward as possible <br>\n","  $\\qquad$ (in other words, maximizing the value function) <br><br>\n","\n","  >Policy control is the ultimate goal of reingorcement learning, but <br>\n","  >Policy evaluation is usually a neccessary first step. \n","\n","\n","<br><br><br>\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"khIbQSoRtRyf","colab_type":"text"},"source":["### __Dynamic programming__ algorithms <br><br>\n","\n","Use the __Bellman equations__ to define __iterative algorithms__ <br>\n","for both __policy evaluation__ and __policy control__\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BJK98ySJuAWz","colab_type":"text"},"source":["### Policy evaluation <br><br>\n","\n","\n","Policy evlauation is the task of determining the state value function $v_{\\pi}$ for a particular policy $\\pi$. <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1iJhxEG4B2EGGKkIFzUm_Yn-JrsdvGAOr\" alt=\"1-01\">\n","\n","<br>\n","\n","__Recall__ <br><br>\n","\n","\n","$v_{\\pi}(s) \\quad \\;$ : the value of a state under a policy $\\pi$ <br>\n","$\\qquad \\quad \\;\\;\\;$ the expected return from that state if we act according to policy $\\pi$ <br><br>\n","\n","$G_t \\qquad \\;$ : Return <br>\n","$\\qquad \\quad \\;\\;\\;$ a discounted sum of future rewards <br><br><br>\n","\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1LW9PiWQAHxTKn1-9LOcf98M7T7LmhGuh\" alt=\"1-02\"> <br>\n","\n","\n","We've seen how the Bellman equation reduces the problem of finding the value of a state $v_{\\pi}$ to a system of linear equations. <br>\n","So, the problem of __policy evaluation__ reduces __to solving the system of linear equations__ <br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n","  - theorically $\\quad : \\quad$ with a variety of method from __linear algebra__ <br><br>\n","\n","\n","$\\text{value of a state} \\; v_{\\pi} \\quad \\xrightarrow{\\quad \\text{Bellman equation} \\quad} \\quad \\text{system of linear equations} \\quad \\xrightarrow{\\quad \\quad \\text{Linear algebra} \\qquad \\; \\;} \\quad \\text{policy evaluation}$ <br><br><br>\n","\n","\n","\n","  - In practice $\\quad : \\quad$  the __iterative solution__ methods of __dynamic programming__ are more suitable __for general MDPs__ <br><br>\n","\n","\n","$\\text{value of a state} \\; v_{\\pi} \\quad \\xrightarrow{\\quad \\text{Bellman equation} \\quad} \\quad \\text{system of linear equations} \\quad \\xrightarrow{\\quad \\text{Dynamic programming} \\quad} \\quad \\text{policy evaluation}$\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8IpU_yV9yaTP","colab_type":"text"},"source":["### Policy control <br><br>\n","\n","Policy control is the task of improving a policy $\\pi$. <br><br>\n","\n","\n","better policy | optimal policy\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=11e6kfRZQn-LlpkNDKFVz96ZyOtiLM2XG\" alt=\"1-03\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1bS4U_QQ49zLgl_QUcL0N9JeD-X0WWUuG\" alt=\"1-04\" width=\"500\">\n","\n","<br><br>\n","\n","\n","The goal of the control task is to __modify a policy__ to produce a new one which is strictly __better__. <br>\n","Moreover, we can try to improve the policy __repeatedly__ to obtain a sequence of better and better policies. <br><br>\n","\n","\n",">When this is no longer possible, it means there's no policy which is strictly better than the current policy. So the current policy must be equal to an optimal policy, and we can consider the control policy task complete. \n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PFH4Zsx_L7ad","colab_type":"text"},"source":["### Dynamics of environment $\\quad : \\text{dynamics function} \\; p$ <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1MWG_gbi-HvfX-ZcQn9ygSxd8znRQ7LzK\" alt=\"1-05\"> <br><br>\n","\n","\n","Imagine <br>\n","we had access to the __dynamics of the environment $\\; p$__. <br><br>\n","\n","\n","$\\rightarrow \\quad$ __Dynamic programming__ uses the various __Bellman equations__ along with __knowledge of $p$__ <br>\n","$\\qquad$ to work out value functions $v_{\\pi}$ and optimal policies $\\pi_{*}$. <br><br>\n","\n","\n",">This week is all about how we can use this knowldege to solve the tasks of policy evaluation and policy control. Even with access to these dynamics, we'll need careful thought and clever algorithms (dynamic programming) to compute value functions $v_{\\pi}$ and optimal policies $\\pi_{*}$. <br><br>\n",">\n",">we'll investigate a class of solution methods called __dynamic programming__ for this purpose. \n","\n","<br>\n","\n",">Classical dynamic programming does not involve interaction with the environment at all. instead, we use dynamic programming methods to compute value functions and optimal policies given a model of the MDP. <br>\n",">(여기서는 주어진 MDP 모델에 대해서 dynamic programming 을 보기 때문에 모델 없이 dynamics environmen (p) 에 대해서 dynamic programming 을 보지는 않는다 ?)\n","\n","<br><br><br>\n","\n","\n","\n","Dynamic programming is very useful for understanding R.L. algorithms. Most __reinforced learning algoriths__ can be seen as an approximation to __dynamic programming__ __without the model__ ! <br><br>\n","\n","( then with the knowledge of dynamics invironment ($p$) ? ) <br><br>\n","\n",">This connection is perhaps most striking in the temporal different space __dyna planning algorithm__ (that we cover in course 2)\n","\n","\n","<br><br><br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"liHiqW0gN6H3","colab_type":"text"},"source":["### Summary <br><br>\n","\n","  - Policy evaluation is the task of determining the state-value function $v_{\\pi}$ for a particular policy $\\pi$ <br><br>\n","\n","  - Control is the task of improving an existing policy $\\pi$ <br><br>\n","\n","  - Dynamic programming techniques can be used to solve both these tasks, <br>\n","  if we have access to the dynamics function $p$\n","\n","\n","\n","<br><br><br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"agc52CC6Xp8L","colab_type":"text"},"source":["## $\\cdot$ Iterative __policy evaluation__ <br><br>\n","\n","\n","  - Outlines the __iterative policy evaluation__ algorithm for __estimating state values__ under a given policy <br><br>\n","\n","  - Apply iterative policy evaluation to cuompute value functions\n","\n","<br><br><br>\n","\n","\n","\n","We'll learn how to use __dynamic programming__ for __policy evaluation__. <br><br>\n","\n","\n","$\\qquad$ Dynamic programming algorithms are obtained by <br>\n","$\\qquad$ turning the __Bellman equations__ into __update rules__. <br><br>\n","\n","\n","$\\qquad$ We'll introduce the first of these algorithms called __' iterative policy evaluation '__. <br>\n","$\\qquad$ (Dynamic programming 중 하나인 iterative policy evaluation ?)\n","\n","<br><br><br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YOQNl3bXfDTq","colab_type":"text"},"source":["### Iterative policy evaluation algorithm <br><br>\n","\n","Outlines the __iterative policy evaluation__ algorithm for __estimating state values__ under a given policy \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Pm1-fIXRE-Ea","colab_type":"text"},"source":["### __Iterative update__ with the Bellman equation's __' recursive form '__ <br><br>\n","\n","\n","__Bellman euqation__ gives us a __recursive expression__ for $v_{\\pi}$ <br>\n","The idea of __' iterative policy evaluation '__ comes from that. <br><br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1qfycH-af1eSkpNPjPixJT_rQzUZkbJVg\" alt=\"1-06\" width=\"500\"> <br>\n","\n","\n","\n","Instead of an equation which holds for the true value function, <br><br>\n","\n","we have a procedure we can apply to iteratively refine our estimate of the value function. <br>\n","$\\Rightarrow \\quad$ This will produce a sequence of better and better __approximations__ to the __value function__\n","<br><br><br>\n","\n","\n","\n","$\\qquad \\; \\text{Take the Bellman equation} \\quad \\rightarrow \\quad \\text{Use it as an update rule}$ \n","\n","<br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8yKt3kSCU2TQ","colab_type":"text"},"source":["### Approximation to the Bellman equation <br><br>\n","\n","\n","__Visualization of how the procedure works__ <br><br>\n","\n","\n","\n","$\\quad \\;\\;$ state value $\\quad$ | picture\n","--- | ---\n","$v_0$ | <img src=\"https://drive.google.com/uc?id=1lc0em2nIzI-AmrKu3sFXrHew-G3nLlcM\" alt=\"1-07\" width=\"400\">\n","$v_1$ | <img src=\"https://drive.google.com/uc?id=1WVLcmY44twGXGcmTXijbOD8GSydb0K6Z\" alt=\"1-08\" width=\"400\">\n","$v_2$ | <img src=\"https://drive.google.com/uc?id=1TrB79ArX-etTehGaZJPzmfvgm6YbgQYa\" alt=\"1-09\" width=\"400\">\n","$v_3$ | <img src=\"https://drive.google.com/uc?id=1DGDetZOb5jcvCv9ZHuGCKTCunD-AkEh-\" alt=\"1-10\" width=\"400\">\n","$v_4$ | <img src=\"https://drive.google.com/uc?id=1q083GnujFBfnNafNJlUr8rCYt4UqHIy0\" alt=\"1-11\" width=\"400\">\n","$v_5$ | <img src=\"https://drive.google.com/uc?id=12zHPKVUV9DsQshWvKEx4n473sizRAInX\" alt=\"1-12\" width=\"400\">\n","$v_6$ | <img src=\"https://drive.google.com/uc?id=1OTGQZ67UJjQdAPyrQK8-7_1Kzi-fMXOO\" alt=\"1-13\" width=\"400\">\n","\n","<br><br>\n","\n","\n","begin with an __arbitrary initialization__ ($v_0$) for our approximate value function. <br>\n","Then, each iteration produces a __better approximation__ by using the __update rule__ (based on the Bellman equation). <br>\n","Each iteration applies this updates to every state $s$, in the state space, which we call a __sweep__. \n","\n","\n","<br><br>\n","\n","\n","optimal state value | picture\n","--- | ---\n","$v_{\\pi}$ | <img src=\"https://drive.google.com/uc?id=1kwxgjPSmw5bntTIAotK9IjdLubEYOohf\" alt=\"1-14\" width=\"400\">\n","\n","<br><br>\n","\n","\n","Applying this update repeatedly leads to a better and better approximation to the state value function $v_{\\pi}$. <br>\n","If this update leaves the value function approximation unchanged, ($\\text{if} \\;\\; v_{k+1} = v_k \\;\\; \\text{for all states}$), <br>\n","then $v_k = v_{\\pi}$ and we have found the value function ! <br><br>\n","\n",">This is because $v_{\\pi}$ is the unique solution to the Bellman equation. <br>\n",">The only way the update could leave $v_k$ unchanged is if $v_k$ already obeys the Bellman equation. (?)\n","\n","\n","<br><br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1abK5Tdq5B2WUX453gEqxLDbOMfoBb_8x\" alt=\"1-15\" width=\"500\"> <br><br>\n","\n","\n","It can be proven that <br>\n","$v_k$ will converge to $v_{\\pi}$ for any choice of $v_0$ in the limit as $k$ approaches infinity. <br><br>\n","\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n"]}]}
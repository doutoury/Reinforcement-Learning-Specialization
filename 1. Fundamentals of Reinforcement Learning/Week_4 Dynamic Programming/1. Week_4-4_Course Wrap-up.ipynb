{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1. Week_4-4_Course Wrap-up","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyOmC7g3GMRpLWXeevR18J1+"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZRMLDyNTyj7e","colab_type":"text"},"source":["## Course Warp-up &ensp; : The Funcdamentals of Reinforcement Learning <br><br>\n","\n","\n","### __Bandit Problem__ <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1m8DTK8ztm2iS5W-0xnKPiJLsg5zVQL6F\" alt=\"4-01\" width=\"500\">\n","\n","We started off with an introduction to the idea of choosing actions to maximize reward in Bandits. <br>\n","In bandits, we have a fixed set of actions (or arms) to choose form. <br>\n","Each action gies us a reward according to some unknown distribution. <br>\n","We would like to always pull the arm that provides the highest reward on average. <br><br>\n","\n","Since we don't know the reward distributions initially, <br>\n","we have to try each arm many times to get an idea of each average. <br><br><br>\n","\n","\n","\n","__Exploitation-Exploration trade-off__ <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1M8dRLdcqIv5I-DgME-ehcp_7noHwu6fR\" alt=\"4-02\" width=\"500\">\n","\n","This brought us the exploration-eploitation trade-off. <br>\n","Pull the arm that looks best now too much, and you might miss out on another better arm that only appeared worse due to insufficient information. <br>\n","Spend too long exploring all the possibilities, and you might sacrigice exploiting an arm that you have good reason to believe has much higher value. <br><br>\n","\n",">We talked about various strategies to handle this trade-off. \n","\n","<br><br>\n","\n","\n","__Bandits do not include everything__ <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1ykQH_N72LLR9buAoVOF6DBfQBQMnDUhw\" alt=\"4-03\" width=\"500\">\n","\n","The k-arm bandit problem present the agent with the same situation at each time-step. <br>\n","There is a simgle best action, and no need to associate different actions to different situations. <br><br>\n","\n","The impact of the agent's action selection is immediate, and the reward is not delayed. \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tIHmPCs-29Hb","colab_type":"text"},"source":["### __Markov Decision Processes__ <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1RTtK98NJ6KHWEK1MjPBecIdYiyY27yl4\" alt=\"4-04\" width=\"500\">\n","\n","To better model the comlexity of real-world problems, <br>\n","we introduced Markove Decision Processes (MDPs). <br><br>\n","\n","__Long-term benefit (?)__ <br>\n","In MDPs, <br>\n","the action chosen by the agent impacts not only the immediate reward but also next state ! <br>\n","In turn, this affects the potential for future reward. <br>\n","So actions can have a long-term consequence. <br><br>\n","\n","__Return__ <br>\n","We introduced the idea of Return, <br>\n","which is a potentially discounted sum of future rewards. <br><br>\n","\n","__MDP formalism for real-world problem__ <br>\n","The MDP formalism can be usde to model many interesting real-world problems ! <br>\n",">The solution methods we explore in this specialization will be applicable to a broad range of problems.The first step is always to frame your problem as an MDP ! \n","\n","<br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ScmzscCm5RUd","colab_type":"text"},"source":["### __Basic Concepts of Reinforcement Learning__ <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1EzTNFxTD_RYwZhaGlEASWJQ6kmRlIGtE\" alt=\"4-05\" width=\"500\">\n","\n","\n","$\\pi$ <br>\n","The policy <br>\n","it tells the agent how to act in each state. \n","\n","<br><bt>\n","\n","\n","$v_{\\pi} \\doteq \\mathbb{E}_{\\pi} \\big[ G_t | S_t =s \\big]$ <br>\n","The value function <br>\n","It estimates the expected future return for each state, or state-action pair, under a certain policy. \n","\n","<br><br>\n","\n","$v_{\\pi} = \\displaystyle \\sum_{a} \\pi(a|s) \\sum_{s'} \\sum_{r} p(s',r|s,a) \\big[ r + \\gamma v_{\\pi}(s') \\big]$ <br>\n","Bellman Equation <br>\n","It links the value of each state (or each state-action pair) to the value of it's possible successors. \n","\n","<br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JatFI5AZ73o2","colab_type":"text"},"source":["### __Dynamic Programming__ <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1D2Kb87SvsW2rBiTG3-Drappjhv-26FwN\" alt=\"4-06\" width=\"500\">\n","\n","__Dynamic Programming__ <br>\n","Finally, we introduced Dynamic Programming algorithms. <br>\n","These algorithms proide methods for solving the two tasks of prediction and control ( of policy ). <br>\n","As long as we have direct access to the environment dynamics $p$. <br><br>\n","\n","\n","__Restriction of dynamics $p$__ <br>\n","In the Reinforcement Learning problem, <br>\n","we will not assume we know the dynamics $p$ ! <br>\n","After all, in the real world, we can't always expect to know the effect of each of our actions until we try them. <br><br>\n","\n","\n","__Central foundation for R.L.__ <br>\n","Dynamic Programming algorithms provide an central foundation for the R.L. algorithm.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bd_aCV9U94EC","colab_type":"text"},"source":["You now have all the background to understand the Reinforcement Learning setting. \n","\n","<br><br><br>\n","\n","\n","### __Next course &ensp; : Sample-based learning methods__ <br><br>\n","\n","In the next course, <br>\n","We will discuss algorithms for estimating value functions and policies directly from experience. <br>\n","These sample-based learning algorithms do not require or even estimate the transition daynamics $p$ ! \n","\n","<br><br><br>\n","\n","\n"]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1. Week_4-2_Policy Iteration (Control)","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyPWN/w1+O0OI++GbftjcEgp"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"XNYs7YcgfQwx","colab_type":"text"},"source":["### Apply Iterative Policy Evaluation <br><br>\n","\n","\n","__Apply iterative policy evaluation to compute value functions__ <br><br><br>\n","\n","\n","\n","__Sweep process to every states__ <br><br>\n","\n","\n","state | $s_1$ | $s_2$ | $s_3$ | $s_4$\n","--- | --- | --- | --- | ---\n","update | <img src=\"https://drive.google.com/uc?id=18hr7U-e1zfr2x6pkHThMy7Mg65HPrk3A\" alt=\"1-16\" width=\"300\"> | <img src=\"https://drive.google.com/uc?id=14my_pX5MUb11WIjQi5VY5eQsOju5Zb8m\" alt=\"1-17\" width=\"300\"> | <img src=\"https://drive.google.com/uc?id=123RzCrUUlniON8IZhbP1LC7YERokCj-q\" alt=\"1-18\" width=\"300\"> | <img src=\"https://drive.google.com/uc?id=1nKqej8IQS6NYGFVvu5pTS3YutpIo-UDB\" alt=\"1-19\" width=\"300\"> \n","\n","<br>\n","\n","To implement iterative policy evaluation, <br>\n","we store two arrays, each has one entry for every state. <br><br>\n","\n","$V \\quad \\; : \\;$ One array stores the __current approximate value function__ <br>\n","$V' \\quad : \\;$ Another array stores the __updated values__ <br><br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1fACrBK0Yq94kvB3udarkAitpluFhPpyQ\" alt=\"1-20\" width=\"400\"> <br>\n","\n","  - By using two arrays, <br>\n","we can compute the new values from the old one state at a time without old values being changed in the process. <br><br>\n","\n","  At the end of a full sweep, we can write all the new values into $V$ (from updated $V'$). <br>\n","Then we do the next iteration. <br><br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1Iep3q8FOPZAXI9kSxINlu9_oraTYaQ64\" alt=\"1-20\" width=\"400\"> <br>\n","\n","  - By using one array, <br>\n","  It is also possible to implement a version with only one array. <br>\n","  in which case, some updates will themselves use new values instead of old. <br><br>\n","\n","  This single array version will usually converge faster. <br>\n","This is because it gets to use the updated values sooner. \n","\n","\n","<br><br><br>\n","\n","\n","\n","For simplicity, <br>\n","we focus on the two array version for example.\n","\n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CBk4fMize6Ps","colab_type":"text"},"source":["### Example : iterative policy evaluation (on a grid world) <br><br>\n","\n","\n","\" How iterative policy evaluation works ? \" <br>\n","( on a particular example ) <br><br><br>\n","\n","\n","\n","\n","#### 4 x 4 grid world <br>\n","$\\rightarrow \\quad$ Episodic MDP (with terminal state located in the top-left and the bottom-right corners) <br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1ruUa6nXLB8bNDMV435chWpa6LAbXnr1I\" alt=\"1-22\"> <br><br>\n","\n","\n","$R = -1 \\qquad : \\;$ the reward is -1 for every transition <br><br>\n","\n","$\\gamma = 1 \\qquad \\quad : \\;$ the undiscounted case (since the problem is episodic) <br><br>\n","\n","$a_u, a_d, a_l, a_r \\; : \\;$ four possible actions in each state up, right, left, right <br>\n","$\\qquad \\qquad \\qquad$ (each action is deterministic) <br>\n","\n","$s_{1}, \\; \\text{...} \\; , s_{14} \\quad : \\;$ Each location on the grid except terminal state location <br><br>\n","\n","$\\pi \\qquad \\qquad \\; \\; \\; : \\;$ let's evaluate the uniform random policy <br>\n","$\\qquad \\qquad \\qquad$ (which selects each of the four actions 1/4 of the time) <br>\n","\n","$v \\qquad \\qquad \\quad : \\;$ the value function represents the expected number of steps until termination <br>\n","$\\qquad \\qquad \\qquad$ (from a given state) <br><br><br>\n","\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1y290ZePPGOH8P8EJHE755fCUSbnCEXKb\" alt=\"1-23\">\n","\n","<br><br>\n","\n","\n","$\\text{sweep} \\qquad \\quad : \\;$ The order we sweep through the states is not important, <br>\n","$\\qquad \\qquad \\qquad \\; \\;$ since we are using the two array version of the algorithm. <br>\n","\n","$\\qquad \\qquad \\qquad \\; \\;$ Let's assume we sweep the states first from left to right, and then from top to bottom. <br>\n","\n","$\\text{initializer} \\quad \\; \\; : \\;$ Initialize all the values in $V$ to $0$ <br>\n","$\\qquad \\qquad \\qquad \\; \\;$ (We never update the value of the terminal state as it is defined to be $0$) <br>\n","$\\qquad \\qquad \\qquad \\; \\;$ (The initial value stored in $V'$ are relevant since they'll always be updated before they are used) <br>\n","\n","\n","<br><br><br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NTE-3vvi1Dgs","colab_type":"text"},"source":["> Note. <br><br>\n",">\n",">정책 policy 의 확률은 전체 actions 중 어떤 action 을 할 지 결정하는 확률분포. <br><br>\n",">\n",">deterministic 과 stochastic 에서 stochastic 의 확률은 정책 policy 이 특정 action 을 결정했을 때에, 그 action 이 일어날지 말지에 대한 확률. (정책이 action 을 결정하여도 해당 action 이 stochastic 하게 일어나지 않을 수도 있다 !) <br>\n",">deterministic 의 경우 정책이 결정한 action 이 1의 확률로 무조건(결정)적으로 일어난다 !\n","\n","\n","<br><br><br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rb2QcSziyIxw","colab_type":"text"},"source":["### One sweep iteration proccess <br><br><br>\n","\n","\n","\n","#### __steps of iteration__ with the update to states $s_1, s_2, \\; \\text{...} \\;, s_{14}$ <br><br>\n","\n","To compute the update, we have to sum over all actions <br><br><br>\n","\n","\n","\n","sequense of states | $s_1$ | $s_2$ | $...$ | $s_{14}$\n","--- | --- | --- | --- | ---\n","updated state value | <img src=\"https://drive.google.com/uc?id=1b77b3Ke3iHPWz48-DNTVqgp44y3tqSXD\" alt=\"1-24\" width=\"400\"> | <img src=\"https://drive.google.com/uc?id=1aRY0MgtE4k_Taq1rQT3mG11LkJKoAfw2\" alt=\"1-25\" width=\"400\"> | $\\quad \\text{...} \\quad$ | <img src=\"https://drive.google.com/uc?id=1w7yfgDtSpWc4T1CEvzvsovJL3oQw-qnH\" alt=\"1-26\" width=\"400\">\n","\n","<br><br>\n","\n","\n","$s_1$ &nbsp; : &nbsp; the result is that $V'(s_1)$ is set to $-1$ <br>\n","\n","  - $a_l$ <br>\n","  Consider the left action for eacxmple one. It has probability 1/4 under the uniform random policy. The dynamics function, $p( \\; )$, is deterministic here. So only the reward $r$ and value for a $s_1'$ contributes to the sum. The sum includes $-1$ for reward, and $0$ for the value of the terminal state. <br>\n","\n","  - $a_r, a_u, a_d$ <br>\n","  Since we initialized all state values to $0$ and the reward for each transition is $-1$, the computation for all the other actions will look much the same. <br><br>\n","\n","\n","\n","$s_2$ &nbsp; : &nbsp; the result is that $V'(s_2)$ is set to $-1$ too <br>\n","\n","  - $a_l$ <br>\n","  Evaluate the left action for eaxample one. The action probability is 1/4 again. In this case, the next state is $s_1$. Although we have updated the value of $s_1$ already, the version of the algorithm we are running we'll use the old value stored in $V$. So the value for $s_1$ in the update is still $0$. <br>\n","\n","  - $a_r, a_u, a_d$ <br>\n","  Again, all the other actions will look much the same. <br><br><br>\n","\n","\n","\n","... <br><br><br>\n","\n","\n","\n","$s_{14}$ &nbsp; : &nbsp; the result is that $V'(s_2)$ is set to $-1$ too <br><br>\n","\n","  - Since every state value is initialized to $0$, <br>\n","  every state's value will be set to $-1$.\n","\n","\n","\n","<br><br><br>\n","\n","\n","\n","#### __Copy__ the updated __state value__ <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1F8CtAmxlcUW4TQ6PV-CnF1sJ3H6Bv4Hs\" alt=\"1-27\" width=\"400\">\n","\n","\n","<br><br>\n","\n","\n","After one sweep is done, update $V$ to whole updated state values of $V'$ \n","\n","\n","<br><br><br>\n","\n","\n","This has been only __one sweep__.\n","\n","\n","<br><br><br>\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4sg6GxyV-3_j","colab_type":"text"},"source":["### Whole sweep iteration proccess <br><br><br>\n","\n","\n","\n","#### The full algorithm for iterative policy evaluation <br>\n","\n","Pseudo code for Iterative policy evaluation <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1dHc2-Y3o7tYjIG0aepOJVqF36nIUxcAr\" alt=\"1-28\">\n","\n","<br><br>\n","\n","$\\text{input} \\; \\pi \\qquad \\qquad : \\;$ Take any policy we want to evaluate <br><br>\n","\n","$V \\leftarrow \\vec{0} \\; , \\; V' \\leftarrow \\vec{0} \\; : \\;$ Initialize two arrays $V$ and $V'$, and let's set them to $0$ <br><br>\n","\n","$\\text{Loop } : \\;\\; \\text{~} \\qquad \\quad : \\; $ loop one sweep of iterative policy evaluation works for multiple sweep <br><br>\n","\n","$max(\\Delta) \\qquad \\quad \\;\\;\\; : \\;$ maximum change between value $V'$ and $V$ <br><br>\n","\n","$\\theta \\qquad \\qquad \\qquad \\quad : \\;$ some user-specified constant (like threshold ?)\n","\n","\n","<br><br>\n","\n","\n","The outer loop continues until the change in the approximate value function becomes small. We track the largest update $max(\\Delta)$ to the state value in a given iteration. <br>\n","The outer loop terminates when this maximum change is less than some user-specified constant called theta $\\theta$. $\\quad \\text{...} \\quad \\Delta < \\theta$ <br><br>\n","\n","\n","Once the change in the approxiate value function is very small, this means we are close to $v_{\\pi}$. <br>\n","As discussed before, once the approximate value function stops chaning, we have converged to $v_{\\pi}$ <br><br>\n","\n","\n","$\\Rightarrow \\quad$ estimating (determining) velue function $v_{\\pi}$ !\n","\n","\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IBBDyoQtN03s","colab_type":"text"},"source":["#### steps of iteration with the update to $V$ <br>\n","\n","$\\quad$ $V$ $\\quad \\leftarrow \\quad$ $V'$ which updated through completion of sweep <br><br><br>\n","\n","\n","\n","step of sweeps | updated values of $V$\n","--- | ---\n","first sweep <br>$V_{1}$ | <img src=\"https://drive.google.com/uc?id=10UE0QFT8aHgrkP_pywjeiUIajdfQz8z2\" alt=\"1-29\" width=\"400\"> \n","second sweep <br>$V_{2}$ | <img src=\"https://drive.google.com/uc?id=10wWP2ndzXkUTqVo1tlfJA4eNR1gns6O9\" alt=\"1-30\" width=\"400\"> \n","third sweep <br>$V_{3}$ | <img src=\"https://drive.google.com/uc?id=1bWMI-aWg_rA1j0Hliq6wnJgA1QUwv1sE\" alt=\"1-31\" width=\"400\"> \n","fourth sweep <br>$V_{4}$ | <img src=\"https://drive.google.com/uc?id=1TXNC9Ia3tJURydDGR-48ZiA3XVW6_jrx\" alt=\"1-32\" width=\"400\"> \n","fifth sweep <br>$V_{5}$ | <img src=\"https://drive.google.com/uc?id=1SScQ1gTdt2WOCqT8n4H4qvf89ZY9A2lt\" alt=\"1-33\" width=\"400\"> \n","sixth sweep <br>$V_{6}$ | <img src=\"https://drive.google.com/uc?id=1_Jxn8wZXdMzDHZ8FimelgbcSHepoWrtH\" alt=\"1-34\" width=\"400\"> \n","$$\\quad \\text{...} \\quad$$ | $$\\begin{align} \\quad \\\\ \\text{...} \\\\ \\quad \\end{align}$$ \n","final sweep <br>$V_{\\pi}$ | <img src=\"https://drive.google.com/uc?id=1ZcDtCz1lX_Ae4UudwZ0sDF3ZKHLQHDsa\" alt=\"1-35\" width=\"400\"> \n","\n","<br><br>\n","\n","\n","$\\theta = 0.001 \\qquad : \\;$ stopping parameter. <br><br>\n","\n","\n","The smaller value we choose, the more accurate our final value estimate will be. <br><br><br>\n","\n","\n","\n","\n","__first sweep__ <br>\n","After first one sweep complete, the maximum change $max(\\Delta) = 1.0$. <br>\n","Since this is greater than 0.001 ($\\theta$, we carry on to the next iteration. <br><br>\n","\n","\n","\n","__second sweep__ <br>\n","After the second sweep, noticw how the terminal state starts to influence the value of the nearest states first. <br><br>\n","\n","__third sweep__ <br>\n","After one more sweep, we see that now the influence of the terminal state has propagated further. <br><br>\n","\n","\n","__&emsp; ...__ <br>\n","We can see how the value of each state is related to it's proximity to the terminal state ! <br>\n","Let's keep running untul our maximum $\\Delta$ is less than $\\theta$ <br><br>\n","\n","\n","__final sweep__ <br>\n","Here is the result we eventually arrive at. <br>\n","Our approximate value function has converged to the value function for the random policy ! \n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dM4ZvWHY9GYD","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - We can turn the __Bellman equation__ into an __update rule__, to __iteratively compute value functions__\n","\n","<br><br>\n","\n",">Soon, you'll see how these ideas can also be used for policy improvement\n","\n","\n","<br><br><br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VUbj6hdS8smv","colab_type":"text"},"source":["## __2. Policy Iteration (Control)__ <br><br>\n","\n","\n","  - Policy improvement <br><br>\n","\n","  - Policy iteration\n","\n","\n","<br><br><br>\n","\n","\n","We just looked at how __dynamic programming__ can be used to __iteratively evaluate__ a __policy__. <br><br>\n","\n","This is the first step towards the control policy task. <br>\n","The goal is to __improve__ a __policy__ ! \n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BvuUdjjW88Mp","colab_type":"text"},"source":["## __$\\cdot$ Policy improvement__ <br><br>\n","\n","\n","  - Understand the __policy improvement theorem__ <br>\n","  (and how it can be used to construct improved policies) <br><br>\n","\n","  - Use a __value function__ for a policy to produce a __better policy__ for a given MDP\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_oHWw2TdZVZl","colab_type":"text"},"source":["\n","### __Greedification__ <br><br>\n","\n","\n","Choose greedy action in each states <br>\n","with respect to the value function $v_{\\pi}$, not to the optimal value function $v_{*}$ <br>\n","( So the agent follows an arbitrary(?) policy $\\pi$, not follows optimal policy $\\pi_{*}$ ) <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1krTcv3sKkuXubF4jjMHIavXHIcttsQFO\" alt=\"1-36\">\n","\n","\n","\n","Previously, <br><br>\n","\n","__given $v_{*}$ (optimal value function)__, we can find the optimal policy ($\\pi_{*}$) __by choosing the Greedy action__. <br>\n","The greedy action __maximizes the Bellman optimality equation__ in each state. <br><br><br>\n","\n","\n","\n","Imagine, <br><br>\n","\n","instead of the optimal value function ($v_{*}$), <br>\n","we __select an action__ which is __greedy__ with respect to the __value function $v_{\\pi}$__ of an __arbitrary policy $\\pi$__. <br><br>\n","\n","$\\Rightarrow \\quad$ __\" Greeification \"__ <br><br>\n","\n","\n","$\\qquad$ What can we say about this new policy ? <br>\n","$\\Rightarrow \\quad$ It is greedy with repect to $v_{\\pi}$. \n","\n","<br><br><br>\n","\n","\n","\n","the first thing to note <br><br>\n","\n","  - this new policy must be different than $\\pi$ <br><br>\n","\n","  - If this greedification doesn't change $\\pi$, <br>\n","  then $\\pi$ was already greedy with respect to its own value function ($v_{\\pi}$). <br>\n","\n","  >This is just another way of saying that $v_{\\pi}$ obeys the Bellman optimality equation. <br>\n","  >In which case, $\\pi$ is already optimal !\n","\n","\n","<br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uezBv12BRjQM","colab_type":"text"},"source":["### __Policy improvement theorem__ <br><br>\n","\n","\n","In fact, the __new policy obtained__ in this way must be a strict __improvement on $\\pi$__, unless $\\pi$ was already optimal. <br>\n","This is a consequence of a general result called the __policy improvement theorem__. <br><br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1NzU8OPIAy5Ze8-5VtZ7W7I88pxsWauB-\" alt=\"1-37\">\n","\n","\n","Recall <br><br>\n","\n","definition of $q_{\\pi} \\quad : \\;$ Action value function <br>\n","$\\qquad \\qquad \\qquad \\quad \\;$ the value of a state if you take action $a$ and then follow policy $\\pi$. <br><br><br>\n","\n","\n","Imagine, <br><br>\n","\n","we take action $a$ according to $\\pi'$, and then follow policy $\\pi$ <br><br><br>\n","\n","\n","  - Policy $\\pi'$ is at least as good as $\\pi$ <br>\n","  If this action $a$ has higher value than the action under $\\pi$, then $\\pi'$ must be better.<br><br>\n","\n","  $\\begin{align} &\\rightarrow & q_{\\pi}\\big(s, \\pi'(s)\\big) &\\geq q_{\\pi}\\big(s, \\pi(s)\\big) \\quad \\text{for all} \\quad s \\in \\mathbb{S} \\\\ \\\\ &\\Rightarrow & \\text{then, } \\quad \\;\\; \\pi' &\\geq \\pi \\end{align}$\n","\n","\n","<br><br><br>\n","\n","\n","\n","  - Policy $\\pi'$ is stricktly better then $\\pi$, if the value is strictly greater __at least one state__. <br>\n","  If in each state, the value of the action selected by $\\pi$ is greater than or equal to the value of the action selected by $\\pi$.  <br><br>\n","\n","  $\\begin{align} &\\rightarrow & q_{\\pi}\\big(s, \\pi'(s)\\big) &> q_{\\pi}\\big(s, \\pi(s)\\big) \\quad \\text{for at least one} \\quad s \\in \\mathbb{S} \\\\ \\\\ &\\Rightarrow & \\text{then, } \\quad \\;\\; \\pi' &> \\pi \\end{align}$\n","\n","<br><br><br>\n","\n","\n","The __policy improvement theorem__ formalizes this idea.\n","\n","\n","<br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9KLCYxqp6wvl","colab_type":"text"},"source":["### Example <br><br>\n","\n","\n","\" How policy improvement theotem works ? \" <br>\n","(on the 4 by 4 grid ) <br><br><br>\n","\n","\n","\n","\n","#### Greedy $\\pi$ policy <br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1-t9LTQh8hV6gWdv3QFAhe9aXkIb88OGG\" alt=\"1-38\" width=\"500\"> <br><br>\n","\n","\n","Here's the final value function we found before. <br>\n","This is the value function for the uniform random policy <br><br>\n","\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1Ry0lBXLOwzZBT17pPV-_Ne5j46JzwLhQ\" alt=\"1-39\" width=\"500\">  <br><br>\n","\n","\n","Now, \" what might the greedy $\\pi$ policy look like ? \" <br><br>\n","\n","In each state, we need to select the action that leads to the next state with the highest value. <br>\n","(in this case, the highest value is the least neagative one) <br><br>\n","\n","The picture shows $\\pi'$ <br>\n","( The greedy actions ) <br><br>\n","\n","This is quite different from the uniform random policy we started with. <br>\n","\n",">Note. <br>\n",">the value shown here do not correspond to the values for $\\pi'$ ! (those are just for $\\pi$?)\n","\n","<br><br>\n","\n","The new policy ($\\pi'$) is guaranteed to be an improvement on the uniform random policy we started with according to the policy improvement theorem. <br><br>\n","\n","If you look more closely at the new policy, we can see that it is in fact optimal ! <br>\n","In every state, the choosen actions lie on the shortest path to the terminal state ! <br><br>\n","\n",">Note. <br>\n",">the value function we started with was not the optimal value function, <br>\n",">and yet the greedy policy with respect to $v_{\\pi}$ is optimal ! \n","\n","<br><br>\n","\n","More generally, <br>\n","the policy improvement theorem only guarantees that the new policy is an improvement on the the original. <br>\n","We cannot always expect to find the optimal policy so easily. \n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"McdcPjoGDaxc","colab_type":"text"},"source":["### Summary <br><br>\n","\n","  - The policy Imtprovement Theorem tells us that a greedified policy is a strict improvement <br><br>\n","\n","  - Use the value function under a given policy, to produce a strictly better policy \n","\n","\n","<br><br>\n","\n",">Next time : <br>\n",">How to use this result to create an iterative dynamic programming algorithm to find the optimal policy. \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZesJjpjH8fuv","colab_type":"text"},"source":["## __$\\cdot$ Policy iteration__ <br><br>\n","\n","\n","  - Outline the __policy iteration algorithm__ for finding the optimal policy <br><br>\n","\n","  - Understand the __dance of policy and value__, <br>\n","  how policy iteration reaches the optimal policy by alternating between __evaluating a policy__ and __improving a policy__ <br><br>\n","\n","  - Apply policy iteration to compute optimal policies and optimal value functions  <br><br><br>\n","\n","\n","\n",">We just learned how the value function computed for the a given policy can be used to find a better policy. <br>\n",">In this video, we will show how we can use this to find the optimal policy by iteratively evaluating and proving a sequence of policies. \n","\n","\n","<br><br><br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mZfi2-jvIgbq","colab_type":"text"},"source":["### Policy iteration algorithm <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1mNXJ2tUI5rI1UPTqJabUjfcG1BFdeFS6\" alt=\"1-40\" width=\"500\"> \n","\n","Recall <br>\n","the Policy Improvement Theorem. <br><br>\n","\n","It tells us that we can construct a strcktly better policy by acting greedily with respect to the value function of a given policy (unless the given policy was already optimal). <br><br>\n","$\\rightarrow \\quad$ \n",">given policy 가 uniform distribution 으로 주어지든 다른 정책이든 간에 <br>\n",">iterative evaluation 을 통해 추산한 values 들 중에서 큰 것만 고르는 new policy 를 고를 수 있으면 그 policy 를 골라야 더 이득 !\n","\n","<br>\n","\n","\n","[ ~ 02:00 ] <br><br>\n","\n","\n","iteration | picture\n","--- | ---\n","1st <br>Evaluation | <img src=\"https://drive.google.com/uc?id=18_ox9A7fKyJdkVfoHEO-iEJUolhwYk1E\" alt=\"1-41\" width=\"400\">\n","2nd <br>Improvement | <img src=\"https://drive.google.com/uc?id=1oorqwBo_R4QOpQsBY1In8uxN69Lr0bOP\" alt=\"1-42\" width=\"400\">\n","3rd <br>Evaluation | <img src=\"https://drive.google.com/uc?id=1hGoJ6kfyCa7Vftn4bQ1qFXiXXf5PFzSr\" alt=\"1-43\" width=\"400\">\n","4th <br>Improvement | <img src=\"https://drive.google.com/uc?id=1ejLAaY312mxNMHbd0RYJnoUk6S-2jS1G\" alt=\"1-44\" width=\"400\">\n","<br>$$...$$<br>&emsp; | <br>$$...$$<br>&emsp;\n","final <br>Iteration | <img src=\"https://drive.google.com/uc?id=1YoeNJQfPKQa2hSqU_He4BRY5puxpEtzh\" alt=\"1-45\" width=\"400\">\n","\n","\n","Iteratively alternating two steps for policy <br><br>\n","\n","policy evaluation step $\\qquad \\qquad$ (using estimation by iterative policy evaluation) <br>\n","policy improvement step $\\quad \\qquad$ (using greedification by policy inprovement theorem) <br>\n","policy evaluation step <br>\n","policy improvement step <br>\n","$\\qquad \\qquad ...$ <br>\n","\n","<br><br>\n","\n","This gives us a sequence of better policies. <br>\n","Each policy is guaranteed to be an improvement on the last unless the last policy was already optimal. <br><br>\n","\n","When we complete an iteration and the policy remains unchanged, we have found the optimal policy. <br>\n","At that point, we terminate the algorithm. \n","\n","<br><br>\n","\n","iteration | convergence\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1IC2qVpcIhQIva8m-r7TckuJGoZnhwnbm\" alt=\"1-46\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1O09fgUSLIP5obyxStbuuuDQPTe7x5p9Y\" alt=\"1-47\" width=\"500\">\n","\n","<br><br>\n","\n","\n","Each policy generated in this way is deterministic. <br>\n","There are finiter number of deterministic policies, <br>\n","so this iterative improvement must eventually reach an optimal policy. <br><br>\n","\n","\n","This method of finding an optimal policy is called policy iteration. <br><br><br>\n","\n","\n","\n",">First, evaluate our current policy $\\pi_1$, <br>\n",">which gives us a new value function that accurately reflects the value of $\\pi_1$ <br><br>\n",">\n",">Then, improve the evaluated policy $\\pi_2$ using $v_{\\pi_1}$, <br>\n",">at this point $\\pi_2$ is greedy with respect to the value function of $\\pi_1$ ($v_{\\pi_1}$ no longer reflects the value of $v_{\\pi_2}$) <br><br>\n",">\n",">Again, evaluation makes our value function accurate with respect to the policy $\\pi_2$. <br>\n",">Once we do this, our policy is once again not greedy ! <br><br>\n",">\n",">... <br><br>\n",">\n",">This dance of policy and value proceeds back and forth, <br>\n",">until we reach the only policy, which is greedy with repect to it's own value finction (the optimal policy) <br><br><br>\n",">\n",">\n",">Only at this point, <br>\n",">the policy is greedy, and the value function is accurate. \n","\n","<br><br>\n","\n","Policy iteration always makes progress towards the intersection <br>\n","by projecting first onto the line $v = v_{\\pi}$ and then onto the line where $\\pi$ is greedy wirh respect to $v$, $\\pi = \\text{greedy(v)}$. <br><br>\n","\n","The real geometry of the space of policies and value functons is more complicated ! <br>\n","but the same intuition holds. <br>\n","\n","\n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Zvds1hDRjKoY","colab_type":"text"},"source":["### Pseudo code <br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1T4lUersO3ElyXZsODeb_pQZhvg0mUrba\" alt=\"1-48\">\n","\n","<br>\n","\n","  1. We initialize $v$ and $\\pi$ in any way we like for each state $s$. <br><br>\n","\n","  2. Next, we do the iterative policy evaluation <br>\n","to make $v$ reflect the value of $\\pi$. <br><br>\n","\n","  3. Then, we do greedification. <br>\n","in each state, we set $\\pi$ to select the maximizing action under the value function. <br><br>\n","\n","  4. If this procedure changes the selected action in any state, we note that the policy is still changing, and set policy stable to false. <br><br>\n","\n","  5. After completing step 3, we chech if the policy is stable. <br>\n","if not, we carry on and evaluate the new policy. <br>\n","\n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"s9nEPBiHjc2r","colab_type":"text"},"source":["### Example : Grid world <br><br>\n","\n","\n","Now look at how this works on a simple problem to build some intuition. <br><br>\n","\n",">Previously, we showed that by evaluating the random policy and greedifying just once, we could find the optimal policy. It's not a interesting caee for policy iteration. Let's modify this problem a little bit to make the control task a bit harder.\n","\n","<br>\n","\n","problem | first state\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1fYcSbtm16piEdbNmq5qoF8Yyd4B3ffPN\" alt=\"1-49\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1SaOjwFkjWKo-BzmXjqWIJraWh0QMBrvB\" alt=\"1-50\" width=\"500\">\n","\n","\n","<br>\n","\n","Goal state $\\quad$ : $(1, 1)$ <br><br>\n","\n","Reward $\\qquad$ : $R = \\begin{cases} -10 &\\text{Bad states} \\\\ -1 &\\text{Other states} \\end{cases}$ <br><br>\n","\n","Discount $\\quad \\;\\;$ : $\\gamma = 1$ <br><br>\n","\n","Policy $\\qquad \\;\\;$ : follow uniform distribution\n","\n","\n","<br><br>\n","\n","The optima policy should follow the winding low cost path in white to the termminal state. <br>\n",">This additional complexity means that policy iteration takes several iterations to discover the path.\n","\n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"oPC52YXPvSvY","colab_type":"text"},"source":["Let's see how this play out. <br><br>\n","\n","&emsp; | evaluation | improvement\n","--- | --- | ---\n","step 1 | <img src=\"https://drive.google.com/uc?id=1c0zzx5xzuEsAlmkdAhU0MMdL9J0BvsDQ\" alt=\"1-51\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1cJlQRAl7v_uKgUexMmxb8rPHk6iaG_7a\" alt=\"1-52\" width=\"500\"> \n","step 2 | <img src=\"https://drive.google.com/uc?id=1GY3Odroqmzu4NMEQ05kodB7AWGDjj4Z8\" alt=\"1-53\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1ollJoe45jGXQ1EqKVIKPftwblzSvvqWO\" alt=\"1-54\" width=\"500\">\n","step 3 | <img src=\"https://drive.google.com/uc?id=1M_SICgCOzp2BpNNQ7Ms0rjyWk1KPDvYJ\" alt=\"1-55\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=122eP44ILDJ_0D7H_wShqqpHb2HqgtSQa\" alt=\"1-56\" width=\"500\">\n","step 4 | <img src=\"https://drive.google.com/uc?id=1oOnBsVJBETV1emjkalZdelqcVTVMqFpW\" alt=\"1-57\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1LQPuM_C_1ps--k1S-nLXOBbK1Y5wsrmf\" alt=\"1-58\" width=\"500\">\n","step 5 | <img src=\"https://drive.google.com/uc?id=16f4cjX3qt4kjyjIZphsEDZwEwxteEeTb\" alt=\"1-59\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=13pIiPnrdlBczbnf0gL0LnHgVTVPuVcU2\" alt=\"1-60\" width=\"500\">\n","\n","<br>\n","\n","~ \n","\n","<br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aYh8_gIcyx2E","colab_type":"text"},"source":["### ~ <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=18gBUeVlj8Hrk8MeCcqTuV0nTnBAvlOaL\" alt=\"1-61\" width=\"500\">\n","\n","<br><br>\n","\n","~ \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]}]}
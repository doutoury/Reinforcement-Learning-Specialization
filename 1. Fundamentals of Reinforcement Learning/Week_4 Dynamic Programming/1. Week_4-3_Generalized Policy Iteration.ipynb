{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1. Week_4-3_Generalized Policy Iteration","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyPsqyeiCQJ8IS7KDscsWLsz"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jh6mMmcngZG2","colab_type":"text"},"source":["## __3. Generalized policy iteration__ <br><br>\n","\n","\n","  - Flexibility of the Policy Iteration framework <br><br>\n","\n","  - Efficiency of Dynamic Programming\n","\n","<br><br>\n","\n","So far, <br>\n","we've presented __policy iteration__ as a __fairly rigid porocedure__. <br>\n","We alternate between evaluating the current policy and greedifying to improve the policy. <br><br>\n","\n","The framwork of __generalized policy iteration__ allows much __more freedom__ than this <br>\n","while maintaining our optimality guarantees. <br>\n","We'll outline some of these alternatives. \n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"8BdHGHhJgvSc","colab_type":"text"},"source":["## __$\\cdot$ Flexibility of the policy iteration framework__ <br><br>\n","\n","\n","  - Understand the framework of __Generalized Policy Iteration__ <br><br>\n","\n","  - Outline __Value Iteration__, an important special case of Generalized policy iteration <br><br>\n","\n","  - Understand the distinction between __synchronous__ and __asynchronous__ <br>\n","  __Dynamic Programming methods__ \n","\n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4nHPP9cahoRe","colab_type":"text"},"source":["### Generalized Policy Iteration <br><br>\n","\n","\n","Policy Iteration | Gerneralized Policy Iteration\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1AEwLWH7Cz-hHbN0maBthP13O53YLLEya\" alt=\"3-01\" width=\"400\"> | <img src=\"https://drive.google.com/uc?id=1Kz76p161AsKmSa2megoDWzEegiitzlrT\" alt=\"3-02\" width=\"400\"> \n","\n","<br>\n","\n","__Policy Iteration__ <br>\n","The dance of policy and value <br>\n","The policy iteration algorithm runs each step all the way to completion. <br><br>\n","\n","__Generalized Policy Iteration__ <br>\n","Relaxing this. <br>\n","Each evaluation step brings our estimate a little closer to the value of the current policy, but not all the way. <br>\n","Each policy improvement step makes our policy a little more greedy, but not totally greedy. <br><br>\n","\n","This process should still make progress towards the optimal policy and the optimal value function. <br>\n","In fact, the theory tells us the same thing. <br>\n","\n",">We will use the term __Generalized Policy Iteration__ to refer to all the ways we can interleave policy evalucation and policy improvement,\n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"9OvofETqPc7m","colab_type":"text"},"source":["### Value Iteration <br><br>\n","\n","The first generalized policy iteration algorithm. <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1_pnj6cN4thMGUS6yMHIaB6nkdY0_BXov\" alt=\"3-03\" width=\"500\">\n","\n","<br>\n","\n","In Value Iteration, <br>\n","We still sweep over all the states, and <br>\n","greedify with respect to the current value function. <br><br>\n","\n","However, <br>\n","We __do not__ run __policy evaluation__ to completion. <br>\n","We perform __just one sweep__ over __all the states__. <br>\n","After that, we greedify again. <br><br>\n","\n",">$\\text{One sweep to all states} \\; \\rightarrow \\; \\text{Greedifying policy} \\; \\rightarrow \\; \\\\\n","\\text{One sweep to all states} \\; \\rightarrow \\; \\text{Greedifying policy} \\; \\rightarrow \\; \\\\\n","\\; \\text{...}$ \n","\n","<br>\n","\n","\n","We can write this as an __update rule__ <br>\n","which applies directly to the __state value function__. \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"O4pi65wASUnt","colab_type":"text"},"source":["### Value Iteration algorithm <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=16a4EYdP9teGMeebJQIcRmp6ypfuROx6I\" alt=\"3-04\"> \n","\n","<br>\n","\n","Hence the name \" value iteration \". <br>\n","The update dose not reference any specific policy, <br><br>\n","\n","The full algorithm looks very similar to Iterative Policy evaluation. <br>\n","Instead of updating the value according to a fixed policy, <br>\n","we update __using the action__ that __maximized the current value estimate__. <br>\n","( Value iteration still converges to $v_*$ in the limit ) <br><br>\n","\n","We can recover the optimal policy from the optimal value function by taking the argmax. <br>\n",">In practice, <br>\n",">we need to specify a termination condition because we can't wait forever. <br>\n",">( we simply terminate when the maximum change in the value function over a full sweep is less than some small value $\\theta$ )\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XvOv0uxMUkdk","colab_type":"text"},"source":["### Avoiding full sweeps <br><br>\n","\n","Value iteration sweeps the entire state space on each iteration just like policy iteration. <br>\n","By the sweep style, there're two types of Dynamic Programmings. <br><br>\n","\n","\n","Systenatic or not | step 1 | step 2 | step 3 | step 4 | step 5 | <br><br>$$\\quad ... \\quad$$<br><br>\n","--- | --- | --- | --- | --- | --- | ---\n","Synchronous DP | <img src=\"https://drive.google.com/uc?id=1MEI0UMRiYphFR2hggWkE4fwd3QW1tjFp\" alt=\"3-05\" width=\"100\"> | <img src=\"https://drive.google.com/uc?id=1JQ8Bg0upJSfC6VvDGU2HQPExMX2SWGVy\" alt=\"3-06\" width=\"100\"> | <img src=\"https://drive.google.com/uc?id=157lHFnxmiOvk9aHIqVtNZ1lxPyGclUw6\" alt=\"3-07\" width=\"100\"> | <img src=\"https://drive.google.com/uc?id=1PWIptOWjqJHaokBKNlLw4Jjw7FvMV0oe\" alt=\"3-08\" width=\"100\"> | <img src=\"https://drive.google.com/uc?id=1yxmW23Uh020nKUD5Wgdi6z9jRkuj4R4z\" alt=\"3-09\" width=\"100\"> | <br><br>$$\\quad ... \\quad$$<br><br>\n","Asynchronous DP | <img src=\"https://drive.google.com/uc?id=1CNXh3WfTTjLPTgpAZRvxkmyn38ov3-gj\" alt=\"3-10\" width=\"100\"> | <img src=\"https://drive.google.com/uc?id=1CwQbib0sDX3xxNxyGJZbfSAikv2Hatvk\" alt=\"3-11\" width=\"100\"> | <img src=\"https://drive.google.com/uc?id=1iSbs8UccfCZYeRil9TxOk9BPwFAO6jJB\" alt=\"3-12\" width=\"100\"> | <img src=\"https://drive.google.com/uc?id=17HjrJCnXONV-QyLYyag7ptTjcx6ScoPw\" alt=\"3-13\" width=\"100\"> | <img src=\"https://drive.google.com/uc?id=1jbdlklMDLdaOpinaIdRQgvIJC-n6jAQZ\" alt=\"3-14\" width=\"100\"> | <br><br>$$\\quad ... \\quad$$<br><br>\n","\n","<br>\n","\n","\n","__Synchronous DP__ <br><br>\n","\n","Methods that perform systematic sweeps. <br>\n","This can be problematic if the statespace is large ! <br>\n","Every sweep could take a very long time. <br><br>\n","\n","\n","__Asynchronous Dynamic Programming__ <br><br>\n","\n","Methods that do not perfrom systematic sweep.\n","They update the values of states in any order, <br>\n","They might update a given state many times before another is updated even once. <br><br>\n","\n","In order to guarantee convergence, <br>\n","asynchronous algorithms must continue to update the values of all state. <br>\n",">If the algorithm updates the same three states forever ignoring all the others, This is not acceptavle because the other states cannot be correct by never updating at all.\n","\n","<br><br>\n","\n","1 | 2 | 3 | 4 | 5 | 6 | 7 | 8\n","--- | --- | --- | --- | --- | --- | --- | ---\n","<img src=\"https://drive.google.com/uc?id=1XLOGgSRIF6NE62-K2tPW9hsjFXUaSARS\" alt=\"3-15\" width=\"100\"> | <img src=\"https://drive.google.com/uc?id=1pa8R_7-pB4uNNjFuy0tBhSZD6xKf_k4P\" alt=\"3-16\" width=\"100\"> | <img src=\"https://drive.google.com/uc?id=18FGHB9ZEPGgUc8Yv1AMikbuUrCrCwLAh\" alt=\"3-17\" width=\"100\"> | <img src=\"https://drive.google.com/uc?id=11CwPPQte8AheZaELeHyRo8wk_oyqJheS\" alt=\"3-18\" width=\"100\"> | <img src=\"https://drive.google.com/uc?id=1Ejt1vJFQY2YVNiw-7RFK6fbgzMBhmJSK\" alt=\"3-19\" width=\"100\"> | <img src=\"https://drive.google.com/uc?id=1vnsyH8ycRkDV-nxWg9QqUuiRmVYfGp5Z\" alt=\"3-20\" width=\"100\"> | <img src=\"https://drive.google.com/uc?id=18jGO4hT34gAoG8rUkshOSA54MaJrgFLB\" alt=\"3-21\" width=\"100\"> | <img src=\"https://drive.google.com/uc?id=1jG7cCplDLAlYmGRpTpIGtsgUSgtwwUrH\" alt=\"3-22\" width=\"100\">\n","\n","<br>\n","\n","__Asynchronous algorithms__ <br>\n","can __propagate__ value information __quickly__ through __selcetive updates !__ <br>\n","Sometimes this can be more efficient than a systematic sweep. <br>\n",">For example, <br>\n",">an asynchronous method can update the states near those that have recently changed value.  \n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"x4dBNO1NixlF","colab_type":"text"},"source":["### Summary <br><br>\n","\n","  - __Value Iteration__ allows us to combine policy evaluation and improvement into a single update <br><br>\n","\n","  - __Asynchronous Dynamic Programming__ methods give us the freedom to update states in any order <br><br>\n","\n","  - __Generalized Policy Iteration__ unifies many algorithms <br>\n","  >Including classical DP methods, Value Iteration, Asynchronous DP, and almost all the methods you will cover in this specializatioon\n","\n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ici7eB779fRt","colab_type":"text"},"source":["## __$\\cdot$ Efficiency of Dynamic Programming__ <br><br>\n","\n","  - Dscribe __Monte Carlo sampling__ as an alternative method for learning a value function <br><br>\n","\n","  - Descirbe __Brute-Force Search__ as an alternative method for finding an optimal policy <br><br>\n","\n","  - Understand the advantage of __Dynamic Programming__ and __Bootstrapping__ over these alternative strategies for finding the optimal policy \n","\n","\n","<br><br><br>\n","\n","\n","\n","We spent the last few videos discussing how Dynamic Programming methods can allow us to compute value functions and policies. But how useful are these methods really ? <br>\n","How do they compare to simple alternatives ? <br><br>\n","\n","In this video, we'll talk about some of the other possible solution strategies. <br>\n","( Monte-Caarlo sampling , Brute-Force search,\n"," ... ) <br>\n","By comparison, Dynamic Programming is actually surprisingly efficient. \n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sVsyEKFi_yXJ","colab_type":"text"},"source":["### Monte Carlo method &emsp; : Sampling <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=18yYPiH_KySB2tiVqlOOFQx8c6ggMKM3w\" alt=\"3-23\" width=\"500\">\n","\n","<br>\n","\n","Sample-based alternative for policy evaluation. <br><br>\n","\n","__The value of each state__ can be treated as a totally __independent estimation problem !__ <br><br>\n","\n","\n","Recall : the value is the expected return from a given state <br>\n","$v_{\\pi} \\doteq \\mathbb{E} \\big[ G_t | S_t = s \\big]$ <br><br>\n","\n","__Monte Carlo sampling method__ <br>\n","The procedure is simple. <br>\n","First, we gather a large number of returns under $\\pi$ and take their average. <br>\n","This will eventually converge to the state value. <br>\n","( by the law of large number ) \n","\n","<br><br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1Mqu1XP1Ql0wEK4WSFwtpuohmJ6qT3dlA\" alt=\"3-24\" width=\"500\">\n","\n","However, <br>\n","we need a large number of returns from each state ... <br>\n","Each returns depends on many random actions, selected by $\\pi$, <br>\n","as well as many random state transitions due to the dynamics ($p$) of the MDP. <br><br>\n","\n","We could be dealing with a lot of randomness here. <br>\n","Each returns might be very different than the true state value. <br>\n","So we may need to average many returns before the estimate converges, <br>\n","and we have to do this for every single state. \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1yQfbLKqKHgE","colab_type":"text"},"source":["### Dynamic Programming &emsp; : Bootstrapping <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1lMPNFL8p_8e3W5KNNEi5T1benpyXqP6p\" alt=\"3-25\" width=\"500\">\n","\n","The key insight of Dynamic Programming <br>\n",": we do not have to treat the evaluation of each state as a seperate problem ! <br><br>\n","\n","We can use the other value estimates $S'$ we have already worked so hard to compute. <br><br><br>\n","\n","\n","\n","\n","__Bootstrapping__ <br>\n","This process of using the vlaue estimate of successor states to improve our current value estimate is known as __Bootstrapping__. <br>\n","This can be much more efficient than a Monte-Carlo method that estimates each value independently. \n","\n","\n","<br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CK-HZndCL1sq","colab_type":"text"},"source":["### Brute-Force search &emsp; : searching all <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1IvXK-srNkEjVldhrbn-dCbHAycxIZYln\" alt=\"3-26\" width=\"500\">\n","\n","Policy Iteration cumputes optimal policies. <br>\n","Brute-Force search is a possiblle alternative. <br><br>\n","\n","This method simply evaluates every possible deterministic policy one at a time, <br>\n","we then pick the one with the hiest value. <br><br>\n","\n","There're a finite number of deterministic policies, <br>\n","and there always exists an optimal deterministic policy ( by Policy Improvement Theorem ) <br>\n","So Brute-Force search will find the answer eventually.\n","\n","<br><br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1aD-tiQ7kMXWSJp2cg4DPu0YH9at1eDq2\" alt=\"3-27\" width=\"500\">\n","\n","However, <br>\n","the number of deterministic policies can be huge. <br>\n","A determinstic policy consists of one action choice per state. <br>\n","So the total number of deterministic policies is exponential in the number of states. <br>\n","Even on a fairly simple problem, this number could be massive, <br>\n","this process could take a very long time. <br><br>\n","\n",">The policy improvement theorem guarantees that policy iteration will find a sequence of better and better policies. <br>\n",">This is a significant improvement over exhaustively trying each and every policy. \n","\n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"0bHUV3lpRNFN","colab_type":"text"},"source":["### Dynamic Programming &emsp; : Efficiency on commputation time <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1VUOEotI0WHBGb_PeCzfU8Oh7g6pHJ2Eq\" alt=\"3-28\" width=\"500\">\n","\n","\n","\" How efficient is Dynamic Programming compared to these naive alternatives ? \" <br><br>\n","\n","\n","Policy Iteration is guaranteed to find the optimal policy in time polynomial in the number of states and actions ! <br>\n","Thus, Dynamic Programming is exponentially faster than the Brute-Force search of the policy space. \n","\n","\n","#### Example in practice <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=18hg3ZP8n2tSu2uwrJ2lY_DDU_ShVPeFH\" alt=\"3-29\" width=\"500\">\n","\n","\n","For example. the original 4x4 grid-world converged in just one step of Policy Iteration. <br><br>\n","\n","When we made the problem harder by adding bad states (blue), <br><br>\n","\n","By Policy Iteration, <br>\n","it still converged in just 5 iterations ! <br>\n","It might also seem alrestrictive that we have to run policy evaluation to completion for each step of policy iteration. <br>\n","In practice, this is not so bad. <br><br>\n","\n","\n","By Brute-Force search, <br>\n","It would take $4^{16}$ to search policies ...! <br>\n","It's so exhausting ! \n","\n","<br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2vvADtRoVcdn","colab_type":"text"},"source":["### Dynamic Programming is much faster in practice <br><br>\n","\n","&emsp; | Policy evaluation\n","--- | --- \n","First step | <img src=\"https://drive.google.com/uc?id=1BBUrUKgUwTImVT8-0f9xCnLrQ5CpM2nl\" alt=\"3-30\" width=\"500\">\n","Second step | <img src=\"https://drive.google.com/uc?id=1jIAi7W2Y0h3Ipr2PMzA4W2uzsBqiQtpx\" alt=\"3-31\" width=\"500\">\n","\n","<br>\n","\n","With each iteration, <br>\n","the policy tends to change less and less. <br><br>\n","\n","The policy evaluation step changes the value function less and <br>\n","thus the evaluation step typically terminates quickly. \n","\n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vpVKAB5cWrMk","colab_type":"text"},"source":["### The curse of Dimensionality <br><br>\n","\n","\n","Generally, <br>\n","solving an MDP gets harder as the number of states grows. <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=168_69TgqjDDiF63-QpnMn2JkUI3DzDX_\" alt=\"3-33\" width=\"500\">\n","\n","\n","__The curse of dimentionality__ <br>\n","It says that the size of the state space gorws exponentially as the number of state variable increases. <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1OovB6Fm2Pj4OB-d9iKb-I2KPJ12vS1Mr\" alt=\"3-32\" width=\"500\">\n","\n","<br><br>\n","\n","A single agent moving around a gridworld is fine. <br>\n","But what if we wanted to coordinate a transporation network <br>\n","of thounds of drivers moving between hundreds of locations ? <br><br>\n","\n","A raw enumeration of the possible states could lead to an exponental blow-up. <br>\n","Clearly, this would lead to problems if we try to sweep the states to perform policy iteration. \n","\n","<br><br><br>\n","\n","\n","__Various techniques for mitigating the curse of dimentionality__ <br><br>\n","\n","In fact, this is not an issue with Dynamic Programming. <br>\n","This is a statement about the difficulty of the problems we are interested in tackling. <br>\n","Various techniques for mitigating this curse exist. <br>\n","We'll continue to deal with this curse for the remainder of our time.\n","\n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nCtMmpUUZbXK","colab_type":"text"},"source":["### Summary <br><br>\n","\n","The most important takeaway is that <br><br>\n","\n","\n","  - __Bootstrapping__ can save us from performing a huge amount of unnecessary work <br>\n","  ( by exploiting the connection between the value of state and it's possible successors ) \n","\n","\n","<br><br><br><br><br>\n"]},{"cell_type":"markdown","metadata":{"id":"BJCTENIGbcQc","colab_type":"text"},"source":["## __$\\cdot$ Week_4 Summary__ <br><br>\n","\n","\n","This week, <br>\n","we learned all about __Dynamic Programming__, and <br>\n","how it can be used to solve the tasks of policy evaluation and control. <br>\n","\n",">These algorithms form the foundation for R.L. algorithms that we'll learn about later. \n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"JxTbUfVldE5A","colab_type":"text"},"source":["### Policy Evaluation <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=15c0GhLms_295yMAi1UBdPJYPWnwqpPSp\" alt=\"3-34\" width=\"500\">\n","\n","Policy evaluatio is the task of determining the state-value function $v_{\\pi}$ for a particular policy $\\pi$. <br><br>\n","\n","Iterative policy evaluation takes the Bellman equation for $v_{\\pi}$ and turns it into an update rule. <br>\n","It produces a sequence of better and better approximations to $v_{\\pi}$ \n","\n","\n","<br><br><br>\n","\n","\n","\n","### Policy Control <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1R33Fh7wf3z7LHOaBx0J3WirkI0gWDK_x\" alt=\"3-35\" width=\"500\">\n","\n","Control refers to the task of improving a policy. <br>\n","The policy improvement theorem tells us how to construct a better policy from a given policy. <br><br>\n","\n","The new policy $\\pi'$ is produced by greedifying with respect to the current values. <br>\n","$\\pi'$ is guaranteed to be strictly better than $\\pi$, unless $\\pi$ was already optimal. \n","\n","<br><br><br>\n","\n","\n","\n","### Policy Iteration &emsp; : Dynamic Programming (algorithm) <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1HPzdcMJHkirSbwqD-pu0Q3pavsgrBT84\" alt=\"3-36\" width=\"500\">\n","\n","The Dynamic Prograrmming algorithm for control is built on the policy improvement theorem. <br>\n","This algorithm is called Policy Iteration. <br><br>\n","\n","\n","Policy Iteration consists of two steps : <br>\n","  1. Policy evaluation <br>\n","  we find the value function for the current policy. \n","  2. Policy improvement &ensp; ( greedification ) <br>\n","  that makes the policy greedy with respect to the current value function.\n","\n","<br>\n","\n","These steps are repeated over and over until the policy doesn't change. <br>\n","In which case, the policies guaranteed to be optimal. \n","\n","<br><br><br>\n","\n","\n","\n","### Generalized Policy Iteration framwork <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1BYgQR0P401MLk7lbaNm3dD6Q8ul8Gaw5\" alt=\"3-37\" width=\"500\">\n","\n","In Generalized Policy Iteration, <br>\n","the evaluation and improvement steps need not run to completion. <br><br>\n","\n","One example of generalized policy iteration is Value Iteration. <br>\n","Generalized Policy Iteration also includes asynchronous dynamic programming methods.\n","\n","<br><br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1W5q-47s_DLO9OkLHVwuuZRf-jzCtjCah\" alt=\"3-38\" width=\"500\">\n","\n","\n","__Synchronous methods__ <br>\n","It repeatedly sweep over the entire state-space. <br>\n","Asynchronous methods are more flexible, and they can update states in any order. <br><br>\n","\n","\n","__Asynchronous methods__ <br> \n","It can more efficiently propagate value information. <br>\n","This can be especialy helpful when state-space is very large. <br>\n",">Asynchronous methods can be designed to focus on  a few relevant states. \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_VVo-DX-eOIB","colab_type":"text"},"source":[">In Daynamic Programming, <br>\n",">In some sense, we assume the best possible situation. <br><br>\n",">\n",">We know exactly how the MDP works, <br>\n",">and yet it still took considerable thought and clever algorithms to efficiently compute optimal policies. <br><br>\n",">\n",">We'll continue to reap the benefits of this cleverness <br>\n",">in almost all the algorithms we see in the remainder of this course."]}]}
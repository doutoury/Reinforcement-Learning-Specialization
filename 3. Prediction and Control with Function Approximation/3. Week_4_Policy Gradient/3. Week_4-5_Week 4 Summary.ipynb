{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. Week_4-5_Week 4 Summary","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyNzjdFWSLTdtGun0BMu1aN6"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"SrLl-g65LbKc","colab_type":"text"},"source":["## $\\cdot$ Week 4 Summary <br><br>\n","\n","\n","This week, <br>\n","we talked about a whole new way to do control, directly learning parameterized policies. <br><br>\n","\n","We introduced a new objective and a new algorithmic framework called <br>\n","Actor-Critic.\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Ie08S36dDoih","colab_type":"text"},"source":["### Actor-Critic <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1LdAglAndH8TutU3K-T2mlRi_zawFgSL1\" alt=\"5-01\" width=\"\">\n","\n","\n","Let's take a step back and look at where this new material fits into the bigger picture. <br><br>\n","\n","We learn policies using function approximation. <br>\n","So we're still on the left-side of the map. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1Xpog8CnQGYGpyFCJylAzswicgpnxYuMQ\" alt=\"5-02\" width=\"500\">\n","\n","<br>\n","\n","We focus on Actor-Critic <br>\n","using the average reward objective. <br><br>\n","\n","From there (red box) we introduced two possible parameterizations <br>\n","depending on whether the actions are continuous or discrete. <br><br>\n","\n","  - For discrete actions, we use the Softmax policy parameterization. <br>\n","  - For continuous action sapces, we use a Gaussian policy <br>\n","  which samples actions from a continuous range. <br><br>\n","\n","This is the first time <br>\n","we implemented an algorithm for continuous actions. <br><br>\n","\n","Average reward actor-critic <br>\n","can be used in the same settings as the differential semi-gradient SARSA algorithm (introduced previously).\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LUwJKpvdGr7u","colab_type":"text"},"source":["### Parameterized Policy <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1e_hIPBnNNxBVGUQrehMAxSN9WHTj3EWR\" alt=\"5-03\" width=\"500\">\n","\n","<br>\n","\n","We strated this week by <br>\n","making the shift from parameterized action-values to parameterized policies ! <br><br>\n","\n","Parameterized policies $\\theta$ take state-action pairs $(s,a)$ <br>\n","and output the associated action probabilities $\\pi(a|s,\\theta)$. <br><br>\n","\n","This function is parameterized by <br>\n","a vector of parameters denoted by $\\theta$. \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"krwfemHWIBAo","colab_type":"text"},"source":["### Softmax Parameterzation <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1TiNsl-1MgoWq9f-Yb9jQVyK8QEMpN_jS\" alt=\"5-03\" width=\"500\">\n","\n","<br>\n","\n","A parameterized poicy $\\theta$ <br>\n","has to be a valid probability distribution. <br><br>\n","\n","That is, <br>\n","every action parobability must be greater than $0$, <br>\n","and the sum over all actions in a given state must be $1$. \n","\n","<br><br>\n","\n","\n","$\\pi(a|s,\\theta) \\quad \\doteq \\quad \\displaystyle \\frac{e^{h(s,a,\\theta)}}{\\sum_{b \\in A}e^{h(s,b,\\theta)}}$ <br><br>\n","\n","A Softmax over action preferences <br>\n","is one way to ensure the parameterized policy obeys these constraints in discrete action spaces.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"l1zxZEvoJgl5","colab_type":"text"},"source":["### The Advantaged of Policy Parameterization <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1tGwHVoSlPQBc2y_p5jZAR12kv3b49OXu\" alt=\"5-05\" width=\"500\">\n","\n","\n","Parameterizing policies $\\theta$ <br>\n","directly has a number of potential advantages. <br><br>\n","\n","  - These advantages include the ability to autonomously converge to a deterministic policy over time. <br>\n","  ( while using a stochastic policy to explore early on ) <br><br>\n","\n","  - Another advantage is the ability to learn stoachastic policies. <br>\n","  This can be useful with fucntion approximation. <br>\n","  ( when the optimal deterministic policy is not representable ) <br><br>\n","\n","  - Finally, A good policy mitht be easier to learn and represent than precise action-values. \n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CJ4-e8sMLQW5","colab_type":"text"},"source":["### The Average Reward Objective <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1PeD8WDkN3Q3FrePKQ7JEWZm7rQCSEFqG\" alt=\"5-06\" width=\"500\">\n","\n","<br><br>\n","\n","\n","To learn parameterized policies $\\theta$, <br>\n","we had to consider a new objective and a new strategy to optimize it ! \n","\n","<br><br>\n","\n","\n","We revisited the average reward objective. <br><br>\n","\n","We showed that the average reward could be expended out like this. <br>\n","$r(\\pi) \\quad = \\quad \\displaystyle \\sum_s \\mu(s) \\sum_a \\pi(a|s,\\theta) \\sum_{s',r} p(s',r|s,a)$ \n","\n","<br><br>\n","\n","\n","Our strategy to optimize this objective was <br>\n","to use Stochastic Gradient Descent. <br><br>\n","\n","For that, <br>\n","we needed an estimate of the gradient of the average reward $\\nabla r(\\pi)$. <br><br>\n","\n","This is chanllenging <br>\n","because the state distribution $\\mu(s)$ depends on the policy parameters $\\theta$.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SdYrTu8-M6w3","colab_type":"text"},"source":["### The Policy Gradient Theorem <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1lFa9z4NRfsJHX9d9V3MCN_9l5--JBogb\" alt=\"5-07\" width=\"500\">\n","\n","<br><br>\n","\n","The Policy Gradient Theorem <br>\n","provides an expression for the gradient $\\nabla$ of the average reward objective $r(\\pi)$ <br>\n","that's convenient to optimize. <br><br>\n","\n","This form allows us to estimate the gradient $\\nabla r(\\pi)$ <br>\n","by sampling states from $\\mu$ <br>\n","by following the policy $\\pi$.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vcZ6h2n0OSdC","colab_type":"text"},"source":["### The Actor-Critic Algorithm <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1vyzFKCWVXPXJJ1kTFgyc6diLQhcJuIZ0\" alt=\"5-08\" width=\"500\">\n","\n","<br>\n","\n","Using the policy gradient theotem, <br>\n","we derive the actor-critic algorithm.  <br><br>\n","\n","Acto-Critic simultaneously learns <br>\n","  - a parameterized policy ( the Actor ) and <br>\n","  - an estimate of the policie's value function ( the Critic ) \n","\n","<br><br>\n","\n","\n","The TD error from the Critic reflects that the Actor did better or worse than the critic expected. <br><br>\n","\n","The Actor is trained to favor actions that exceed the Critic's expectations, <br>\n",">the critic is trained to improve it's value estimates of the actor <br>\n",">so that it knows what value it should expect for this actor.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"U6j6RdpzQBZG","colab_type":"text"},"source":["### Actor-Critic for Discrete Actions and Continuous Actions <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1jG4HN9bOC-ARr8FJw_C7WUkWBONs-Kla\" alt=\"5-09\" width=\"500\">\n","\n","<br>\n","\n","We demonstrated <br>\n","how to the Actor-Critic algorithm can be used for both discrete and continuous action spaces. \n","\n","<br><br>\n","\n","For Discrete actions, <br>\n","we used a Softmax policy parameterization. <br><br>\n","\n","For Continuous actions, <br>\n","we used a Gaussian policy. <br>\n","( with state dependent mean and variance ) <br>\n","\n",">This made the Gaussian policy over actions conditional on the state.\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"n1NU9Q6CRQ9R","colab_type":"text"},"source":["### End <br><br>\n","\n","\n","Now you know how to parameterize and learn policies directly. <br><br>\n","\n","In addition to the action-value methods ( we looked at before ), <br>\n","this is a valuable new tool in your Reinforcement Learning toolbox.\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JkX0g9pUR034"},"source":["## $\\cdot$ Course Wrap-up <br><br>\n","\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. Week_4-3_Actor-Critic for continuing tasks","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyPrSCbkxz7OMMnuah2bdMbU"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"39ARKswlLNZg","colab_type":"text"},"source":["## __3. Actor-Critic for Continuing Tasks__ <br><br>\n","\n","\n","  - Estimating the Policy Gradient <br><br>\n","\n","  - Actor-Critic Algorithm\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"SrLl-g65LbKc","colab_type":"text"},"source":["## $\\cdot$ Estimating the Policy Gradient <br><br>\n","\n","\n","  -  Derive a sample-based estimate for the gradient of the average reward objective\n","\n","<br><br>\n","\n","\n","We have an objective for policy optimization. <br>\n","We also have the policy gradient theorem, <br>\n","which gives us a simple expression for the gradient of that objective. <br><br>\n","\n","In this video, <br>\n","we'll complete the puzzle by showing \" how to estimate this gradient \" <br>\n","using the experience of an agent interacting with the environment.\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Mywz-hTqxNYr","colab_type":"text"},"source":["### Getting Stochastic Samples of the Gradient <br><br>\n","\n","\n","We want to derive a gradient descent algorithm for our policy. <br>\n","We have our objective and it's gradient due to the Policy Gradient Theorem. <br><br>\n","\n","Now, we need to figure out \" how to approximate this gradient \". <br><br>\n","\n","Infact, what we will do is get a stochastic sample of the gradient.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1TZyvNfcE9zA7wDDIXMAA7fzajc7UWJTD\" alt=\"3-01\" width=\"500\">\n","\n","<br><br>\n","\n","\n","Recall <br>\n","this expression for the gradient of the average reward. <br><br>\n","\n","Computing the sum over states is really impractical. <br>\n","But we can do the same thing we did <br>\n","when deriving our stochastic gradient descent rule for policy evaluation. <br><br>\n","\n","We simply make updates from states we observe while following policy $\\pi$. <br>\n","$S_0, A_0, R_1, S_1, A_1, \\;\\;\\; \\cdots \\;\\;\\; , S_t, A_t, R_t+1, \\;\\;\\; \\cdots$ <br><br>\n","\n","This gradient from state $S_t$ provides an approximation <br>\n","to the gradient of the average reward $\\nabla r(\\pi)$. <br>\n","$\\nabla r(\\pi) \\quad = \\quad \\displaystyle \\sum_a \\nabla\\pi(a|\\color{brown}{S_t},\\theta_t) q_{\\pi}(\\color{brown}{S_t},a)$ <br><br>\n","\n","As we discussed before for stochastic gradient descent, <br>\n","we can adjust the weights with this approximation , <br>\n","and still guarantee you will reach a stationary point.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1WM-nqwiR8DSoL9QryddzE8zsxB9dmJTk\" alt=\"3-02\" width=\"500\">\n","\n","<br><br>\n","\n","\n","This is what the stochastic gradient descent update looks like <br>\n","for the policy parameters. <br>\n","$\\theta_{t+1} \\quad \\doteq \\quad \\theta_t + \\alpha \\displaystyle \\sum_a \\nabla \\pi(a|S_t,\\theta_t) q_{\\pi}(S_t,a)$ <br><br>\n","\n","We could stop here, <br>\n","but let's simplify this further.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"iFMrAklu1iAl","colab_type":"text"},"source":["### Unbiasedness of the Stochastic Samples <br><br>\n","\n","\n","Let's re-examine this from a perspective based on expectations. <br>\n","This will help us simplify the update and give you more insight into why the update makes sense. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1ES4Dy0LOpXr2VYfhXyPoriTqMLgSmIR4\" alt=\"3-03\" width=\"500\">\n","\n","<br><br>\n","\n","\n","Notice that <br>\n","the sum over states weighted by $\\mu$ <br>\n","can be re-written as an expectation under $\\mu$. <br>\n","( using sampled state $S$ instead of all every state $s$ ) <br>\n","$= \\displaystyle \\sum_s \\mu(s) \\sum_a \\nabla \\pi(a|s,\\theta) q_{\\pi}(s,a)$ <br>\n","$\\Rightarrow \\mathbb{E}_{\\mu} \\big[ \\displaystyle \\sum_a \\nabla \\pi(a|S,\\theta) q_{\\pi}(S,a) \\big]$\\ <br><br>\n","\n","Recall that <br>\n","$\\mu$ is the stationary distribution for $\\pi$ which reflects state visitation under $\\pi$. <br>\n",">Q. What is stationary distribution ? <br>\n",">A. https://en.wikipedia.org/wiki/Stationary_distribution\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1G4kQrqnabhD7l4YW7G-C_74E05nSoMok\" alt=\"3-04\" width=\"500\">\n","\n","<br><br>\n","\n","\n","In fact, <br>\n","the state's we oberve while following $\\pi$ are distributed according to $\\mu$ <br>\n","$S_t \\sim \\mu$ <br><br>\n","\n","By computing the gradient from a state $S_t$, <br>\n","we get an unbiased estimate of this expectation. <br><br>\n","\n","Thinking about our stachastic gradient as an unbiased estimate <br>\n","suggests one other simplification ! \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1AlV0xxm3TKSZPl-odc4Hc71sla_DrwfT\" alt=\"3-05\" width=\"500\">\n","\n","<br><br>\n","\n","Notice that <br>\n","inside the expectation we have a sum over all actions $\\displaystyle \\sum_a$. <br>\n","We want to make this term even simpler, and get rid of ther sum over all actions ! <br><br>\n","\n","If this was an expectation over actions, <br>\n","we could get a stochastic sample of this two(?), and avoid summing over all actions !\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9x7zS-JCuW0Z","colab_type":"text"},"source":["### Getting Stochastic Samples with One Action <br><br>\n","\n"," \n","<img src=\"https://drive.google.com/uc?id=1IOtTvakDZYtS_4fpQM2YG1Qjt0X-4bAX\" alt=\"3-06\" width=\"500\">\n","\n","<br>\n","\n","Here we're going to see <br>\n","how we can get an unbiased gradient estimate using only one action 80, <br>\n","which is the action taken by the agent. <br><br>\n","\n","It would be nice if the sum of our actions was weighted by $\\pi$ and so was an expectation under $\\pi$. <br><br>\n","\n","That way we could sample it <br>\n","using the agent's action selection, which is distributed according to $\\pi$. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1HVHdnFDrYZhyQOJAJfkOizfu2Ok3xyG-\" alt=\"3-07\" width=\"500\">\n","\n","<br>\n","\n","It turns out this is an easy problem to solve ! <br><br>\n","\n","To get a weighted sum correponding to an expectation <br>\n","we can just multiply and divided by $\\pi(a|s,\\theta)$. <br><br>\n","\n","Now we have an expectation over actions drawn from $\\pi$ for this term (in the red-box)! <br>\n","$= \\mathbb{E}_{\\pi} \\big[ \\frac{\\nabla \\pi(A|S,\\theta)}{\\pi(A|S,\\theta)} q_{\\pi}(S,A) \\big]$\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LpEqkDvXzUhZ","colab_type":"text"},"source":["### Stochastic Gradient Ascent for Policy Parameters <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1A9QR69iUPTSi7wWabg_yxFlpCqRRA7eT\" alt=\"3-08\" width=\"500\">\n","\n","<br>\n","\n","The new stochastic gradient ascent update now looks like this. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1lMY0iDhqhzkQ2vIhq5fe-Zsg0hoxnUep\" alt=\"3-09\" width=\"500\">\n","\n","<br><br>\n","\n","\n","As an aside, <br>\n","it is common to rewrite this gradient as the gradient of the natural logarithm of $\\pi$ ! <br><br>\n","\n","This is based on a formula from calculus for the gradient of a logarithm. <br>\n","$\\nabla \\big(f(x)\\big) = \\displaystyle \\frac{\\nabla f(x)}{f(x)}$ <br><br>\n","\n","Using this rule, <br>\n","we get that the gradient of log $\\pi$ equals the gradient of $\\pi$ over $\\pi$ <br>\n","$\\ln \\pi(a|s,\\theta) = \\displaystyle \\frac{\\nabla \\pi(a|s,\\theta)}{\\pi(a|s,\\theta)}$\n","\n","<br><br>\n","\n","\n","So this update is equivalent to what we started with.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VrMfp0Jw2VRF","colab_type":"text"},"source":["### Why do we do this ? <br><br>\n","\n","\n","One reason is that <br>\n","it is actually simpler to compute the gradient of the logarithm of certain distribution ! <br><br>\n","\n","The other less important reason is that <br>\n","it let's us write this gradient more compactly.\n","\n","<br><br>\n","\n",">In the end, <br>\n",">it is just a mathematical thrick. <br>\n",">so don't let it distract you from the underlying algorithm.\n","\n","<br><br>\n","\n","\n","We now have something that looks like many of the learning rules used in this course. (???) <br><br>\n","\n","We adjust the parameter $\\theta$ proportionally to a stochastic gradient of the objective. <br><br>\n","\n","We use a step size parameter $\\alpha$ to control the magnitude of the step in that direction. <br>\n","So $\\alpha$ has the same role it always has.\n","\n","<br><br>\n","\n","\n","We now have a nice clean update rule to learn the policy parameters !\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ylx4TAbN9vyd","colab_type":"text"},"source":["### Computing the Update <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1FdJctjxM5xsEgxTk0tAIknrUSzfvbmJl\" alt=\"3-10\" width=\"500\">\n","\n","<br>\n","\n","The last thing to talk about is <br>\n","how to actually compute the stochastic gradient for a given state and action. <br><br>\n","\n","We just need two components, <br>\n","  - the gradient of the policy <br>\n","  $\\nabla \\ln \\pi(A_t|S_t,\\theta_t)$ <br>\n","  - an estimate of the differential values <br>\n","  $q_{\\pi}(S_t,A_t)$ <br><br>\n","\n","The first is easy, <br>\n","we know the policy and this parameterization, <br>\n","and so can compute it's gradient. <br><br>\n","\n","The second, <br>\n","The action value can be approximated in a variety of ways. <br>\n","For example, <br>\n","we could use a TD algorithm that learns differential action-values. <br><br>\n","\n","In an upcoming video, <br>\n","we will go through one particular choice in detail, <br>\n","as well as how to compute the gradient for specific policy parameterization.\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GpDD1M_4_M37","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - we derive a policy gradient learning rule for the average reward setting\n","\n","<br><br>\n","\n","In the next video, <br>\n","we will see how to use this rule.\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Zev_L711AZVs"},"source":["## $\\cdot$ Actor-Critic Algorithm <br><br>\n","\n","\n","  - Describe the actor-critic algorithm <br>\n","  for control with function approximation <br>\n","  for continuing tasks\n","\n","<br><br>\n","\n","Do we have to choose between directly learning the policy parameters and learning a value function ? <br><br>\n","\n","No ! <br>\n","Even within policy gradient methods, valaue-learning methods like TD still have an important role to play. <br><br>\n","\n","In this setup, <br>\n","the parameterized policy plays the role of an actor, <br>\n","while the value function plays the role of a critic, <br>\n","evaluating the actions selected by the actor. <br><br>\n","\n","These so called actor-critic methods <br>\n","were some of the earliest TD-based methods introduced in Reinforcement Learning. \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LGsjo45rB-w-","colab_type":"text"},"source":["### Approximating the Action-Value in the Policy Update <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1wf-2zrdn3R-McXIIOAr5OIUG1IOPgI2J\" alt=\"3-11\" width=\"500\">\n","\n","<br>\n","\n","We finished off the last video with this expression for the policy gradient learning rule. <br>\n","$\\theta_{t+1} \\quad = \\quad \\theta_t + \\alpha \\nabla \\ln \\pi (A_t | S_t,\\theta_t) q_{\\pi}(S_t,A_t)$ <br><br>\n","\n","But, we don't have access to $q_{\\pi}$, <br>\n","so we'll have to approximate it ! \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1vdOloIMG8rTyXu74Bzq3jxtCTVfhOlS8\" alt=\"3-12\" width=\"500\">\n","\n","<br>\n","\n","We can do the usual TD thing, <br>\n","the one-step bootstrap return. <br><br>\n","\n","That is the differential reward plus the value of the next state. <br>\n","$R_{t+1} - \\bar{R} + \\hat{v}(S_{t+1},W)$ \n","\n","<br><br>\n","\n","\n","#### Critic part and Actor part of the actor-critic algorithm <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=181G46cwlgCgimFvm8eUu7gLujWZgQB1E\" alt=\"3-13\" width=\"500\">\n","\n","<br><br>\n","\n","  - Critic part of the actor-critic algorithm <br><br>\n","\n","  As usual, <br>\n","  the parameterized function $\\hat{v}(s,W)$ is learned estimate of the value function. <br>\n","  In this case, <br>\n","  $\\hat{v}(s,W)$ is the differential value function. <br><br>\n","\n","  This is the critic part of the actor-critic algorithm. <br><br>\n","\n","  The critic provides immediate feedback. <br><br>\n","\n","  To train the critic, <br>\n","  we can use any state-value learning algorithm. <br>\n","  We will use the average reward version of semi-gradient TD(0).\n","\n","<br><br>\n","\n","\n","  - Actor part of the actor-critic algorithm <br><br>\n","\n","  The parameterized policy is the actor ! <br><br>\n","\n","  It uses the policy gradient updates shown here.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zL0cWiP6TeRv","colab_type":"text"},"source":["### Subtracting the Current State's Value Estimate <br><br>\n","\n","\n","policy gradient update without baseline | policy gradient update with baseline\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=135Q9FWuGH7A9J9z_T_be5bsvF89CEYHv\" alt=\"3-14\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1yAj8Q8KsTe71dg1B5HBaIQ6yXprRpafy\" alt=\"3-15\" width=\"500\">\n","\n","<br>\n","\n","We could use this form of the update, <br>\n","but there is one last thing we can do to improve the algorithm. <br><br>\n","\n","We can subtract off what is called a baseline ! <br>\n","$\\hat{v}(S_t,W)$ is the baseline in this case. <br><br>\n","\n","Instead of using the one-step value estimate alone, <br>\n","we can subtract the value estimate for the state $S_t$ to get the update that looks like this. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1uZhuIRG5k8PV6Y2MCuVxTXmWj9NHiXNv\" alt=\"3-16\" width=\"500\">\n","\n","<br>\n","\n","Notice that <br>\n","this expression is equal to the TD error $\\delta$ ! <br><br>\n","\n","The expected value of this update is the same as the previous one. <br>\n","Why is this ?\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"k1uhDk8HWMQj","colab_type":"text"},"source":["### Adding a Baseline <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1Vw5wAmegidHFSQZCDov1P_caYDgJNH_R\" alt=\"3-17\" width=\"500\">\n","\n","<br>\n","\n","Let's take the expectation of the update conditioned on a particular state $S_t$ at time $t$. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1IW5M5RT8b9AFEFqAR1htWgpcyPu3rLQy\" alt=\"3-18\" width=\"500\">\n","\n","<br>\n","\n","Taking the expectation of the sum is the same as the sum of the expectations. <br><br>\n","\n","We can use this <br>\n","to seperate out the expectation of our original term <br>\n","from the expectation which involves the subtracted value function. <br><br>\n","\n","It turns out the expectation of the second term is $0$. <br>\n","So we can add this baseline to the update without changing the expectation of the update. <br><br>\n","\n",">You can varify this for yourself. <br>\n",">To start, write the expectation as a sum of our(?) actions, <br>\n",">and pull the $\\hat{v}()$ term out of the sum. <br>\n",">( we leave this as an exercise )\n","\n","<br><br><br>\n","\n","\n","#### So why do we add this baseline if the update is the same in expectation ? <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=188vXMKmkRjNuGz5mjz6hXbTXxGtrz0Z8\" alt=\"3-19\" width=\"500\">\n","\n","<br>\n","\n","Subtracting this baseline tends to reduce the variance of the update <br>\n","which results in faster learning !\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gx2Z8AOvaMYN","colab_type":"text"},"source":["### How the Actor and the Critic Interact <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1bd89OMN1nnXU42b4xKCWvLfa2VbyV-dI\" alt=\"3-20\" width=\"500\">\n","\n","<br>\n","\n","This update makes sense intuitively. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1uclsq8RaLJwMDFFwTiYMWRee65KMufTK\" alt=\"3-21\" width=\"500\">\n","\n","<br>\n","\n","After we execute an action, <br>\n","we use the TD error to decide how good the action was compared to the average for that state. <br><br>\n","\n","If the TD error is positive, <br>\n","then it means the selected action resulted in a higher value than expected. <br>\n","Taking that action more often should improve our policy.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1-36_IdQ98E7iKjq4baJ-mvUyhOo_FKV9\" alt=\"3-22\" width=\"500\">\n","\n","<br>\n","\n","That is exactly what this update does. <br><br>\n","\n","It changes the policy parameters($\\theta$?) <br>\n","to increase the probability of actions that were better than expected according to the critic. <br><br>\n","\n","Correspondingly, <br>\n","if the critic is disappointed and the TD error is negative, <br>\n","then the probability of the action is decreased.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1Z8RsrAhmzEEGsLxTbRz77d52za2rDp4G\" alt=\"3-23\" width=\"500\">\n","\n","<br>\n","\n","The actor and the critic learn at the same time constantly interacting. <br><br>\n","\n","The actor is continually changing the policy to exceed the critic's expectation, and <br>\n","the critic is constantly updating it's value function to evaluate the actor's changing policy.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"A9x6XDl8fKsk","colab_type":"text"},"source":["### Actor-Critic algorithm <br><br>\n","\n","\n","With the policy update in place, <br>\n","we're ready to go through the full algorithm for average reward actor-critic.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1ueU9Wa1l8XOl1cl26w-HjXL9YAhszFxZ\" alt=\"3-24\" width=\"500\">\n","\n","<br><br>\n","\n","\n","To start, <br>\n","we specify the policy parameterization and the value function parameterization. <br>\n",">Input : a differentiable policy parameterization $\\pi(a|s,\\theta)$ <br>\n",">Input : a differentialble state-value function parameterization $\\hat{v}(s,W)$\n","\n","Fot example, <br>\n","we might use Tile Coding to construct the approximate value function and a Soft-max policy parameterization. \n","\n","<br><br>\n","\n","\n","We will need to maintain an estimate of the average reward $R$ just like we did in the differential SARSA algorithm. <br>\n","We initialize this to $0$ <br>\n",">Initialize $\\bar{R} \\in \\mathbb{R}$ to $0$\n","\n","<br><br>\n","\n","\n","We can initialize the weights and the policy parameters however we like. <br>\n",">Initialize state-value weights $W \\in \\mathbb{R}^d$ and policy parameter $\\theta \\in \\mathbb{R}^{d'} \\quad$ (e.g. to $0$)\n","\n","We initialize the step size parameters for the value estimate, the policy, and the average reward <br>\n","and they could all be different. <br>\n",">Algorithm parameters : $\\alpha^W > 0, \\;\\; \\alpha^\\theta > 0, \\;\\; \\alpha^{\\bar{R}} > 0$\n","\n","<br><br>\n","\n","\n","We get thie initial state from the environment, and then begin acting and learning. <br>\n",">Initialize $S \\in \\mathbb{E}$ <br>\n",">Loop forever (for each time step)\n","\n","<br><br>\n","\n","\n","On each time step, <br><br>\n","\n","we choose the action according to our policy <br>\n","and recieve the next state and reward from the environment. <br>\n",">Loop forever (for each time step) <br>\n",">$\\quad A \\sim \\pi(\\; \\cdot \\; | S,\\theta)$ <br>\n",">$\\quad$ Take the action $A$, $\\quad$ observe $S', \\; R$\n","\n","<br>\n","\n","Using this information, <br>\n","we compute the differential TD error <br>\n","and update our running estimate of the average reward. <br>\n",">$\\delta \\quad \\leftarrow \\quad R - \\bar{R} + \\hat{v}(S',W) - \\hat{v}(S,W)$ <br>\n",">$\\bar{R} \\quad \\leftarrow \\quad \\bar{R} + \\alpha^{\\bar{R}} \\delta$\n","\n","<br>\n","\n","We update the value function weights using the TD update. <br>\n",">$W \\quad \\leftarrow \\quad W + \\alpha^W \\delta \\nabla \\hat{v}(S,W)$\n","\n","<br>\n","\n","Finally, <br>\n","we update the policy parameters using our policy gradient update !\n",">$\\theta \\quad \\leftarrow \\quad \\theta + \\alpha^\\theta \\delta \\nabla \\ln \\pi(A \\; |S,\\theta)$\n","\n","<br>\n","\n",">$S \\quad \\leftarrow \\quad S;$\n","\n","<br><br>\n","\n","That's it ! <br><br>\n","\n","This algorithm is designed for continuing tasks. <br>\n","So we can run it indefinitely and continue to improve the policy forever.\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"75OH2tM_gtO6","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - It is useful to learn a value function to estimate the gradient for the policy parameters <br><br>\n","\n","  - The actor-critic algorithm implements this idea, <br>\n","  with a critic that learns a value function for the actor\n","\n","\n","<br><br>\n","\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]}]}
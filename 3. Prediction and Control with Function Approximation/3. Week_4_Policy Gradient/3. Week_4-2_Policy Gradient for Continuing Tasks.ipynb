{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. Week_4-2_Policy Gradient for Continuing Tasks","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyN1XoEOIVJ/mQUxJgUeQoHL"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"39ARKswlLNZg","colab_type":"text"},"source":["## __2. Policy Gradient for Continuing Tasks__ <br><br>\n","\n","\n","  - The Objective for Learning Policies <br><br>\n","\n","  - The Policy Gradient Theorem\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"SrLl-g65LbKc","colab_type":"text"},"source":["## $\\cdot$ The Objective for Learning Policies <br><br>\n","\n","\n","  - Describe the objective for policy gradient algorithms\n","\n","<br><br>\n","\n","\n","Now that we've introduced the idea of parameterizing policies directly, <br>\n","we're ready to talk about how we can learn to improve a parameterized policy. <br><br>\n","\n","Just like with action-value based methods, <br>\n","The basic idea will be to specify an objective, <br>\n","and figure out how to estimate the gradient of that objective from an agent's experience.\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"U6Kg5wvutui1","colab_type":"text"},"source":["### The Goal of Reinforcement Learning $\\quad : \\;$ Maximizing Reward in the Long Run <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1YMpMFdZ4k5n-WV-n3XjdOHgFIAlo4_b7\" alt=\"2-01\" width=\"500\">\n","\n","<br>\n","\n","Formulating an obejective for learninng a parameterized policy <br>\n","is in some sense more straightforward than it was for action-value based methods ! <br><br>\n","\n","We've said many times that the ultimate goal of Reinforcement Learning <br>\n","is to learn a policy that obtain as much reward as possible in the long run. <br><br>\n","\n","It turns out that when we parameterize our policy directly, <br>\n","we can also use this goal directly as the learning objective.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"d78dLfoovTSY","colab_type":"text"},"source":["### Formalizing the Goal as an Objective <br><br>\n","\n","\n","Throughout this specialization, <br>\n","we've introduced a few different interpretations of <br>\n","what it means to obtain as much reward as possible in the long run.\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1E_demdBKtLMMeA_5n4g09GDShcS_5be_\" alt=\"2-02\" width=\"500\">\n","\n","<br>\n","\n","For the episodic case, <br><br>\n","\n","we can use the undiscounted retrun, <br>\n","which is the sum over a whole episode. \n","\n","<br><br>\n","\n","\n","For the continuing case, <br><br>\n","\n","we introduced the discounted return, <br>\n","which places more emphasis on immediate reward in order to keep the sum finite. <br><br>\n","\n","Most recently, we introduced the average reward formulation. <br>\n","Here, we maximize the long-term average of the reward. <br>\n","The appropriate return here is the sum of the differences between the immediate reward and it's average. <br><br>\n","\n","Because the lont-trerm average subtracted, this sum is finite even without discounting !\n","\n","<br><br>\n","\n","Here wa have three potential problem formulation to consider. <br>\n","We're going to restrict our attention the continuing setting with average reward. <br><br>\n","\n","Remember what out aim is here <br>\n","to find a way to directly optimized parameters of a policy. <br><br>\n","\n","The first step is to write the average reward objective in a form that we can optimize.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"t17r9F6iyjim","colab_type":"text"},"source":["### The Average Reward Objective <br><br>\n","\n","Previously, we estimated the average reward to learn action-values ! <br>\n","( in an average reward variant of SARSA ) <br><br>\n","\n","Now our aim is to learn a policy <br>\n","that directly optimized average reward. <br><br>\n","\n","Toward this aim, let's start by writhing the average reward under a policy in a more useful form. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=15NrBFw0hqqLlJUystqXwtsAM-WgLW3JO\" alt=\"2-03\" width=\"500\">\n","\n","<br>\n","\n","\n","We can write the average reward $r(\\pi)$ achieved by a particular policy $\\pi$ like this. <br>\n","$r(\\pi) \\quad = \\quad \\displaystyle \\sum_s \\mu(s) \\sum_a \\pi(a|s,\\theta) \\sum_{s',r} p(s',r|s,a)r$ \n","\n","<br><br><br>\n","\n","\n","\n","#### Undertand the Average Reward Objective (?) <br><br>\n","\n","\n","Let's break this formula down starting from the inner sum and moving out. <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1BytbAgNah7gmpdX3OlWyF5y_RMbgCsnP\" alt=\"2-04\" width=\"500\">\n","\n","<br><br>\n","\n","\n","This inner sum gives the expected reward if we starts state in $s$ and take action $a$. <br>\n","$\\rightarrow \\quad \\mathbb{E} \\big[ R_t|S_t=s,A_t=a \\big]$ <br><br>\n","\n","This is simply the sum over all reward $r$ we might receive weighted by their probability $p(s',r|s,a)$ from $s$ and $a$. <br><br>\n","\n","We sum over next states $s'$ to get marginal probabilities (주변확률) over the reward. <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1Rb2ItVHOnHeCzHFdAi3CBPxZZM1fVI_9\" alt=\"2-05\" width=\"500\">\n","\n","<br><br>\n","\n","\n","The next level of the summation gives us the expected reward under the policy $\\pi$ from a particular state $s$. <br>\n","$\\rightarrow \\quad \\mathbb{E}_{\\pi} \\big[ R_t | S_t = s \\big]$ <br><br>\n","\n","This is over all possible actions $a$ weighted by the probability $\\pi(a|s,\\theta)$ under (policy?) $\\pi$. <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1g2pEoqKNkR3VV7usjir59xjWSc_kCe96\" alt=\"2-06\" width=\"500\">\n","\n","<br><br>\n","\n","\n","Finally, we get the overall average reward <br>\n","by considering the fraction of time we spend in state $s$ under policy $\\pi$. <br>\n","$\\rightarrow \\quad \\mathbb{E} \\big[ R_t \\big]$ <br><br>\n","\n","The distribution $\\mu$ provides these probabilities $\\mu(s)$. <br>\n","The expected reward across states $\\mathbb{E}_{\\pi}(R_t)$ is a sum over $s$ of the expected reward in a state $\\mathbb{E}_{\\pi} \\big[ R_t | S_t = s \\big]$ weighted by $\\mu(s)$.\n","\n","<br><br><br>\n","\n","\n","\n","$r(\\pi)$ is our average reward learning objective ! \n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9JYPsxFLyBWr","colab_type":"text"},"source":["### Optimizing The Average Reward Objective <br><br>\n","\n","\n","Our goal of policy optimization <br>\n","will be to find a policy which maximizes the average reward $r(\\pi)$ ! \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1ePxysAoCEy6g_i2f_xGKo07VSG7lT0cM\" alt=\"2-07\" width=\"500\">\n","\n","<br>\n","\n","\n","#### Basic approach <br><br>\n","\n","to estimate the gradient of the objective $\\nabla r(\\pi)$ with respect to the policy parameters, <br>\n","ans adjust the paramters based on the estimate ! <br><br>\n","\n","$\\rightarrow \\quad$ Policy Gradient method <br>\n","$\\qquad$ The class of methods they use this idea are often referred to as policy gradient methods.\n","\n","<br><br><br>\n","\n","\n","\n","#### Indirect learning policy in GPI v.s. Direct learning policy in PG <br><br>\n","\n","\n","Up until now, <br>\n","we've used a very different approach. <br>\n","We use the Generalized Policy Iteration framework (GPI) to learn approximate action-values. <br><br>\n","\n","Then we use these approximate values indirectly to infer a good policy. \n","\n","<br><br>\n","\n","Now, <br>\n","we're interested in learning policies directly ! \n","\n","<br><br><br>\n","\n","\n","\n","#### There's also a superficial difference. <br><br>\n","\n","\n","In GPI <br>\n","minimizing the Mean Squared Error for learning <br><br>\n","\n","In GP <br>\n","maximizing an objective for learning <br><br>\n","\n",">That means <br>\n",">we will want to move in the direction of the gradient ! <br>\n",">( rather than the negative gradient )\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aqmHm3qmI4xx","colab_type":"text"},"source":["#### The Challenge of Policy Gradient Methods <br><br>\n","\n","\n","However, <br>\n","There're are few challenges in computing this gradient ! <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1gGjcQ_VN7XPdbvWs35eWwChXsUjpFSCb\" alt=\"2-08\" width=\"500\">\n","\n","<br>\n","\n","The main difficulty <br>\n","is that modifying our policy changes the distribution $mu$ ! <br>\n","$\\rightarrow \\quad$ $\\mu(s)$ is depends on $\\theta$ ! <br><br>\n","\n","This contrast the value function approximation <br>\n","where we minimized Mean Squared value Error under a particular policy. <br><br>\n","\n","There <br>\n","the distribution $\\mu$ was fixed. <br>\n","It does not change as the weights and the parameterized value function change. <br>\n","$\\rightarrow \\quad \\mu(s)$ is independent of $W$\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1myDaotLeis68cTbPI08Nk8oRc6srlts4\" alt=\"2-09\" width=\"500\">\n","\n","<br>\n","\n","We were therefore able to estimate gradients for state drawn from $\\mu$ by simply following the policy. \n","\n","<br><br><br>\n","\n","\n","#### the Policy Gradient Theorem $\\quad : \\;$ Solution for learning policy <br><br>\n","\n","This is less straightforward, <br>\n","a $\\mu$ itself depends on the policy we are optimizing. <br><br>\n","\n","Luckily, <br>\n","there's an excellent theoratical answer to this challenge, <br>\n","called the Policy Gradient Theorem ! <br><br>\n","\n","( we'll talk more about that next )\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"iZHVF6N9KYg9","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - We can use the averege reward as an objective for policy optimization\n","\n","<br><br>\n","\n","\n","In upcoming lectures, <br>\n","we will disucss how to actually optimize this objective from sampled experience. \n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"z8MhBilCOTR3"},"source":["## $\\cdot$ The Policy Gradient Theorem <br><br>\n","\n","\n","  - Describe the result of the policy gradient theorem <br><br>\n","\n","  - Understand athe importance of the policy gradient theorem\n","\n","<br><br>\n","\n","\n","We just discussed an objective for policy optimization. <br>\n","The next step is to figure out how the agent can optimize it based on it's own experience. <br><br>\n","\n","In this video, <br>\n","we will describe the Policy Gradient theorem. <br><br>\n","\n","This is a key theoretical result. <br>\n","It allows us to write the gradient of the average reward so that it is easier to estimate from experience !\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FOSLCg-yPW4i","colab_type":"text"},"source":["### Gradient Ascent <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1PD1JQomMp-MMFy2QJsYnLlANTHm663cr\" alt=\"2-10\" width=\"500\">\n","\n","<br>\n","\n","To optimize the Mean Squared value Error, <br>\n","we used methods based on Stochastic Gradient Descent. <br><br>\n","\n","We estimate the negative of the gradient of our objective, <br>\n","and adjust the weights of the value function in that direction.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=18yCb5GX6NkYvzOHlrI2CDUozGtO7sUmL\" alt=\"2-11\" width=\"500\">\n","\n","<br>\n","\n","Policy Gradient methods use a similar approach, <br>\n","but with the average reward objective and the policy parameters $\\theta$. <br><br>\n","\n","we want to maximize the average reward rather than minimizing it ! <br>\n","This means we do Gradient Ascent, and move in the direction of the positive of the gradient.\n","\n","<br><br><br>\n","\n","\n","\n","Remember that <br>\n","a simple recipe for solving a problem is <br><br>\n","\n","to first specify an objective <br>\n","then estimate the gradient of that objective, <br>\n","and finally, adjust the weights in that direction. <br><br>\n","\n","Step 1 is done. <br>\n","Now the next step is to estimate the gradient of our objective.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jkj9VsrrRLWP","colab_type":"text"},"source":["### The Gradient of the Objective <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1eO8K26ZtG9tdEEYY_9fHjyfs0ykxNtWM\" alt=\"2-12\" width=\"500\">\n","\n","<br>\n","\n","Recall <br>\n","the Average Reward Objective. <br><br>\n","\n","Let's compute the gradient.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1uRhiuBfvc8hkjDjTJW99I-ZnXavK7E28\" alt=\"2-13\" width=\"500\">\n","\n","<br>\n","\n","We can apply the product rule of calculus to the objective to yield two terms. <br><br>\n","\n","This is pretty complex. <br>\n","So let's look at it a bit more closely. \n","\n","<br><br>\n","\n","\n","The first term involves the gradient of the stationary distribution over states. <br>\n","$\\nabla \\mu(s)$ <br><br>\n","\n","Unfortunately, <br>\n","the gradient of $\\mu$ is not straightforward to estimate. <br><br>\n","\n","The stationary distribution $\\mu$ depends on a long-term interaction between the policy and the environment.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LvghGiTPzqyz","colab_type":"text"},"source":["### The Policy Gradient Theorem <br><br>\n","\n","\n","Luckily, <br>\n","the Policy Gradient Theorem gives us a simpler expression for this gradient. <br><br>\n","\n","Here we show the results of the theorem. <br>\n","It's worth walking through this expression. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1-vlPHEbCT052wpTPiQ4Vmg4SBnPCrVUx\" alt=\"2-14\" width=\"500\">\n","\n","<br>\n","\n","In the inner sum, <br>\n","we have the gradient of the policy times the action-value function. <br><br>\n","\n","Let's try to understand this term a little better.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-J6FhXVN5sQO","colab_type":"text"},"source":["### Understanding $\\displaystyle \\sum_a \\nabla \\pi(a|s,\\theta) q_{\\pi}(s,a)$ <br><br>\n","\n","\n","The gradient of the policy $\\pi$ is easy to compute <br>\n","as long as our policy parameterization is differentiavle ! <br><br>\n","\n","The gradient of the policy <br>\n","tells you how to adjust your parameters to increase the probability of a certain action. \n","\n","<br><br>\n","\n","\n","#### Example $\\quad$ for understanding the policy gradient <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1fkNMbHKiSEbSPtrQy7uHwo2XgkVI_keC\" alt=\"2-15\" width=\"500\">\n","\n","<br>\n","\n","Consider the simple grid world shown here. <br><br>\n","\n","As usual, the agent cam move up, donw, left, or right. <br>\n","For simplicity, let's assume the policy is controlled by just two parameters ($\\theta_1$ and $\\theta_2$). <br><br>\n","\n","The parameters are curently set to the point marked here. <br>\n","The arrows indicate the agent's action probabilities in a particular state with the current parameter settings.\n","\n","<br><br>\n","\n","\n","#### the policy gradient for Up action case <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=18IJkJGNTtGOc9LaUgdtUs8QtViSOzPuI\" alt=\"2-16\" width=\"500\">\n","\n","<br>\n","\n","The gradient for the up action might look something like this on the plot. <br><br>\n","\n","The gradient tells us <br>\n","how to change the policy parameters to make that action more likely to be selected in the given state ! <br><br>\n","\n","By moving the paramters in the direction of the gradient, <br>\n","we increase the probability for the up action. <br>\n","\n",">This necessarily means decrease in the probability of some of the other actions !\n","\n","<br><br>\n","\n","\n","#### the policy gradient for Left action case <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1fotEkNBTSkhrSdvAapOj69OYZrb7dER_\" alt=\"2-17\" width=\"500\">\n","\n","<br>\n","\n","Different actions have different gradients. <br><br>\n","\n","the gradient of the left action probability may look like this. <br><br>\n","\n","Moving the parameters in that direction will increase the probability of the left action, <br>\n","and decrease the probability of some of the other actions. <br><br>\n","\n",">This might all sound a bit abstract. <br>\n",">We wll show concrete examples of computing this gradient in the coming lectures.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"G7rSXyZA-Jmv","colab_type":"text"},"source":["#### the policy gradient for overall action <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=12B1I_miZNFuQINQptHN1BDGrQhBzxWo1\" alt=\"2-18\" width=\"500\">\n","\n","<br>\n","\n","Now, <br>\n","let's bring some reward into the situation <br>\n","and think about what this whole term means. <br><br>\n","\n","This is a sum over the gradients of each action probability weighted by the value of the associated action. <br><br>\n","\n","Imagine,  <br>\n","we've added a rewarding state in the bottom right. <br><br>\n","\n","The up and left actions <br>\n","move away from rewarding state, and should have negative value. <br>\n","The down and right actions <br>\n","move toward it the rewarding state, and should have positive value. \n","\n","<br><br>\n","\n","\n","The precise values will depend on the current policy. <br>\n","but, let's say they look something like this. <br><br>\n","\n","The weighted sum <br>\n","gives a direction to move the parameters that decreases the pro bability of moving up or left since their value is negative, <br>\n","while increasing the probability of moving down or right since their value is positive. <br><br>\n","\n","That direction might look something like this on the plot.\n","\n","<br><br>\n","\n","\n","#### Average Reward expression with the policy gradient <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1qDTodicjIQqLOtECBrYiVIGP_Z55oYUl\" alt=\"2-19\" width=\"500\">\n","\n","<br>\n","\n","The gradient $\\nabla r(\\pi)$ expression given by the Policy Gradient Theorem <br>\n","takes this expression and sums that over each state. <br><br>\n","\n","This gives the direction to move the policy parameters to most rapidly increase the overall average reward !\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QS5Ti7vnCDwG","colab_type":"text"},"source":["### The Policy Gradient Theorem <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1uIVzrpe_72Eul3zPha5-PBt9dqVLLmXm\" alt=\"2-20\" width=\"500\">\n","\n","<br>\n","\n","We now have a simple expression for the gradient ! <br><br>\n","\n","Importantly, <br>\n","this expression does not contain the gradient of the state distribution $\\mu$ ! <br>\n","( which is challenging to estimate ! ) <br><br>\n","\n",">The proof of the policy gradient theorem <br>\n",">can be found in the course textbook. \n","\n","<br><br>\n","\n","\n","As we will see in the next lecture, <br>\n","this gradient is straightforward to estimate ! <br><br>\n","\n","That means we will be able to build an incremental policy gradient algorithm using an agent's experience !\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZIw5EjauDdQF","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - The policy gradient theorem gives an expression for the gradient of the average reward <br><br>\n","\n","  - Understand the terms in this gradient\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]}]}
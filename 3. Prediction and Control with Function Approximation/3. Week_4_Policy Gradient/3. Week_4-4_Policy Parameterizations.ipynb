{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. Week_4-4_Policy Parameterizations","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyNCLzk2RBRo9Wqx8yvrG9VR"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"39ARKswlLNZg","colab_type":"text"},"source":["## __4. Policy Parameterizations__ <br><br>\n","\n","\n","  - Actor-Critic with Softmax policies <br><br>\n","\n","  - Demonstration with Actor-Critic <br><br>\n","\n","  - Gaussian Policies for Continuous Actions <br><br>\n","\n","  - Week 4 Summary\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"SrLl-g65LbKc","colab_type":"text"},"source":["## $\\cdot$ Actor-Critic with Softmax policies <br><br>\n","\n","\n","  - Derive the actor-critic update for a softmax policy parameterization with linear action preferences <br><br>\n","\n","  - Implement this algorithm\n","\n","<br><br>\n","\n","\n","Actor-Critic elegantly mixes direct policy optimization, value functionsn and temporal difference learning. <br><br>\n","\n","The actor-Critic method we discussed previously was quite general. <br>\n","we didn't specify the function approximation, nor the policy parameterization. <br><br>\n","\n","Today, <br>\n","we discuss one specific implementation. <br>\n","Actor-Critic with Linear function approximation and a Softmax policy parameterization.\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ScQ-XErdShme","colab_type":"text"},"source":["### Recap $\\quad : \\;$ Actor-Critic <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1L2110NcTdcLULCM32ZKHiY6idPykbAog\" alt=\"4-01\" width=\"500\">\n","\n","<br>\n","\n","Previously, <br>\n","we introduced the actor-critic algorithm. <br><br>\n","\n","That algorithm combines <br>\n","  - the policy evaluation, which is the Critic, and <br>\n","  - the policy gradient rule to update the policy, which is the Actor.\n","\n","<br>\n","\n","$W \\quad \\leftarrow \\quad W + \\alpha^W \\hat{v}(S,W)$ <br>\n","The Critic uses semi-gradient TD. <br><br>\n","\n","$\\theta \\quad \\leftarrow \\quad \\theta + \\alpha^\\theta \\delta \\nabla \\ln \\pi(A \\;| S,\\theta)$ <br>\n","The Actor uses the TD error ($\\delta$?) from the critic to update the policy parameters. <br><br>\n","\n","\n",">Critic 과 Actor 각각 <br>\n",">자기 자신을 업데이트 하는데에 상대방을 사용 (?)\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"o0GjgLwYUA52","colab_type":"text"},"source":["### Policy Update with a Softmax Policy <br><br>\n","\n","\n","All of this is a little bit high level abstract. <br>\n","Let's consider a specific implementation. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1PaUGg6K7zOg17vw7kDhXjUhw1uMbDJ-i\" alt=\"4-02\" width=\"500\">\n","\n","<br>\n","\n","Let's build an algorithm <br>\n","for a finite set of actions and continuous states. <br><br>\n","\n","A common choice of policy parameterization for finite action is the Softmax policy. <br>\n","(we discussed before) \n","\n","<br><br>\n","\n","\n","  - $h(s,a,\\color{brown}{\\theta})$ <br>\n","  We can use function approximation to represent the preferences. <br>\n","  The parameters of this functions are the policy parameters denoted by $\\theta$, <br>\n","  so out policy gradient learning rule will update $\\theta$. <br><br>\n","\n","  - $\\color{brown}{e}^{h(s,a,\\theta)}$ <br>\n","  We use a Softmax policy that exponentiates the preferences, <br><br>\n","\n","  - $1/\\color{brown}{\\sum_{b \\in A}} e^{h(s,b,\\theta)}$ <br>\n","  and divides by the sum. <br>\n","  This guarantees the resulting action probabilities are positive and summed to $1$. <br><br>\n","\n","  - $\\pi(a|\\color{brown}{s},\\theta)$ <br>\n","  Notice the policy is written as a function of the current state $s$. <br>\n","  This is like having a different Softmax distribution for each state.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1WiG_cd-WLOzhqgbUDugvjWBKbXyIehc9\" alt=\"4-03\" width=\"500\">\n","\n","<br>\n","\n","Watch the agent moves through the state space. <br><br>\n","\n","The state-dependent action preferences <br>\n","give rise to potentially different distributions for each state ! <br><br>\n","\n","To select an action, <br>\n","we follow a simple procedure. <br><br>\n","\n","  1. In the current state, <br>\n","  we query the Softmax for each action. <br>\n","  This generates a vector of probabilities, one entry for each action. <br><br>\n","\n","  2. Given this vector, <br>\n","  we pick an action proportionally to these probabilities.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cHmIobCJqvl_","colab_type":"text"},"source":["### Features of the Action Preference Function <br><br>\n","\n","\n","Now, lets discuss <br>\n","\" how our parameterization choices impact the value function, the action preferences, and our update equations. \" <br><br>\n","\n","Let's use the same features for our value estimates and the policy.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1lpERiAqlG7uiAHY1HrjoxFW8qIYlVPlX\" alt=\"4-04\" width=\"500\">\n","\n","\n","Remember, <br><br>\n","\n","the Critic updates an estimate of the state value function, <br>\n","so the critic only requires a feature vector characterizing the current state. <br>\n","$\\rightarrow \\quad X(s)$ <br><br>\n"," \n","The actor's action preferences depend on the state and action, <br>\n","so our action preference function requires a state-action feature vector. <br>\n","$\\rightarrow \\quad X_h(s,a)$ \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1MoedKgXyNGRdOgWGcYEl7WyUTzH2WYA-\" alt=\"4-05\" width=\"500\">\n","\n","<br>\n","\n","We have encountered this issue before. <br><br>\n","\n","We use the same strategy as before, <br>\n","Stacked state features ! <br><br>\n","\n","That is we use a copy of the state feature vector for each action. <br>\n","The size of the policy parameter vector $\\theta$ is laarger than the weights $W$ used by the critic. <br><br>\n","\n","Since there are three actions, <br>\n","this is three times larger.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wD7HIAP5uimW","colab_type":"text"},"source":["### Actor-Critic with a Softmax Policy <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1MVPVnmkNlsbUPMPXiEn0xuFzCNcXy65G\" alt=\"4-06\" width=\"500\">\n","\n","\n","Now <br>\n","let's look at the update equations for the actor and the critic <br>\n","with our linear Softmax parameterization.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1JRjOBbULMmZC5FA9jwH-z68so8A6gT5Y\" alt=\"4-07\" width=\"500\">\n","\n","<br>\n","\n","The first one is easy. <br><br>\n","\n","The gradient of the linear value function is just the feature vector. <br>\n","$\\nabla \\hat{v}(s,W) = X(s)$ <br><br>\n","\n","So the critic's weights update is just $\\alpha$ times the TD error times the feature vector. <br>\n","Same old semi-gradient TD.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1TmKHjzqVUt9ZGQmXW06o0DgN02G6jvgF\" alt=\"4-08\" width=\"500\">\n","\n","<br>\n","\n","The actor's update to the preferences requires a bit more thought. <br>\n","Lucklily, the gradient is not so bad. <br><br>\n","\n",">We will leave out the derivation, <br>\n",">and just give you the final result. <br>\n",">( You can verify it if you like ) \n","\n","<br>\n","\n","The gradient has two parts. <br>\n","$\\nabla \\ln \\pi(s|s,\\theta) = X_h(s,a) - \\displaystyle \\sum_b \\pi(b|s,\\theta) X_h(s,b)$ <br><br>\n","\n","The first is the state-action features for the selected action. $X_h(s,a)$ <br>\n","The second is the state-action features multiplied by the policy summed over all actions. $\\displaystyle \\sum_b \\pi(b|s,\\theta) X_h(s,b)$ <br>\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9SVq6oQZ1NHs","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - We discussed the concrete (specific) implementation of an actor-critic algorithm\n","\n","<br><br>\n","\n","\n","Now you know everything you need to know to implementation this algorithm on your own ! <br><br>\n","\n","Next time <br>\n","we'll try this algorithm out, and see how it works.\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WEA1YTmK18rt"},"source":["## $\\cdot$ Demonstration with Actor-Critic <br><br>\n","\n","\n","  - Desing concrete function approximation for an average reward actor-critic algorithm <br><br>\n","\n","  - Analyze the performance of an average reward agent\n","\n","<br><br>\n","\n","\n","In this video, <br>\n","we will see how the Actor-Critic algorithm works on a classic control task, the Pendulum swing up problem.\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9vt8iAUI40YA","colab_type":"text"},"source":["### Pendulum Swing-Up <br><br>\n","\n","\n","1 | 2\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1aljQam9V6oIYmP1lVBPmDySCXoSQoUr2\" alt=\"4-09\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1K6kvRLEYqyXwuIhm2UXPKsxDjDtKwlbk\" alt=\"4-10\" width=\"500\">\n","\n","<br>\n","\n","Today, we'll investigate the pendulum swing up task. <br><br>\n","\n","The agent controls the pendulum. <br>\n","The controls are simple. <br>\n","The agent can apply torque on a pivot point marked by the black dot. <br><br>\n","\n","The goal is to get the pendulum to balance upright. <br><br>\n","\n","The pendulum starts in it's rest posiition hanging down with no velocity. <br>\n","The pendulum can move freely subjected only to gravity and the actions applied by the agent. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1APvIp9-xd-gM_mEMbyxCKLmobVB6hL9_\" alt=\"4-11\" width=\"500\">\n","\n","<br>\n","\n","The state of the pendulum is the angle of the pole and the anglular velocity. <br>\n","( both are real valued quantities ) <br>\n","$s \\doteq \n","\\begin{bmatrix} \n","\\beta \\\\ \n","\\dot{\\beta} \n","\\end{bmatrix}\n","\\begin{align} \n","\\quad \\text{Angular Position} \\\\ \n","\\quad \\text{Angular Velocity} \n","\\end{align}$<br><br>\n","\n","Our simulation has just three discrete actions. <br>\n","apply a constant angular acceleration clockwise, counter-clockwise, or apply no angular acceleration. <br>\n","$a \\in {-1,0,1}$ <br><br>\n","\n","We use a Softmax policy for this task, <br>\n","because the action space is discrete. <br>\n",">Later on, <br>\n",">we'll use a different parameterization for a continuous action variant of the pendulum task.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qGWgdUsWrGEk","colab_type":"text"},"source":["### The Reward in Pendulum Swing-Up <br><br>\n","\n","\n","The next step is to find the reward function. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1TzeKnGOdNqFv3bVRg1zcnDgR2-itUGL0\" alt=\"4-12\" width=\"500\">\n","\n","<br>\n","\n","Reward <br>\n","The goal is to get the pendulum pointing directly up and keep it that way. <br>\n","So the reward is equal to the negative absolute angle from vertical. <br>\n","$R \\doteq -|\\beta|$ <br><br>\n","\n","Reward condition <br>\n","If the pendulum reaches a large angular velocity, then it could damage the system, that would be bad. <br>\n","To avoid this, we constrain the range of the agular velocity to between $-2\\pi$ and $2\\pi$. <br>\n","$-2\\pi < \\dot{\\beta} < 2\\pi$ <br><br>\n","\n","Contining task <br>\n","If this limit is exceeded, then we reset the pendulum to the resting position. <br>\n","The goal is to keep the pendulum as close upward(?) as possible at all times. <br>\n","This continues indefinitly. there are no episodes and no terminations. <br>\n","So it makes sense to formulate this as a continuing task.\n","\n","<br><br><br>\n","\n","\n","\n","#### The pendulum is interesting for a few reasons <br><br>\n","\n","\n","  - The actions are not strong enough to move the pendulum directly to the vertical position from the resting position. <br><br>\n","\n","  - A good policy must apply actions <br>\n","  that move the pendulum away from the desired position in order to gain enough momentum to swing up. <br><br>\n","\n","  - Finally the vertical position is unstable. <br>\n","  So good pocily must continually balance the pole.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"F7hm1ExTvRhP","colab_type":"text"},"source":["### Parameterization and Features <br><br>\n","\n","\n","Let's supply Softmax Actor-Critic algorithm to out simulated version of the pendulum task. <br>\n","Before we can do that, we have to design a function approximator and a policy parameterization. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=103tyTpJcEhiuOwX3qCjDN5GZJ1uZSg7r\" alt=\"4-13\" width=\"500\">\n","\n","<br>\n","\n","2-dimentional state problem <br>\n","The pendulum task has a low dimentional state. <br>\n","Just the angle and angular velocity. <br><br>\n","\n","Tile Coding <br>\n","This 2-dimentional problem is a good fit for Tile Coding. <br>\n","The approximate value function $\\hat{v}(s,W)$ the action preferences $h(s,a,\\theta)$ <br>\n","will be linear in the Tile Coding, but not linear in the state variables. \n","\n","<br><br><br>\n","\n","\n","\n","#### Tile Coding set up <br><br>\n","\n","State variable condition <br>\n","The first step in setting up a tile coding is to figure out the range of each of the state variables. <br><br>\n","\n","  - This is easy for the angular position $\\beta$(?). <br>\n","  it just has to be between $\\pi$ and $-\\pi$. <br>\n","  $-\\pi < \\beta < \\pi$ <br><br>\n","\n","  - The angular velocity $\\dot{\\beta}$ will lie between $2\\pi$ and     $-2\\pi$ due to the constraint we discussed earlier. <br>\n","$-2\\pi < \\beta < 2\\pi$ \n","\n","<br><br>\n","\n","\n","Large and many tilings <br>\n","It's usually good to use large tiles and many tilings. <br>\n","We use 32 tilings of size 8 by 8. <br>\n","\n",">The broad tiles enable considerable generalization and thus fast early learning.\n","\n",">many tilings help with discrimination.<br>\n",">This should result in an accurate approximation of value function and the action preferences.\n","\n","<br>\n","\n","Remeber <br>\n","for actions preferences $X_h(s,a)$, <br>\n","we need a feature vector for each state-action pair. <br>\n","As before, <br>\n","we simply use a seperate copy of the state features for each action.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dnI5_gQ6A4Du","colab_type":"text"},"source":["### Step-Sizes <br><br>\n","\n","\n","What about the step sizes for the average reward, the actor, and the critic ? <br>\n","That's quite a few parameters. And each of them have a big impact on performance !\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1BzB_2885YYHFr3sCtZI_PZwkvQVBW5C4\" alt=\"4-14\" width=\"500\">\n","\n","\n","We performed the systematic sweep to determine a good setting for each (step stizes). <br><br>\n","\n","This matches the general reason that we want the critic to update at a faster rate. <br>\n","That way, the critic can accurately critique the more slowly changing policy. \n","\n","<br><br>\n","\n","\n",">For the sake of time, we will skip those. <br>\n",">You'll learn all about this implementation in your assessment.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JFAaj8rpEDdC","colab_type":"text"},"source":["### Early Learning <br><br>\n","\n","\n","Let's look at a video of the agent as it first begins interacting with the pendulum environment.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1WYM-cLQPWSDuMoKgubklihMYCXTNhhOj\" alt=\"4-15\" width=\"500\">\n","\n","<br>\n","\n","The agent begins by acting randomly. <br>\n","But it quickly learns to add energy to reach states closer to vertical.\n","\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Y5nBVZxYF8Os","colab_type":"text"},"source":["### Final Learning <br><br>\n","\n","\n","Let's skip ahead a bit. <br>\n","The final behavior looks something like this.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=16YpLrLb8kxXpMFUHw8j2GfMJPAJnORHs\" alt=\"4-16\" width=\"500\">\n","\n","<br>\n","\n","We added random perturbation to demonstrate the robustness of the learnen policy. <br><br>\n","\n","This is like flicking the pendulum with your finger every once in a while. <br>\n","The perturbations are introduced only after the agent had learned a good balancing policy. <br><br>\n","\n","The perturbations are shown with black arrows in the video. <br>\n","We can see how the trained agent is able to recover smoothly, and is robust to these flicks. \n","\n","<br><br><br>\n","\n","\n","\n","#### 검증 <br><br>\n","\n","\n","It's really satisfying to watch an agent we programmed mastering the pendulum problem. <br>\n","But we have to be skeptical scientists too ! <br><br>\n","\n","This is just a demonstration. <br><br>\n","\n","\" Does the agent successfully learn to balance every time ? (or do we just lucky ?) \" <br>\n","\" How quickly and rocustly does it learn ? \" <br><br>\n","\n","To answer these questions, <br>\n","we ran the experiment 100 times, and averaged the performance.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RxG0rtl8IPKY","colab_type":"text"},"source":["### Results After 100 Runs <br><br>\n","\n","\n","To measure the performace of the agent, <br>\n","let's plot an exponentially weighted average of the reward over all time steps.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1jS1Vl3XpTap1kPsn3GwIfPdZsmgNRK5-\" alt=\"4-17\" width=\"500\">\n","\n","<br>\n","\n","The error bars show the standard error and the mean. <br>\n","The error bars are quite small. <br><br>\n","\n","In fact, the error bars are so small, you can hardly see them at all. <br><br>\n","\n","So we are pretty confident <br>\n","this plot accurately reflects our agent's average performance on this task with the parameters we have chosen.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=19S2mokTEF9MP4tEj5GWkY5Ye6yw4XBQB\" alt=\"4-18\" width=\"500\">\n","\n","<br>\n","\n","  1. The average reward starts off low, <br>\n","  but increases very quickly off the start.  <br><br>\n","\n","  2. After a short time, the rate of improvement slows. <br>\n","  But improvement continues as the agent continues to refine it's policy. <br><br>\n","\n","  3. Performance peaks near $0$. <br>\n","  This means the agent is generally able to balance the pendulum upright with minor deviations (편차). <br>\n","  Then it stays steady from that point forward.\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LD89gP32LCSR","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - We introduced a specific implementation of an actor-critic learning system <br><br>\n","\n","  - We tested it on a classic control task\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Yzng7FpxMQpB"},"source":["## $\\cdot$ Gaussian Policies for Continuous Actions <br><br>\n","\n","\n","  - Derive the actor-critic update for a Gaussian policy <br><br>\n","\n","  - Apply average reward actor-critic with a Gaussian policy to a particular task continuous actions\n","\n","<br><br>\n","\n","One of the nice things about policy-based methods <br>\n","is that they give us a natural way to deal with very large or even continuous action spaces. <br><br>\n","\n","We don't have to use a policy parameterization that assigns individual probabilities to each action. <br>\n","We could instead learn the parameters of some distribution over actions. <br><br>\n","\n","In this video, <br>\n","We will discuss how to learn a state-dependent Gaussian distribution over continuous actions.\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UHr9pxgSyDZi","colab_type":"text"},"source":["### Continuous Action Space <br><br>\n","\n","\n","Let's look at the same pendulum swing-up task we epxlored last time.\n","The state space and reward is exactly the same. <br><br>\n","\n","This time, we will use a continuous interval for the action space <br>\n","instead of just three discrete levels of angular acceleration !\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1nkp2SYmsFgdgtzcP0kbNsSJOinlG3RMK\" alt=\"4-19\" width=\"500\">\n","\n","<br>\n","\n","The angular acceleration can be anywhere between $-3$ and $+3$, which are the minimum and maximum. <br><br>\n","\n","We cannot assign a probability to every action seperately <br>\n","because the space of possible actions is infinite. \n","\n","<br><br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1TkEgakyfUejBh5ZAdIM6EWl1zwGpceFp\" alt=\"4-20\" width=\"500\">\n","\n","<br>\n","\n","One solution is to restrict actions to a discrete subset of the full range of possibilities. <br>\n","This is what we did previously <br>\n","\n",">In this case, <br>\n",">we compromise by picking a set of actions that were gentle enough to allow fine-grain control. \n","\n",">But <br>\n",">this also inhibits the agent's ability to apply larger acceleration. <br>\n",">We'd really like the agent to be able to apply <br>\n",">large acceleration when it's appropriate and smaller acceleration to correct it's balance.\n","\n","<br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1Thp1MeP57ZoacQBdiOA2m3DzzyStmBpt\" alt=\"4-21\" width=\"500\">\n","\n","<br>\n","\n","Towards this goal, <br>\n","another strategy is to parameterize the policy as a continuous distribution, Such as Gaussian distribution !\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xwxUHY9909zi","colab_type":"text"},"source":["### Gaussian Distribution <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1HmrEiFEXNoqU3HVhcpWjrYtCrJsEDQIH\" alt=\"4-22\" width=\"500\">\n","\n","<br>\n","\n","__PDF $\\quad : \\;$ Probability Density Function__ <br><br>\n","\n","A Gaussian (of?) random variable $x$ has the probability density function (PDF) shown here. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1Jw6jhzYoxt9HL1cpBZvQnKwyFr_sDOX2\" alt=\"4-23\" width=\"500\">\n","\n","<br>\n","\n","__Probability Density Graph__ <br><br>\n","\n","Here a few examples of this PDF with different means and variances. <br><br>\n","\n","Note that <br>\n","the y-acis in this case is density, and not probability ! <br>\n","Probability density means that for a given range, <br>\n","the probability of $x$ lying in that range will be the area under the probability density curve. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=18NEl7BIo0IaWQI5jISo2mHPsK04PtvWM\" alt=\"4-24\" width=\"500\">\n","\n","<br>\n","\n","__Mean $\\mu$__ <br><br>\n","\n","The parameter $\\mu$ controls the mean of the distribution. <br>\n","If $\\mu$ is $0$, ths distribution will be sentered around $0$. <br><br>\n","\n","For this curve, $\\mu$ is $-2$, so the distribution is centered around $-2$. \n","\n","<br><br>\n","\n","\n","low $\\sigma$ case | high $\\sigma$ case\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1IQykpX05ekGtccc5O9AXurjzKLXJsyOA\" alt=\"4-25\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=18Hzm2SNkd5f6kYNMwSJZqT1YN6F6LyPx\" alt=\"4-26\" width=\"500\">\n","\n","<br>\n","\n","__Standard Deviation $\\sigma$__ <br><br>\n","\n","The paramer sigma $\\sigma$ is the standard deviation, the square root of the variance. <br><br>\n","\n","If sigma is low like this curve, <br>\n","the distribution will be sharply peaked around the mean value $\\mu$.<br><br>\n","As sigma becomes larger, <br>\n","the distribution is more spread out like this curve, and $x$ is drawn from a wider range of values.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Q4TOhE6K4QDc","colab_type":"text"},"source":["### Gaussian Policy <br><br>\n","\n","\n","Let's define our policy using a Gaussian over actions. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1p0mqtNltOK9ByDeGNu0N0AIVR4QnSt6p\" alt=\"4-27\" width=\"500\">\n","\n","<br>\n","\n","__Gaussian Policy__ <br><br>\n","\n","$\\pi(a|s,\\theta) \\quad \\doteq \\quad \\displaystyle \\frac{1}{\\sigma(s,\\theta)\\sqrt{2\\pi}} \\;\\; \\exp \\big( - \\frac{(a - \\mu(s,\\theta))^2}{2\\sigma(s,\\theta)^2} \\big)$ <br><br>\n","\n","We make the parameters $\\mu$ and $\\sigma$ functions of the state. <br><br>\n","\n","This way, the agent can assign different action distribution to different state.\n","\n","<br><br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1c-C9r4xwQ0TGCSVXHgPHV_B9ysZXyGbK\" alt=\"4-28\" width=\"500\">\n","\n","<br>\n","\n","__Parameters__ $\\quad$ ( functions of state $f(s,\\theta)$ ) <br><br>\n","\n","\n","$\\mu(s,\\theta) \\quad \\doteq \\quad \\theta_{\\mu}^T X(s)$ <br><br>\n","\n","$\\mu$ can be any patameterizaed function. <br><br>\n","\n","But to keep things simple, <br>\n","let's make it a linear function of the state features. \n","\n","<br><br>\n","\n","\n","$\\sigma(s,\\theta) \\quad \\doteq \\quad \\exp \\big( \\theta_{\\sigma}^T X(s) \\big)$ <br><br>\n","\n","The policy parameters $\\theta$ associated with $\\mu$ are denoted by $\\theta_{\\mu}$ <br>\n","The parameterized function $\\sigma$ has one constraint. It must be positive. <br><br>\n","\n","To enforce this, <br>\n","we will parameterize it as the exponential of a linear function. <br><br>\n","\n","The policy parameters $\\theta$ associated with $\\sigma$ are denoted by $\\theta_{\\sigma}$ \n","\n","<br><br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1oOwbP_wxILfOIh2y7h_cA9MPFbxaL_vv\" alt=\"4-29\" width=\"500\">\n","\n","<br>\n","\n","__Policy Parameters__ <br><br>\n","\n","$\\theta \\quad \\doteq \\quad \\begin{bmatrix} \\theta_{\\mu} \\\\ \\theta_{\\sigma} \\end{bmatrix}$ <br><br>\n","\n","Our policy parameters now consists of these two stack parameter vector of equal size.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xw324y-k_rAW","colab_type":"text"},"source":["### Sampling Actions From the Gaussian Policy <br><br>\n","\n","\n","We've defined our Gaussian Policy. <br><br>\n","\n","Remember, <br>\n","the main point of a policy is that it gives us a way to select actions ! \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1vIU8Hg2t_RcVK05rfEMuFbMCraxXnpyW\" alt=\"4-30\" width=\"500\">\n","\n","<br>\n","\n","#### Gaussian poilicy <br><br>\n","\n","To select actions with this policy, <br>\n","we sample from the Gaussian.\n","\n",">Many programming libraries include functions to sample from Gaussian. <br>\n",">( so we won't worry about that here )\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=11dE1C9to02ZjQ3n7VL6XznRyvY0UKFbV\" alt=\"4-31\" width=\"500\">\n","\n","<br>\n","\n","#### Sampling procedure <br><br>\n","\n","The procedure is simple. <br><br>\n","\n","In a state, <br>\n","we compute $\\mu$ and $\\sigma$ from that state. <br><br>\n","\n","We then call the Gaussian random number generator with that $\\mu$ and $\\sigma$. <br>\n","( for sampling )\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1oQY8t73OkUvFJkJjvQemTFen36NHFtcr\" alt=\"4-32\" width=\"500\">\n","\n","<br>\n","\n","#### Sampling action from a wide range of actions <br><br>\n","\n","\n","In one state, <br>\n","$\\mu$ and $\\sigma$ might look like this. <br><br>\n","\n","For that state, <br>\n","a wide range of actions are likely to be sampled ! <br><br>\n","\n","Here's an action that might be sampled in this state. (red dot)\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=16ZGeRqYJAqvckRdb3nLnRTHXjPYai2QI\" alt=\"4-33\" width=\"500\">\n","\n","<br>\n","\n","#### Sampling action from a narrow range of actions <br><br>\n","\n","In other state, <br>\n","$\\mu$ ans $\\sigma$ might look like this. <br><br>\n","\n","For this state, <br>\n","Only a narrow range of actions are likely to be sampled ! <br><br>\n","\n","Here's an action that might be sampled in this state.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"I2fqProLnhhA","colab_type":"text"},"source":["### Gaussian Policies in Action <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1MJxelXJuyL6W3dJh4yVLEZWaOyD4cOfu\" alt=\"4-34\" width=\"500\">\n","\n","<br>\n","\n","Sigma $\\sigma(s,\\theta)$ essentially controls the dgree of exploration ! <br><br>\n","\n","We typically initialize the variance $\\sigma$ to be large <br>\n","so that a wide range of actions are tried.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1qDXluFgzJQRNF-ShZ1oaEtSUF79i0GaK\" alt=\"4-35\" width=\"500\">\n","\n","<br>\n","\n","As learning progresses, <br>\n","we expect the variance to shink and the policy to concentrate around the best action in each state ! <br><br>\n","\n","Like many parameterized policies, <br>\n","the agent can reduce the amount of exploration over time through learning. \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YVoVaCE8tqN3","colab_type":"text"},"source":["### Gradient of the Log of the Gaussian Policy <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1Ad5LgeFV3YeEvLTgFqUDCQeVp1jIC453\" alt=\"4-36\" width=\"500\">\n","\n","<br>\n","\n","\n","#### Similar algorithm of Continuous actions to Discrete actions <br><br>\n","\n","__similar__ <br><br>\n","\n","Though we now have continuous actions instead of dicrete actions, <br>\n","we can use the same Actor-Critic architecture. <br><br>\n","\n","The algorithm is actually very similar. <br><br>\n","\n","\n","__different__ <br><br>\n","\n","The main difference is that <br>\n","the gradient of the policy is defferent <br>\n","because the parameterization is defferent. <br><br>\n","\n","Underneath, <br>\n","the objective also looks slightly different <br>\n","because we intgergrate over actions instead of summing. \n","\n","<br><br>\n","\n","\n","#### the Gradient of the Log of the Gaussian Policy <br><br>\n","\n","But sampling the gradient ends up being the same. <br><br>\n","\n","So all we have to do is <br>\n","work out the gradient of the natural log of policy. <br><br>\n","\n",">We'll skip the steps here, <br>\n",">but the result is shown on the slide. <br>\n",">As an exercise, try to verify it for yourself.\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3rM19ulxzW5c","colab_type":"text"},"source":["### Andvantages of Continuous Actions <br><br>\n","\n","\n","We now havea an update rule for continuous actions. <br><br>\n","\n","But \" why did we do all this <br>\n","given that the discrete actions that we used previously seemed to work fine on this problem ? \" \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1UKFscoU4ZnmfX_5xtq-dYijBGIMBTHa-\" alt=\"4-37\" width=\"500\">\n","\n","<br>\n","\n","The most obvious reaseon is <br>\n","it might not be straightforward to choose a discrete set of actions. <br><br>\n","\n",">For example, <br>\n",">imaginer a robot trained to play golf. <br>\n",">Putting accurately requires puite a bit of precision. <br>\n",">We could try to pick up a discrete set of forces to apply to the club, and hopt it's good enough. <br>\n",">But inevitably, there will be situations where it isn't. <br>\n",">Depending on the distance and terrain, we may need to use the full range of forces.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=18rsTQPRkEoyL9WKRqpcu19wn-suTpfrx\" alt=\"4-38\" width=\"500\">\n","\n","<br>\n","\n","Continuous actions also allow us to generalize over actions. <br><br>\n","\n",">For example, <br>\n",">the Gaussian policies smoothly assings probability density to nearby actions. <br><br>\n",">\n",">Imagine an action is found to be good and the update increases density for that action. <br>\n",">The density for a nearby action will also increae <br>\n","with the agent generalizing that those actions are likely also to be good.\n","\n","<br>\n","\n","> Finally, <br>\n",">even if the true action set is discrete <br>\n",">but very large, it might be better to treat them as a continuous range. <br>\n",">( The Law of Large number ) <br><br>\n",">This gives a natural way to generalize over them <br>\n",">and avoids the costly process of exploring each one independently.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tR9IhmAO3NOX","colab_type":"text"},"source":["### Pendulum Swing Up with a Gaussian Policy <br><br>\n","\n","\n","Now, let's see how this Actor-Critic algorithm performs on the pendulum swing-up task. <br><br>\n","\n","Again, we swept over a many step size values. <br>\n","We use seperate step sizes for $\\theta_{\\mu}$ and $\\theta_{\\sigma}$.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1u_u2cK6nfUiKYvBmN8rZjLAVm2JEVF9I\" alt=\"4-39\" width=\"500\">\n","\n","<br>\n","\n","Here we visualize the policy in early learning. <br><br>\n","\n","It doesn't look too different from when we were using a discrete action set. <br>\n","It quickly learns to rock the pendulum back and forth. <br><br>\n","\n","The size of the area around the pivot indicates the magnitude of the angular acceleration applied at a given time.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1ptnHEz6xmdFtujCeDACtfc74urnm2NZ4\" alt=\"4-40\" width=\"500\">\n","\n","<br>\n","\n","Here we visualize the policy at the end of learning. <br><br>\n","\n","It's learning a very nice balancing policy. <br><br>\n","\n","Here again, we added perturbations <br>\n","to highlight the robustness of the learned policy.\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oBVgXBuN8UTy","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - we can apply actor-critic to problems with continuous actions <br><br>\n","\n","  - we showed an example of <br>\n","  how this can be done with a Gaussian policy parameterization\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]}]}
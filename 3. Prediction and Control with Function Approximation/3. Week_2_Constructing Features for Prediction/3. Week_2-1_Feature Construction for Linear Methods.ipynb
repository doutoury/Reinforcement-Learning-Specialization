{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. Week_2-1_Feature Construction for Linear Methods","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyMJClRwfNy28TSuHpDTDrvt"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yydQ9qjsKBRW","colab_type":"text"},"source":["# Week_2 <br>\n","\n","INDEX <br><br>\n","\n","\n","  - Feature Construction for Linear Methods <br>\n","    - Coarse Coding <br>\n","    - Generalization Properties of Coarse Coding <br>\n","    - Tile Coding <br>\n","    - Using Tile Coding in TD <br><br>\n","  \n","  - Neural Networks <br>\n","    - What is a Neural Netowork ? <br>\n","    - Non-linear Approximation with Neural Networks <br>\n","    - Depp Neural Networks <br><br>\n","\n","  - Training Neural Networks <br>\n","    - Gradient Descent for Training Neural Networks <br>\n","    - Optimization Stragies for NNs <br>\n","    - David Silver : on Deep Learning + RL = AI ? <br>\n","    - Week 2 Review\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"39ARKswlLNZg","colab_type":"text"},"source":["## __1. Feature Construction for Linear Methods__ <br><br>\n","\n","\n","  - Coarse Coding <br><br>\n","\n","  - Generalization Properties of Coarse Coding <br><br>\n","\n","  - Tile Coding <br><br>\n","\n","  - Using Tile Coding in TD \n","\n","\n","\n","\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"SrLl-g65LbKc","colab_type":"text"},"source":["## $\\cdot$ Coarse Coding <br><br>\n","\n","\n","  - Describe Coarse Coding <br><br>\n","\n","  - Descirbe how Coarse Coding relates to State Aggregation\n","\n","\n","<br><br>\n","\n","The feature is used to construct value estimates, <br>\n","(value estimates) are one of the most important part of Reinforement Learning Agent. <br><br>\n","\n","We'll discuss one simple but effective way to do it. <br>\n","$\\rightarrow \\quad$ Coarse Coding ! \n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QQ7onlrnVWVE","colab_type":"text"},"source":["### Recall $\\quad : \\;$ Linear value function approximation <br><br>\n","\n","\n","Let's talk about a new way to create features for Linear value functions.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1AT_C0vaGM6HjcfSjI_R-8We6740O9ESh\" alt=\"1-01\" width=\"500\">\n","\n","<br>\n","\n","Recall that approximate value functions are parameterized by weight vector $W$. <br>\n","$v_{\\pi}(s) \\approx \\hat{v}(s,W)$ <br><br>\n","\n","To calculate the value of a state, <br>\n","we first compute the features and construct the feature vector $X(s)$. <br>\n","Then the value of the state is approximated by dot product between the weight vector and the featrue vector. <br>\n","$\\hat{v}(s,W) = W^T \\cdot X(s)$ \n","\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1xCUO1zGo7SKwElJYzavyuiguLuQwB7AW\" alt=\"1-02\" width=\"500\">\n","\n","\n","Recall that a Tabular representation can be expressed as a binary feature vector. <br>\n","Each state is associated with a different feature. <br>\n","If the agent is in a state, then the feature corresponding to that state is $1$. <br>\n","All other features are $0$. <br><br>\n","\n","the Tabular case is just a special case of Linear function approximation <br>\n","where the feature vector is an indicator or one-hot encoding of the state. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=13MuI6S25Ixht_dxuqwYLG5rCvA3LI70y\" alt=\"1-03\" width=\"500\">\n","\n","<br>\n","\n","Of course, this is not feasible when the size of the state space becomes much larger than the agent's available memory.\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_vnCsjkgYO-w","colab_type":"text"},"source":["### Recall $\\quad : \\;$ State Aggregation <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1xGgWv8d5PdI5SnsMOs-cB1EGn7zURFWP\" alt=\"1-04\" width=\"500\"> \n","\n","<br>\n","\n","\n","\n","\n","Suppose we want to represent the location of this fish <br>\n","that's swimming in a pond, a two-dimensional state. <br><br>\n","\n","The fish can be one of infinitely many locations. <br>\n","It's impossible to represent all these locations with a finite lookup table ! \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1BBHG0HSNVua-DFUUVgo5UlCAkrQvAAMH\" alt=\"1-05\" width=\"500\">\n","\n","<br>\n","\n","Recall that we can use State Aggregation to associate nearby states  with the same feature ! <br><br>\n","\n","This is like treating all states within each square as the same state. <br>\n","In this example, all the groups have roughly the same shapae.\n","\n","<br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1N5AGDrYuuuVfRS-eIR4imWcB75aNZwo0\" alt=\"1-06\" width=\"500\">\n","\n","<br>\n","\n","In this example, all the groups have roughly the same shape, <br>\n","but it's not necessary to use the same shape ! <br><br>\n","\n","In general we can aggregate states using any shapes we want as long as those shapes do not have any gaps or overlap. <br><br>\n","\n","<br><br>\n","\n","\n","State Aggregation using uniform shape | State Aggregation using free shape | Coarse Coding using overlaped shape\n","--- | --- | ---\n","<img src=\"https://drive.google.com/uc?id=10heKoBlJpPNVbUhzQjJjMgWLId9NIj-d\" alt=\"1-07\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1FofMl5xgiwVrnWwsEhFEl5TVm4Fu62uI\" alt=\"1-08\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1fj-iF9p0v8jdvgU8QbmsXIhf9fNEFtwP\" alt=\"1-09\" width=\"500\">\n","\n","<br>\n","\n","State Aggregation does not usually allow the shapes to overlap. <br>\n","But, this restriction is not necessary. <br><br>\n","\n","In fact, by allowing overlap, <br>\n","We obtain a more flexible class of feature representations called Course Coding. \n","\n","\n","\n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FORcyjDclBKI","colab_type":"text"},"source":["### Corse Coding <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1edORF-_POlGSQdchW0UbJL-yazC-WTqH\" alt=\"1-10\" width=\"500\">\n","\n","<br>\n","\n","Let's look at the example feature vector for the fish's current location in the pond. <br><br>\n","\n","Remember, <br>\n","each index in the feature vector corresponds to one of the shapes. <br>\n","The feature corresponding to the circle is active or set to $1$ if the fish is within that circle. <br>\n","Otherwise the feature set to $0$. <br><br>\n","\n","The features receptive feild corresponds to the locations that activate that feature. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=17Iii6B1WzE-B4M9jR7DugGA2Z-662HWk\" alt=\"1-11\" width=\"500\">\n","\n","<br>\n","\n","When the fish moves to a new locations, it's usually covered by a different set of circles. <br><br>\n","\n","Nearby states will have similar feature activations, <br>\n","but they may also have different components active including different numbers of active features. <br><br>\n","\n","In this example, <br>\n","there is always at least one active feature, and at most three active features. \n","\n","<br><br><br>\n","\n","\n","\n",">All the ideas we've discussed so far are not limited 2-dimensional state spaces. <br>\n",">Coarse Coding can also be applied to hiegher dimensional inputs. <br>\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4-omWRdsl6ko","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - Tabular states can be represented with a binary one-hot encoding <br><br>\n","\n","  - Coarse Coding is a generalization of State Aggregation\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4qDKIpBCpxX8"},"source":["## $\\cdot$ Generalization Propertiese of Coarse Coding <br><br>\n","\n","\n","  - Describe how coarse coding parameters affect discrimination and generalization <br><br>\n","\n","  - Understand how discrimination and generalization affects learning accuracy (and speed)\n","\n","\n","<br><br>\n","\n","\n","We talked about a few different representations : <br><br>\n","\n","  - Tabular <br><br>\n","\n","  - State Aggregation <br><br>\n","\n","  - Coarse Coding\n","\n","<br><br>\n","\n","\n","\" Why Coarse Soding might be useful ? \" <br><br>\n","\n","Today we'll talk about <br>\n","\" how changin the properties of Coarse Coding affects Generalization and Discrimination. \"\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BsabMq8UsQYN","colab_type":"text"},"source":["### Broadness of generalization <br><br>\n","\n","\n","We've talked about <br>\n","how course coding groups states into featrues of arbitrary shapes and sizes. <br>\n","They can be circle, elipses, squares, or a combination of different shapes. <br><br>\n","\n","Let's look at <br>\n","how changing the shapes and sizes of features impacts generalization and discriminations, <br>\n","and so affects the speed of learning and the value functions we can represent. \n","\n","<br><br>\n","\n","\n","1 | 2\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1ybucGLbX0YDNWql9-ESt2qN34fPvLw8y\" alt=\"1-12\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1ZlhzBuK5BmB9UQcU5FWMGwvc3G6S9wc8\" alt=\"1-13\" width=\"500\"> \n","\n","<br>\n","\n","Notice that performing an update to the weights in one state <br>\n","changes the value estimate for all states within the receptive feilds of the active features. \n","\n","<br><br>\n","\n","If the union of the receptive feilds for the active features is large, <br>\n","the feature representation generalizes more. <br><br>\n","\n","Conversely, if the union is small, <br>\n","there's little generalization. <br><br>\n","\n","Here, the larger circle on the right generalizes more broadly distributing the update across a larger number of states. \n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gsagZrMhvIv3","colab_type":"text"},"source":["### Direction of generalization <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=14P5FP_5waG2H7hKUoV9W8ntszXeHtsjr\" alt=\"1-14\" width=\"500\"> \n","\n","<br>\n","\n","Generalizaiton is not just a scalar quantity however. <br>\n","Using different shapes in Coarse Coding can change the direction of generalization as well. \n","\n","<br><br>\n","\n","\n","Let's compare the previous example with circles to how coarse coding generalizes with vertically elongated elipses. <br><br>\n","\n","The receptive feilds made from ellipses are longer than they are wide. <br>\n","With these ellipses, Coarse Coding primarily generalizes in the vertical dimension. \n","\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5u0PnAHpwqhH","colab_type":"text"},"source":["### State discrimination <br><br>\n","\n","\n","So we've talked about how the shape and size of the receptive feilds impact generalization, and so the speed of learning. <br><br>\n","\n","\" What about the final accuracy of our estimates ? \" <br>\n","This is where Discrimination comes in. \n","\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1aBMdbNT0Gy0S_0S5KTVVFokTaWRz5zc7\" alt=\"1-15\" width=\"500\"> \n","\n","<br>\n","\n","Recall that <br>\n","the ability to distinguish between values for two different states is called Discrimination. \n","\n","<br><br>\n","\n","\n","In Coarse Coding, <br>\n","the overlap between circles dictates the level of discrimination. <br><br>\n","\n","It is impossible to do perfect discrimination <br>\n","because we can never update the value of one state without impacting the values of other states. \n","\n","<br><br>\n","\n","\n","The colored shapes depict the discriminative ability of this particular Coarse Coding. <br>\n","We've only highlighted a few regions to keep the visualization simple. <br><br>\n","\n","Every state within the same colored shape will have the exact same feature vector. <br>\n","As a result, they must all have the same approximate value. <br><br>\n","\n","The smaller these regrions are, the better we can discriminate. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1558hSHwt6tTcufYhpzIG-8MrUhU4skVQ\" alt=\"1-16\" width=\"500\"> \n","\n","<br>\n","\n","With many circles, <br>\n","the regions becomes smaller, and we can discriminate more finely btween the values of different states. <br><br>\n","\n","Or we can make the circles samller. \n","\n","<br><br><br>\n","\n","\n","\n","So the size, number, and the shape of the features all affect the discriminative ability of the representation.\n","\n","\n","\n","\n","<br><br><br>\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5Os6PefUz-Ic","colab_type":"text"},"source":["### 1-D Example <br><br>\n","\n","\n","Let's look at a simple example with the one-dimensional input space. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1_tvQdzm9_P4htVqn2VQCG3vGQQpu02x1\" alt=\"1-17\" width=\"500\"> \n","\n","<br>\n","\n","Consider learning approximation to a step function. <br>\n","Let's assume we can sample the true function values in order to update our estimates. <br><br>\n","\n","This example should help you better understand <br>\n","how representation choices impact the speed of learning and the quality of the final approximation. \n","\n","<br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1C1XytYo11NYPuB506mX-l7YuP51zfr0A\" alt=\"1-18\" width=\"500\"> \n","\n","<br>\n","\n","For our 1-dimensional function, <br>\n","the receptive feilds of each feature will be represented as overlaping intervals. <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1CIXSUWLSEph2KB9Yflc8D4mqjGzyWOGR\" alt=\"1-19\" width=\"500\"> \n","\n","<br>\n","\n","Let's start with this relatively short interval. <br><br>\n","\n","We'll lay about 50 of these intervals <br>\n","so that they overlap ranomly over the domain of our function. <br><br>\n","\n","Let's see how our estimate of the function changes <br>\n","as we ranodmly sample the true of the function. <br><br>\n","\n","We start with an initial estimate of $0$ depicted as a flat-line. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1lB39K_cLdRIyvcBA3FQzSlFltKExQskO\" alt=\"1-20\" width=\"500\"> \n","\n","<br>\n","\n","The receptive feild for each feature is quite small. <br><br>\n","\n","So even after many samples, (40 samples) <br>\n","our approximation of the step function is not that great. <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1ez68ENyP8pTtnBLrJjokS7NEeFWHdTeR\" alt=\"1-21\" width=\"500\"> \n","\n","<br>\n","\n","With much more tratining, <br>\n","our approximation finally obtains a close match to the true function. <br>\n","But it's not perfect. <br><br>\n","\n","This is easy to see by inspecting the approximation of the top of the function. <br>\n","It's not nearly as flat or smooth. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1oM-q4c1GbbxCIX7HUAZRIkR63_K6lFiO\" alt=\"1-22\" width=\"500\"> \n","\n","<br>\n","\n","Let's try this again with longer intervals ! <br><br>\n","\n","The receptive feild of each feature is quite large. <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1ez68ENyP8pTtnBLrJjokS7NEeFWHdTeR\" alt=\"1-23\" width=\"500\"> \n","\n","<br>\n","\n","This means we can approximate the rough shape of the function with relatively few samples. (40 samples) <br><br>\n","\n","As we sample the function more, <br>\n","our estimate forms a better and better approximation of the true function. \n","\n","<br><br>\n","\n","\n","The broad generalization of the longer intervals made learning faster. <br>\n","We needed less samples to get a good approximation. <br><br>\n","\n","The large number of longer intervals also resulted in better discrimination, <br>\n","better final approximation of the true function. \n","\n","<br><br>\n","\n","\n",">In this example, <br>\n",">Longer intervals ended up achieving better Generalization and Discrimination. <br><br>\n",">\n",">But this may not always be the case. <br>\n",">Each task may require different feature properties and there's not one general solution. \n","\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"o7-Bv-uGX_ST","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - The size, number, and shape of the feature affects generalization <br><br>\n","\n","  - The resulting shape intersections affect the ability to discriminate \n","\n","\n","<br><br>\n","\n","Coarse Coding is a very general type of representation. <br>\n","Understanding how it generalizes and discriminates during learning will help us\n","understand other representations. <br>\n","( including Neural Networks )\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_kWKWzbLY8rf"},"source":["## $\\cdot$ Tile Coding <br><br>\n","\n","\n","  - Explain how Tile Coding achieves both generalization and Discrimination <br><br>\n","\n","  - Understanding the benefits and limitations of Tile Coding \n","\n","<br><br>\n","\n","Today, <br>\n","let's loot at a computationally efficient way to perform Course Coding $\\quad \\rightarrow \\quad$ Tile Coding ! \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DrO1RKohZCHI","colab_type":"text"},"source":["### Tile Coding <br><br>\n","\n","\n","Tile Coding is a specific type of Couse Coding. <br>\n","Tile Coding pergorms an exhaustive partition of the state space using overlapping grids. <br><br>\n","\n","This is perhaps best understood through an example. <br>\n","A pond with two-dimensional continuous state space.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1TYm5ejbwXTiWOUO-NQ6v-eYIsq6dIUKP\" alt=\"1-24\" width=\"500\">\n","\n","<br>\n","\n","Unlike Coarse Coding which uses arbitrary shapes, <br>\n","Tile Coding uses squares ! \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1uE_JueeXIwSeytXKcie-Y5vmp8M46JYY\" alt=\"1-25\" width=\"500\">\n","\n","<br>\n","\n","What's the most convenient way to lay out a bunch of squares over space ? <br>\n","It's a grid ! $\\quad$ ( called this grid a tiling ) <br><br>\n","\n","So far using this is just State Aggregation. <br>\n","Larger tiles will result in increased generalization. <br>\n",">Although the ideal tile size depends on the specific problem, <br>\n",">it's generally a good idea to use larger tiles. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1LzOzkNcbC2Zq5HX3rWrl0oA4d9_92kop\" alt=\"1-26\" width=\"500\">\n","\n","<br>\n","\n","To improve the dicriminative ability of our tile coding, <br>\n","we can put several tilings on top of each other. <br>\n","Each tiling is offset by a small amount. <br><br>\n","\n","Offsetting each layer of tiling creates many small intersections. <br>\n","This result in better Discrimination ! <br><br>\n","\n","In practice, <br>\n","it is useful to use a large number of tilings. \n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZXwEzEUspf1H","colab_type":"text"},"source":["### Shape of generalization <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=11qpnUqg09jcD3pAqFQ6YRjyTa4mFV25Y\" alt=\"1-27\" width=\"500\">\n","\n","<br>\n","\n","For one tiling, <br>\n","generalization only occurs within the square. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1PGp0-GDVTD0TMW_eqd1xbcHejcAHa-7g\" alt=\"1-28\" width=\"500\">\n","\n","<br>\n","\n","But with multiple tilings, <br>\n","Updates in this state generalize to these other states ! <br><br>\n","\n","the gengeralization in this case is diagonal. <br>\n","If we had used random offsets, <br>\n","then the generalization would be more spherical and homogeneous. \n","\n",">This is discussed more in the textbook. \n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YPlFdIR9sLzu","colab_type":"text"},"source":["### Direction of generalization <br><br>\n","\n","\n","Let's talk about how to control the generalization properties further. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=15CuJ81nV0UFm1Y1crMFeUl0wyvu7N0Nu\" alt=\"1-29\" width=\"500\">\n","\n","<br>\n","\n","We can do this by creating a grid of rectangles rather than squares. <br>\n","An efficient way to do this is to scale each dimension of the state space. <br><br>\n","\n","\n","The environment appears to have gotten squished here, <br>\n","actually, layering squares over this squished environment is like layering rectangles over the unsquished environment. \n","\n","<br><br>\n","\n","\n","By using rectangles, <br>\n","we can control the broadness of the generalization across each dimension of the state space ! \n","\n","<br><br><br>\n","\n","\n","\n","### Computational efficiency of Tile Coding <br><br>\n","\n","\n","Tile Coding can represent a wide range of functions, <br>\n","but it's utility does not end there. \n","\n","<br><br>\n","\n","Tile Coding is also computationally efficient. <br>\n","Since grids are uniform, it's easy to compute which cell the currenct state is in. <br><br>\n","\n","Due to it's computational efficiency, <br>\n","Tile Coding can be used to quickly run preliminary experiments in low-dimensional environments ! \n","\n","<br><br>\n","\n","However, <br>\n","as the number of dimensions grows, the number of required tiles grows exponentially. <br>\n","As a result, it can be necessary to tile input dimension separately. <br>\n","Whether or now input dimensions can be treated independently depends on the specific problem. \n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NuEhyI3utwjG","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - The shape, size of tiles, and the number of tilings affects the generalization and discrimination of value function approximation with tile coding <br><br>\n","\n","  - Why tile coding is a computationally efficient version of coarse coding\n","\n","\n","<br><br>\n","\n","In this video, <br>\n","we talked about Coarse Coding, and <br>\n","how it is a generalization of state aggregation. \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0vk59DE23P8E"},"source":["## $\\cdot$ Using Tile Coding in TD <br><br>\n","\n","\n","  - Explain how to use tile coding with TD learning <br><br>\n","\n","  - Identify important properties of tile coded representations\n","\n","\n","<br><br>\n","\n","So far we've learned about Tile Coding. <br>\n","But we haven't connected it back to Reinforcement Learning yet. <br><br>\n","\n","Today, <br>\n","we will explain \" How to use Tile Coding together with linear TD \". \n","\n","<br><br><br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bjPlwS1-4Is8","colab_type":"text"},"source":["### Sparse binary representations <br><br>\n","\n","\n","The number of active tile is always significantly less than the number of total tiles. <br>\n","We can use this to calculate the value function efficiently.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1R_ZkumQyWqXIIu5sk68ahuZznMEVUAim\" alt=\"1-30 \" width=\"500\">\n","\n","<br>\n","\n","Recall that <br>\n","with Linear function approximation, <br>\n","the value function is a dot product between a weight vector $W^T$ and a feature vector $X(s)$. <br>\n","$V_{\\pi} \\approx \\hat{v}(s,W) = W^T \\cdot X(s)$ \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1pBhPSjsgz3SMFOmmzdcSQez8QKEPqm65\" alt=\"1-31\" width=\"500\">\n","\n","<br>\n","\n","Let's see what this dot product looks like with a sparse binary feature vector. <br><br>\n","\n","If we were to multiply the two vectors element-wise (산술연산), <br>\n","many of the elemnet-wise products will be $0$. <br><br>\n","\n","This means we only have to consider the weights ($w_i$) at the non-zero elemnets ($1$) of the feature vector ($X(s)$) <br>\n","because the features are binary, the weights at the non-zero features are multiplied by $1$. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1jUPsuJr9x9vCHQWQVctwd3k5ckaXZtFO\" alt=\"1-32\" width=\"500\">\n","\n","<br>\n","\n","Computing the dot product in the usual way would be expensive ! <br><br>\n","\n","Instead, we can just sum the weights corresponding to the active features. <br>\n","$\\hat{v}(s,W) = w_{10} + w_{26} + w_{42} + w_{53}$ <br><br>\n","\n","Note that this takes a certain amount of time <br>\n","because the number of active features ($X(s)$ 의 원소 갯수 ?) is the same in every state. \n","\n","<br><br>\n","\n","\n","The feature vectors produced by Tile Coding may query in the value function chaep computationally. \n","\n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Mw8IQq6x6icB","colab_type":"text"},"source":["### A simple example <br><br>\n","\n","\n","Let's look an example of value function with tile coded features. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1uJ1e_F0jTPJxF5phf3O8Ev2DznR_8Ypy\" alt=\"1-33\" width=\"500\">\n","\n","<br>\n","\n","Suppose the pond with two tilings of four tiles each. <br>\n","We've color-coded the tilings and each tiling qaure is indexed as shown. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1ltxhlVU7qaFUnaG848riBnTshFmy4T8q\" alt=\"1-34\" width=\"500\">\n","\n","<br>\n","\n","If the position of the fish and a weight vector are as shown, <br>\n","what is the value of the fish's position ? <br><br>\n","\n","We can see the fish intersects square 1 belonging to the purple tiling and square 4 belonging to the orange tiling. <br><br>\n","\n","This results in the following feature vector for this state. <br>\n","Now we simply calculate the state's value with a dot product. <br><br>\n","\n","The feature vector is binary. <br>\n","So we can just add up the weights at the locations with non-zero features. <br><br>\n","\n","This gives us a value of \" $2$ \". \n","\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IbbO-Ob-Kh-V","colab_type":"text"},"source":["### Tile Coding v.s. State Agrregation <br><br>\n","\n","\n","Now let's run experiment with Tile Coding and compare it to State Aggregation ! <br>\n","Recall, the Tile Coding effectively multiples instances of State Aggregation or tilings layered on top of each other. \n","\n","<br><br>\n","\n","\n","1 | 2 | 3\n","--- | --- | ---\n","<img src=\"https://drive.google.com/uc?id=1LIeUdTbIKQqOg2L7VE1tsZC79U0yeOXi\" alt=\"1-35\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=11tTZltPMi3J6d_Xc1-dp7K7VmukKZ7kp\" alt=\"1-37\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1HdrPxxCiyO_PZ4_ToAp3SgXIbvBKuWph\" alt=\"1-36\" width=\"500\">\n","\n","<br>\n","\n","Our environment is the 1000 state Random walk. <br><br>\n","\n","The states are numbered from 1 to 1000 <br>\n","ans all episodes begin in state 500. <br><br>\n","\n","At each time step, <br>\n","the agent randomly transitions to one of the 200 neighboring states on either side. <br>\n","Over shooting states 1 or 1000 will result in a transition to the terminal state. <br>\n","\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1LtlYJs4aFMDiEyjx4UUcFoc0LggpuWcx\" alt=\"1-38\" width=\"500\">\n","\n","<br>\n","\n","Let's talk about how we can use function approximation in this problem. <br><br>\n","\n","  1. State Aggregation. <br>\n","  Let's use the state aggregation with five groups, each corresponding to 200 states\n","\n","  2. Tile Coding <br>\n","  Let's design a tile code for this problem with 50 tilings. <br>\n","  We treat the 1000 states as an interval where each tile correponds to 200 states. \n","\n","<br><br>\n","\n","Sinse each tile has 200 states, <br>\n","you might think we only need five tiles to cover the full space of 1000 states. <br><br>\n","\n","However, each tiling is slightly shifted or offest from the others. <br>\n","An extra tile is needed to cover the space left uncovered. <br>\n","Because of this, each of our tilings contains six tiles rather than five. \n","\n","<br><br>\n","\n","\n","Let's compare these two function approximators with Gradient Monte-Carlo. <br><br>\n","\n","The step size $\\alpha$ is scaled by the number of active features. <br>\n","$\\alpha = 0.0001 * 50$ for tile coding <br>\n","$\\alpha = 0.0001 * 1$ for state aggreaation. \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"eAqfG68rSuEo","colab_type":"text"},"source":["### Result of Tile Coding v.s. State Agrregation <br><br>\n","\n","\n","Let's look at the result.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=17QrkttLrNXA470HL_lrr7oJl_AKiZq5Q\" alt=\"1-39\" width=\"500\">\n","\n","<br>\n","\n","We plot the Value Error averaged over 30 runs after each episode. <br><br>\n","\n","Both agents learn quickly due to aggresive generalization. <br><br>\n","\n","However, <br>\n","the Tile Code representation is able to better discriminate between states, <br>\n","and achieves a lower value error. \n","\n","<br><br>\n","\n","\n","It's interesting that <br>\n","even though the tile coder has many more parameters, it can learn just as quickly as the core state aggregation ! <br><br>\n","\n","But it also achieves better discrimination ! <br>\n","because of the small intersections created by multiple overlapping tiles. \n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xODE9WDGS3OE","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - Tile Coding can be used to approximate value function in policy evaluation <br><br>\n","\n","  - Compared Tile Coding's discriminative ability to that of State Aggregation\n","\n","<br><br>\n","\n","\n","Next time, <br>\n","we're going to move beyond fixed representations, <br>\n","and begin learning representations with Neural Networks. \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. Week_2-3_Training Neural Networks","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyPEfwpW5sQVBJ2Evkoi5T7w"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"39ARKswlLNZg","colab_type":"text"},"source":["## __3. Traing Neural Networks__ <br><br>\n","\n","\n","  - Gradient Descent for Training Neural Networks <br><br>\n","\n","  - Optimization Strategies for NNs <br><br>\n","\n","  - David Dilver $\\quad : \\;$ on Deep Learning + RL = AI ? <br><br>\n","\n","  - Week 2 Review\n","\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"SrLl-g65LbKc","colab_type":"text"},"source":["## $\\cdot$ Gradient Descent for Training Neural Networks <br><br>\n","\n","\n","  - Derive the gradient of Neural Network <br><br>\n","\n","  - Implement Gradient Descent on a Neural Network\n","\n","\n","<br><br>\n","\n","The updates for our algorithms have been pretty simple and most have been based on Gradient Descent. <br>\n","The same is true for Neural Networks. <br><br>\n","\n","The back propagation algorithm is actually not that complex. <br>\n","It is in fact, just Gradient Descent. <br>\n","But the gradient is a bit more complex because of the nested functions. \n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FaVYVFmeXaOP","colab_type":"text"},"source":["### Recap on Gradient Descent <br><br>\n","\n","\n","1 | 2\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1tQZSZprtUMzoe86GalnTQvCl_Kn56gpn\" alt=\"3-01\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=12oAreEgKNFxkQMEqumM74mYwQOduE0mH\" alt=\"3-02\" width=\"500\">\n","\n","\n","<br>\n","\n","As in liner function approximation, <br>\n","the first step is define a loss on the parameters of the neural netwrok. <br>\n","And then derive the gradient. <br><br>\n","\n","\n","The loss function specifies how far the network's predictions are from being correct ! <br>\n","Our goal in training the neural netwrok is to find the parameters which minimize this loss function. <br><br>\n","\n","The gradient of a function points in the direction of greatest ascent. $\\frac{\\partial L}{\\partial w}$ <br>\n","By moving in the direction opposite the gradient $-\\frac{\\partial L}{\\partial w}$, we move in the direction that most quickly minimizes the loss. <br><br>\n","\n","So how do we compute this gradient of the loss function for a neural network ? \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hBKgUB9paVtk","colab_type":"text"},"source":["### Notation <br><br>\n","\n","\n","Before we get started, <br>\n","let's establish some notation.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1vWSiwKPsTd8jd1UCIzsucUrOQHuEOpO0\" alt=\"3-03\" width=\"500\">\n","\n","<br>\n","\n","The input to the network is $s$ <br>\n","The output of the network is $\\hat{y}$ <br>\n","The hidden layer is the learned features, called $x$ <br><br>\n","\n","The weights $A$ produce the features $x$, <br>\n","The weights $B$ linearly weight $x$ to produce the output $\\hat{y}$. \n","\n","<br><br>\n","\n","\n","The output of each layer of the network can be represented as a vector. <br>\n","$S \\quad A \\quad X \\quad B \\quad \\hat{Y}$ <br><br>\n","\n","\n","\n","\n","<br><br>\n","\n","\n","1 | 2\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1ORfl3y1KNuQxDHCaG-CiS_tM7htGybgS\" alt=\"3-04\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1s_i9szMr30LSJQ1PwyRydCUrlgDwJDO-\" alt=\"3-05\" width=\"500\">\n","The first index $i$ of the weight matrices $A$ refers to the inputs to that layer, <br> The second index $j$ of the weight matrices $A$ refers to the outputs. | One example of $L \\quad : \\;$ Squared Error\n","\n","<br>\n","\n","We assume for now, <br>\n","we have a \" generic loss $L$ \", so we can describe the basic idea. <br><br>\n","\n","An example of $L$ is the Squared Error. <br>\n","$L(\\hat{y}_k, y_k) = (\\hat{y}_k - y_k)^2$ <br><br>\n","\n","For the derivation, we will keep $L$ generic. <br>\n","We will give you the specific update with the Squared Error later. \n","\n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-BBjK6WqmH1S","colab_type":"text"},"source":["### Goal <br><br>\n","\n","\n","Before we dive into the derivation, let's get oriented. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1jfGjX8D0BWqPEg2w5Fs7n14ukQ1ZjFwy\" alt=\"3-06\" width=\"500\">\n","\n","<br>\n","\n","Though the derivation is a bit involved, <br>\n","we will find that the final updates is quite simple. <br>\n","$\\qquad A = A - \\alpha \\delta^A S \\qquad \\qquad B = B - \\alpha \\delta^B X$ <br><br>\n","\n","Each matrix of parameters has an update <br>\n","that looks like an ' error term $\\delta$ ' times ' the input to that layer $S$ (for $A$) or $X$ (for $B$) '.\n","\n","<br><br>\n","\n","\n","Further, <br>\n","we will find that $\\delta^A$ can be efficiently computed as a function of $\\delta^B$ ! <br>\n","The errors of $\\delta^B$ from the output of the network are propageted backwards to this earlier layer <br>\n","to help determine the role $A$ had in producing that error ($\\delta^A$ ?).\n","\n","<br><br>\n","\n","\n","So now,\n","\" How de we get $\\delta^A$ and $\\delta^B$ ? <br>\n",">For fun and of course educational purposes, <br>\n",">let's do the full derivation now.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Q1YyOc-TlFMP","colab_type":"text"},"source":["### Deriving the gradient <br><br>\n","\n","\n","Let's start at the output of the network and work backwards ! <br>\n","There's a good reason for this as you will see. \n","\n","<br><br><br>\n","\n","\n","#### Deriving the gradient for $B$ <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=12FlPl8lcGevhKNrpBJcX-rdNc3Ra8VkJ\" alt=\"3-07\" width=\"500\">\n","\n","<br>\n","\n","We start by taking the partial derivative of the Loss function <br>\n","with respect to the first set of weights $B_{jk}$. <br>\n","$\\frac{\\partial L (\\hat{Y}_k, Y_k)}{\\partial B_{jk}}$ \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1oWmfw08i6BtObycxmzB2Yf8plpnxSDl-\" alt=\"3-08\" width=\"500\">\n","\n","<br>\n","\n","We use the chain rule <br>\n","given the derivative of $L (\\hat{Y}_k, Y_k)$ with repect to $\\hat{Y}_k$, <br>\n","times the derivative of $\\hat{Y}_k$ with respect to $B_{jk}$. <br>\n","$\\frac{\\partial L (\\hat{Y}_k, Y_k)}{\\partial \\hat{Y}_k} \\frac{\\partial \\hat{Y}_k}{\\partial B_{jk}}$ \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1lk0zlm1T3xisBahhNlL_AeM5HmkZf4be\" alt=\"3-09\" width=\"500\">\n","\n","<br>\n","\n","The next step <br>\n","is again to use the chain rule for this derivative. <br><br>\n","\n","To make this easier, <br>\n","let's introduce a new variable, $\\theta$. <br>\n","Theta is the output of the hidden layer $X$ times the last set of weights $B$. <br>\n","$\\theta \\doteq X B$ \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1kUgrCLRk_S7SMnyNQXr7lGdpJAIV4IaQ\" alt=\"3-10\" width=\"500\">\n","\n","<br>\n","\n","Let's rewrite $\\hat{Y}$ using $\\theta$. <br>\n","$\\hat{Y} \\doteq f_B(XB) \\quad \\rightarrow \\quad \\hat{Y} \\doteq f_B(\\theta)$ \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1gcrWiXM9LDZy60Dfn8hUgrU-5MYAw2yu\" alt=\"3-11\" width=\"500\">\n","\n","<br>\n","\n","Let's expand the derivative of $\\hat{Y}$ with repect to $B$. <br><br>\n","\n","Using the chain rule, <br>\n","we get the derivative of $f_B(\\theta)$ with respect to $\\theta$, <br>\n","times the derivative of $\\theta$ with respect to $B$. <br>\n","$\\frac{\\partial L (\\hat{Y}_k, Y_k)}{\\partial \\hat{Y}_k} \\frac{\\partial f_B(\\theta_k)}{\\partial \\theta_k} \\frac{\\partial \\theta_k}{\\partial B_{jk}}$ \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1B_tHAQVOxeGa4FVAGpsSaJqLlThWfZoE\" alt=\"3-12\" width=\"500\">\n","\n","<br>\n","\n","The derivative of $\\theta$ with respect to $B$ is $X$. <br>\n","$\\frac{\\partial \\theta_k}{\\partial B_{jk}} = X_j$ <br>\n","This is because $\\theta$ is a linear function of $B$, <br>\n","and because $X$ does not depend on $B$. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1zW_whZ0nTeODDhGfHmKqpvfpWr9GB99M\" alt=\"3-13\" width=\"500\">\n","\n","<br>\n","\n","Now we're done deriving the gradient for $B$, <br>\n","$\\frac{\\partial L(\\hat{Y}_k,Y_K)}{\\partial \\hat{Y}_k} \\frac{\\partial f_B(\\theta_k)}{\\partial \\theta_k} X_j$ \n","\n","<br><br>\n","\n","\n","Next we'll do the gradeint for $A$. <br><br>\n","\n","\n","\n","\n","\n","<br><br>\n","\n","[ 03:00 ~ 06:00 ] ( back-propagation 수식 유도 )\n","\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"j_G9yw78tQCj","colab_type":"text"},"source":["#### An example of the gradient <br><br>\n","\n","\n","But before that, <br>\n","let's make some choices for the loss and activation <br>\n","so that we can see a specific instance of this general equation for $B$.\n","\n","<br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=19S9a5qzWG6CN6yM2qwUsWJ8D2k0gGBbh\" alt=\"3-14\" width=\"500\"> \n","\n","<br>\n","\n","Let's define $L$ to be the squared error, <br>\n","$L = \\frac{1}{2} (\\hat{y}_k - y_k)^2$ <br><br>\n","\n","and use a linear activation on the last layer. <br>\n","$f_B(\\theta_k) = \\theta_k$ <br><br>\n","\n","The derivative of the loss with respect to $\\hat{y}_k$ is this. <br>\n","$\\frac{\\partial L (\\hat{y}_k,y_k)}{\\partial \\hat{y}_k} = (\\hat{y}_k - y_k)$ <br><br>\n","\n","The derivative of the activation for the output layer is $1$. <br>\n","$\\frac{\\partial f_B(\\theta_k)}{\\partial \\theta_k} = \\frac{\\partial \\theta_k}{\\partial \\theta_k} = 1$ <br><br>\n","\n",">This is because $f_B(\\theta_k)$ is the \" identity \", <br>\n",">and the derivative $\\theta$ with respect to itself is $1$.\n","\n","<br><br>\n","\n","Here is the generic gradient of the loss with respect to $B$. <br>\n","Plugging this into the gradient with repect to $B$, we get $(\\hat{y}_k - y_k)$ times $1$ times $X_j$. <br><br>\n","\n","$\\begin{align} \n","\\frac{\\partial L(\\hat{y}_k,y_k)}{\\partial B_{jk}} &= \\frac{\\partial L(\\hat{y}_k,y_k)}{\\partial \\hat{y}_k} \\frac{\\partial f_B(\\theta_k)}{\\partial \\theta_k} X_j \\\\\n","&= (\\hat{y}_k - y_k) X_j \n","\\end{align}$ <br><br>\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jS2X__54O8xC","colab_type":"text"},"source":["#### Deriving the gradient for $A$ <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1bhp2fxnNzKkZPUuERWfSkhHI_3rJrRQB\" alt=\"3-15\" width=\"500\">\n","\n","<br>\n","\n","To ease notation while computing the gradient with respect to $A$, <br>\n","let's define a new term $\\delta_k^B$ <br>\n","$\\delta_k^B = \\frac{\\partial L(\\hat{y}_k,y_k)}{\\partial \\hat{y}_k} \\frac{\\partial f_B(\\theta_k)}{\\partial \\theta_k}$ <br><br>\n","\n","We can replace most of the gradient for $B$ <br>\n","so that it now becomes $\\delta_k^B$ times it's inpute $X_j$. <br>\n","$\\delta_k^B X_j$\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1CvTq02GHsGcSx57V9K_0wJxKfn3CAUdU\" alt=\"3-16\" width=\"500\">\n","\n","<br>\n","\n","Now, to compute the gradient with repect to $A$, <br>\n","we need to go through the same steps as for $B$. <br><br>\n","\n","The main difference <br>\n","is that we have one extra chain rule step <br>\n","because the weights $A$ also affect $X$. <br><br>\n","\n","Let's start there. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1G2vPrRV7hVubQwTPJUOFdCJKrM25X0hW\" alt=\"3-17\" width=\"500\">\n","\n","<br>\n","\n","Let's expand this derivative using the chain rule. \n","\n","<br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1ybIxdAGCd2n1dKntw2sSRonC_SrvGidK\" alt=\"3-18\" width=\"500\">\n","\n","<br>\n","\n","Before we expand the next term, <br>\n","let's introduce another helper variable <br>\n","$\\psi \\doteq SA$ \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1x19WsvwzzgWrNxSJfQLq_dztIR8QWYtj\" alt=\"3-19\" width=\"500\">\n","\n","<br>\n","\n","And rewrite $X$ to use the $\\psi$. <br>\n","$X \\doteq f_A(SA) \\quad \\rightarrow \\quad X \\doteq f_A(\\psi)$ <br>\n","We then expand the next term. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1INSHhGVw6_39TonlEn7G-6ctRvvJL_lm\" alt=\"3-20\" width=\"500\">\n","\n","<br>\n","\n","Using the chain rule again, <br>\n","we get the derivative of $f_A(\\psi)$ with respect to $\\psi$ <br>\n","times the derivative of $\\psi$ with respect to $A$. <br>\n","$\\frac{\\partial X_j}{\\partial A_{ij}} = \\frac{\\partial f_A(\\psi_j)}{\\partial \\psi_j} \\frac{\\partial \\psi_j}{\\partial A_{ij}}$\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=17ssLX_zH1Ahv5FFjuHQdD5q3GeA2I1ry\" alt=\"3-21\" width=\"500\">\n","\n","<br>\n","\n","Because $\\psi = SA$, <br>\n","we get that $\\frac{\\partial \\psi_j}{\\partial A_{ij}} = S_i$.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1oU1x7IHeaDAdeRQjKgwSMxnTUnDb4NTR\" alt=\"3-22\" width=\"500\">\n","\n","<br>\n","\n","Putting it all together, <br>\n","we finally get the derivative with respect to $A$.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1hXssW5dlwZGnZD84g17c6I5TnC9scyQo\" alt=\"3-23\" width=\"500\">\n","\n","<br>\n","\n","We now have all we need <br>\n","to compute the gradient of the network parameters ! <br><br>\n","\n","We can clean up this derivative by again, <br>\n","defining a term $\\delta_j^A$. <br>\n","$\\delta_j^A = (B_{jk} \\delta_k^B) \\frac{\\partial f_A(\\psi_j)}{\\partial \\psi_j}$ <br><br>\n","\n","Now the derivative with respect to $A$ is $\\delta_j^A$ times it's input $S_j$.\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xmYjLhmDnmLz","colab_type":"text"},"source":["#### Result of gradient <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1qvivaOKknIQxZFCIo6zQC1m6YKMI71OT\" alt=\"3-24\" width=\"500\">\n","\n","<br>\n","\n","Notice that <br>\n","both gradients can be rewritten in a similar form. <br><br>\n","\n","They have a term $\\delta$ that contains an error signal times their input. \n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_EGFddyVxIoZ","colab_type":"text"},"source":["### The backprop algorithm <br><br>\n","\n","\n","Let's take a brief look at some pseudocode <br>\n","for implementing the backprop algorithm with Stochastic Gradient Descent. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1DteUkrcz3bfk7MkUyGFH2Tkdd3uO1-qi\" alt=\"3-25\" width=\"500\">\n","\n","<br><br>\n","\n","\n","For each data point $(s,y)$ in our dataset $D$, <br>\n","we first get our prediction $\\hat{y}$ from the network. <br>\n","This is typically called the forward pass. <br>\n","Then, we compute the gradients starting from the output. \n","\n","<br><br>\n","\n","\n","We first compute $\\delta_k^B$ <br>\n","and the gradient for $B$, ( $\\nabla_B^{jk}$ ), <br>\n","then we use this gradient to update the parameters $B$, with the step size $\\alpha_B$ <br>\n","$B = B - \\alpha_B \\nabla_B$ <br><br>\n","\n","Next, we update the parameters $A$. <br>\n","We compute $\\delta_j^A$ which uses $\\delta_k^B$. <br>\n",">Notice that <br>\n",">by computing the gradients of the end of the network first, <br>\n",">we avoid recomputing the same terms for $A$, that were already computed for $\\delta_k^B$. <br>\n",">In fact, <br>\n",">This is the main idea behind Back Propagation ! <br>\n",">It is simply gradient descent with this efficient strategy to compute gradients. \n","\n","We then comput the gradient for $A$, ( $\\nabla_A^{ij}$ )<br>\n","and update $A$ with this gradient using step size $\\alpha_A$ <br>\n","$A = A - \\alpha_A \\nabla_A$ \n","\n","<br><br>\n","\n","\n","This derivation n algorithm easily extend to deeper networks ! <br>\n","The delta for the earlier layer is similarly computed recursively using the delta in the next layer ! <br>\n","The update on each layer is always of the form of a $\\delta$ times the input to that layer ! <br><br>\n","\n","Essentially, <br>\n","\" the network passes back error information stored in the delta at each layer. \"\n","\n","<br><br><br>\n","\n","\n","\n","#### Example of the backprop algorithm $\\quad$ ( in some particular network ) <br><br>\n","\n","\n","To make this less abstract, <br>\n","let's look at the backprop algorithm for a particular network. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1Gk2d13O2RrNqFVxeK3-Ud6RV6BUFg7MP\" alt=\"3-26\" width=\"500\">\n","\n","<br><br>\n","\n","Here's the pseudo-code <br>\n","if we use the ReLU activation on the hidden layer, and a linear unit for the output. <br><br>\n","\n","First, we compute the error for the output layer. <br>\n","$\\delta_k^B = (\\hat{Y}_k - Y_k) X_j$ <br><br>\n","\n","Then, we comput the derivative of the ReLU units with respect to $\\psi$. <br>\n","$u = \\frac{\\partial f_A(\\psi_j)}{\\partial \\psi_j}$ <br><br>\n","\n","finally, we use the error signal from the output layer $\\delta_k^B$ along with $u$ <br>\n","to compute the error signal for the hidden layer $\\delta_j^A$. <br><br>\n","\n","The rest remains the same.\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zEi2VQjOxNX3","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - The gradient can be used to update the parameters of a neural network with stochastic gradient descent <br>\n","  ( deriving the gradient of a neural network ) <br><br>\n","\n","  - Backprop can save computation by computing gradients starting at the output of the network\n","\n","<br><br>\n","\n","\n","This was pretty detailed derivation. <br>\n","You do not actually need to remember all of these steps. <br>\n",">The goal was to give you some insight into the update for a neural network <br>\n",">and how that might change with different choices for the loss, activations, and layers. \n","\n","<br><br>\n","\n","In the next, <br>\n","we'll give you a few more tools to make your neural network implementation more effective in practice. \n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WSsGMgshzPFT"},"source":["## $\\cdot$ Optimization Strategies for NNs <br><br>\n","\n","\n","  - Undedrstand the importance of initialization for nural networks <br><br>\n","\n","  - Describe optimization techniques for training neural networks. \n","\n","\n","<br><br>\n","\n","Supervied Learning systems based on Deep Neural Networks are now the go-to solution for <br>\n","image classification, speech recognition, and natural language processing. <br><br>\n","\n","Part of the reason is due to the dramatic increases in training data and affordable computation. <br>\n","But to really take advantege of these increases in data and computation, improvements and traing were needed. <br>\n","A couple of simple optimization strategies made it much easier to train networks and helped accelerate adoption. <br><br>\n","\n","We'll talk about these today. \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AW0rvcNd20NN","colab_type":"text"},"source":["### It matters where you start <br><br>\n","\n","\n","Like many machine learning methods, neural networks are trained using an iterative process. <br>\n","This procedure, must have some startingb point. <br><br>\n","\n","The choice of this starting point can play a big role in the performance of the neural network.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1ROEVGe2nBoEln8SV_YA2ogU0CWgfNrX7\" alt=\"3-27\" width=\"500\">\n","\n","<br>\n","\n","Let's look at an example with only one weight. <br><br>\n","\n","On the y-axis, we'll show the loss function. <br>\n","On the x-axis, we'll show values of the weight. <br><br>\n","\n","As you know, <br>\n","gradient descent will iteratively move the weight towards the nearst stationary point. \n","\n","<br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1bBxHO4bjjHTMlxkHMNtD95-LkjViLWgh\" alt=\"3-28\" width=\"500\">\n","\n","<br>\n","\n","But neural network loss functions are not quite so simple. <br><br>\n","\n","If we start in this nearly flat region $w_a$, <br>\n","it can be hard to make any progress with gradient descent, since the gradient is near $0$. <br><br>\n","\n","If instead we start inside the small bouwl $w_b$ <br>\n","then we can quickly find the local optima. <br><br>\n","\n","But wouldn't it be nice if it couls started somewhere here $w_c$, <br>\n","where we can quickly find an even better optimal point ?\n","\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7s3Vk5P-8pQe","colab_type":"text"},"source":["### Weight initialization <br><br>\n","\n","\n","One simple yet effective initialization strategy <br>\n","is to randomly sample the initial weights from a normal distribution with small variance. <br>\n","$W_{init} \\sim \\mathcal{N}(0,1)$\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1tzhfY72aE4c_7PDGv7_shAzPjxwwOdS4\" alt=\"3-29\" width=\"500\">\n","\n","<br>\n","\n","This way, <br>\n","each neuron has a different output from other neuron within it's layer. <br>\n","This provides a more diverse set of potential features. <br><br>\n","\n","By keeping the variance small, <br>\n","we ensure that the output of each neuron is within the same range as it's neighbors. <br><br>\n","\n","\n","<br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1Or6s8AefZ2m77EG_lOk4tQzdsjZqf3vJ\" alt=\"3-30\" width=\"500\">\n","\n","<br>\n","\n","One downside to this strategy is that, <br>\n","as we add more inputs to a neuron, the variance of the output grows. <br>\n","We can get around this issue by scaling the variance of the weights, by $\\frac{1}{\\sqrt{n_{inputs}}}$ <br><br>\n","\n","After we have picked the starting point for our network, <br>\n","we start incrementally making small improvements to the weights using stochastic gradient descent steps. \n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"29Gd52KMtCZ4","colab_type":"text"},"source":["### Other ways to improve training <br><br>\n","\n","\n","Another way to improve training <br>\n","is to consider a more sophisticated update mechanism. <br><br>\n","\n","Two common strategies are <br>\n","  - to use the heavy-ball method (also called momentum) <br>\n","  - vector step size adaptaion\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-OdgW_N-MbK7","colab_type":"text"},"source":["### Update momentum <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1-hwyZKZZ85sw3jilo3fKtk2nQ3n0DIVQ\" alt=\"3-31 \" width=\"500\">\n","\n","<br>\n","\n","Let's talk about momentum first. <br><br>\n","\n","Imagine, <br>\n","this is the trajectory of a 2-dimensional weight vector under stochastic gradient descent. <br><br>\n","\n","Here's the stochastic gradient descent update rule <br>\n","$W_{t+1} \\quad \\leftarrow \\quad w_t - \\alpha \\nabla_W L(W_t)$ \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=153vbE40yLv-i4bGM-TmDHekn2kK6lUkD\" alt=\"3-32 \" width=\"500\">\n","\n","<br>\n","\n","And here's the update modified to include momentum. <br>\n","$W_{t+1} \\quad \\leftarrow \\quad w_t - \\alpha \\nabla_W L(W_t) + \\lambda M_t$ <br>\n","$M_{t+1} \\quad \\leftarrow \\quad \\lambda M_t - \\alpha \\nabla_W L$ <br><br> \n","\n","Notice, It is similar to <br>\n","the regular stochastic gradient descent update $\\; + \\;$ and extra term called the ' momentum ' $\\; M$. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1l4rqHtSCkyI_FxVmjRjvadp8QSv2Trit\" alt=\"3-33 \" width=\"500\">\n","\n","<br>\n","\n","The momentum term $\\quad M$<br>\n","summarizes the history of the gradients using a decaying sum of gradients with decay rate lambda $\\lambda$ <br><br>\n","\n",">If recent gradient have all been in similar directions, <br>\n",">then we gain momentum in that direction. <br>\n",">This mean we make a large step in that direction.\n","\n",">If recent updates have confilicting directions, <br>\n",">then it kills the momentum.\n","\n","<br>\n","\n","The momentum term will have little impact on the update, <br>\n","and we will make a regular gradient descent step. <br><br> \n","\n","Momentum probably accelerates learing, <br>\n","meaning it gets to a stationary point more quickly.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mycZ7IIoqIPN","colab_type":"text"},"source":["### Vector step sizes <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1X7VFTPu-kyTHdcQkwXyiy-HdKj2mdkrn\" alt=\"3-34 \" width=\"500\">\n","\n","<br>\n","\n","Another potential improvement is <br>\n","to use seperate step size for each weight in the network. <br><br>\n","\n","So far, <br>\n","we have only talked about a global \" scalar step size \". <br><br>\n","\n","This is well-known to be problematic <br>\n","becuse this can result in updates that are too big for some weights and too small for other weights ! <br><br>\n","\n","In practice, <br>\n","adapting the step sizes for each weight based on statistics about the learning process <br>\n","results in much better performance. \n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9s5Z7HA1xo2e","colab_type":"text"},"source":["### How does the update change ? <br><br>\n","\n","\n","Now, <br>\n","\" how does the update change ? \" <br><br>\n","\n","\n","The change is very simple. <br> \n","Instead of updating with a scalar $\\alpha$, <br>\n","there's a vector of step sizes indexed by $t$ to indicate that it can change on each time-step. <br><br>\n","\n","Each dimension of the gradient is scaled by it's corresponding step size instead of the global step size.\n","\n","<br><br>\n","\n","\n","There're a vraiety of methods to adapt a vector of step sizes. <br>\n","You'll get to implement one in your assignment. \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SOgknlwy7pPn","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - Better to initialize weights <br>\n","  such that the node outputs have similar variance <br><br>\n","\n","  - Momentum in weight updates can speed up neural networks optimization\n","\n","\n","<br><br>\n","\n","Today, <br>\n","we discussed ways to improve the training of neural networks. <br><br>\n","\n","Specifically, <br>\n","we discussed one strategy for initializing the weights in a neural netwrok <br>\n","and how to accelerate learning using momentum and step size adaptation.\n","\n","<br><br>\n","\n","\n",">Now, <br>\n",">you are all set \" to implement a TD agent with a Neural Network ! \"\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]}]}
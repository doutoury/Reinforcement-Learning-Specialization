{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. Week_2-4_David Silver & Week 2 Summary","provenance":[],"private_outputs":true,"collapsed_sections":["i3t87wScCa_M","3GaqiGnqpQ0s","quQkQ7urrH50","UcYctI1Qta1q"],"authorship_tag":"ABX9TyMbgzTYIJVfT3iT14ruDciZ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"i3t87wScCa_M"},"source":["## $\\cdot$ David Silver $\\quad : \\;$ Deep learning + RL = AI ? <br><br>\n","\n","\n","  - \n","\n","\n","<br><br>\n","\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SrLl-g65LbKc","colab_type":"text"},"source":["## $\\cdot$ Week 2 Summary <br><br>\n","\n","\n","This week, <br><br>\n","\n","we discussed methods for representing large impossibly continuous state space ! <br>\n","$\\rightarrow \\quad$ Ways to construct features. <br><br>\n"," \n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3GaqiGnqpQ0s","colab_type":"text"},"source":["### Week 2. Feature representation & update through Neural Network <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1dB0sv9XDmrSrobqFrKz8xfw0lmgA_xLr\" alt=\"4-01\" width=\"500\">\n","\n","<br>\n","\n","A representation is an agent's internal encoding of the state ! <br><br>\n","\n","The agent constructs features to summarize the current input. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1Ze5Ek14nsYDLkKhqd9D7VdWBnZvCBYVq\" alt=\"4-02\" width=\"500\">\n","\n","<br>\n","\n","Whenever we are talking about features and representation learnings, <br>\n","we are in the alnd of function approximation. <br><br>\n","\n","This brings us to the left side of our course-map. \n","\n","\n","<br><br><br>\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"quQkQ7urrH50","colab_type":"text"},"source":["### Coarse Coding <br><br>\n","\n","\n","First, we introduce Coarse Coding <br><br>\n","\n","Coarse Coding is related to State-Aggregation. <br>\n","It groups together neighboring states where each grouping can have an arbitrary shape. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1PoOI463kMR3VudKe_qtQka1RxZpY4tnk\" alt=\"4-03\" width=\"500\">\n","\n","<br>\n","\n","One particular example of a 2-dimensional Coarse Coding is represented by these overlapping circles. <br><br>\n","\n","Each circle is a feature that is $1$ when the state is inside the circle, and $0$ when the state is outside the circle.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UcYctI1Qta1q","colab_type":"text"},"source":["### Tile Coding <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=17ljTPPC0pl5yh7bEi4-ZYpY1GnqcQjrA\" alt=\"4-04\" width=\"500\">\n","\n","<br>\n","\n","Next, we dicussed a particular type of Coarse Coding, called Tile Coding. <br><br>\n","\n","Tile Coding generates features suing a setg of overlapping grid. <br>\n","Each grid is called a tiling. <br><br>\n","\n","The tiling itself has no overlap or space between the squares, <br>\n","Only one feature can be active at a time !\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1AkCrj67qKBdiS9KSsJjrrZGcaQdzG5sZ\" alt=\"4-05\" width=\"500\">\n","\n","<br>\n","\n","By stacking multilple offset tilings, <br>\n","we can discriminate between different states. <br><br>\n","\n","The shape, size, and number of tilings <br>\n","help us balance generalization, discrimination, and computational efficiency.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LlSRSCcsv4NL","colab_type":"text"},"source":["### Neural Networks <br><br>\n","\n","\n","We then discuss <br><br>\n","\n","  - a way to learn the representation On-line with Neural Networks. <br><br>\n","\n","  - With Coarse Coding techniques, the representation is fixed before learning ! \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1BmwC_0ByVlHIxahTk0kXNTyPPA94Yf_Y\" alt=\"4-06\" width=\"500\">\n","\n","<br>\n","\n","A feed-forward neural network uses a series of layers to produce a representation. <br><br>\n","\n","In each layer, multiple neurons received the same input and produce distinct outputs. <br>\n","These outputs are then fed to next layer and the process repeats. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1qFOWTzlDLlu2DiYwium9e5Q_6rbgmO9v\" alt=\"4-07\" width=\"500\">\n","\n","<br>\n","\n","Each neuron computes it's output by taking a weighted sum of the input, <br>\n","and pssing the sum through an activation function. \n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DFamt8fHzANG","colab_type":"text"},"source":["### Training Neural Networks <br><br>\n","\n","\n","To train a Neural Network, <br>\n","we use an iterative process called Gradient Descent. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=108OT2exCFZipDRrowHbmHr5o_Lx1IkOI\" alt=\"4-08\" width=\"500\">\n","\n","<br>\n","\n","We pass the inputs into the network to produce predictions, <br><br>\n","\n","then we compare those predictions to the outputs and compute our loss function. <br><br>\n","\n","Finally, we compute the derivative of the loss function, <br>\n","and apply our learning rule to the weights. \n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_sVu-Wmq0Uz9","colab_type":"text"},"source":["\n","Next week, <br><br>\n","\n","We'll talk about learning to maximize reward with function approximation !\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]}]}
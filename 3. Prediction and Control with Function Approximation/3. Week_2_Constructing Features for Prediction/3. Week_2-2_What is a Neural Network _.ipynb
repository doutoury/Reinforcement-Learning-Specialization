{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. Week_2-2_What is a Neural Network ?","provenance":[],"private_outputs":true,"collapsed_sections":["39ARKswlLNZg","P4LQqrAOMLJ9","wGj1fgD4OASI","wILeN3YRWOZ1","SbyhvOxWf5lk","MX4KW4v1hgGR","YjgqRyampS9X","OuLCIbRTtdxq","sajH1go8vkza","eGOAeKc6TewM"],"authorship_tag":"ABX9TyN3wB1gJvjpmB1p+hz8rrzP"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"39ARKswlLNZg","colab_type":"text"},"source":["## __2. Neural Networks__ <br><br>\n","\n","\n","  - What is a Neural Network ? <br><br>\n","\n","  - Non-Linear Approximation with Neural Networks <br><br>\n","\n","  - Deep Neural Networks <br><br>\n","\n","\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"SrLl-g65LbKc","colab_type":"text"},"source":["## $\\cdot$ What is a Neural Network ? <br><br>\n","\n","\n","  - Define a Neural Network $\\quad$ ( feed forward neural networks ) <br><br>\n","\n","  - Define an activation function <br><br>\n","\n","  - Understand how a neural network is a parameterized function\n","\n","<br><br>\n","\n","We will discuss Neural Networks, <br>\n","a flexible and powerful class of Non-linear Function Approximation ! <br><br>\n","\n","Reinforcement Learning with Neural Networks <br>\n","has been used to create a world champion go-player (바둑기사) to create Atari agetns capable of uperguman performance, and even a help with aoutonomous cars. \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"P4LQqrAOMLJ9","colab_type":"text"},"source":["### Simple Neural Network <br><br>\n","\n","\n","Let's walk through a simple neural network. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1GPfGJm_gu4H7D5Q9K2h8q24fm5UVNUsf\" alt=\"2-01\" width=\"500\">\n","\n","<br>\n","\n","Each of these circles represents nodes in the network. <br>\n","Each of the lines represents connections between nodes. <br>\n","The nodes are organized into layers. $\\quad$ ( Input layer / Hidden layer / Output layer ) \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1ZbNKUAup63wxlz-ZaZ6VjvCsL3ybgvmZ\" alt=\"2-02\" width=\"500\">\n","\n","<br>\n","\n","  1. Whe new data comes in, <br>\n","  it starts in the input layer and is sent through the connections to the next layer of nodes. <br><br>\n","\n","  2. This (hidden) layer performs some computation on the data, <br>\n","  then sends the results to the next layer. <br><br>\n","\n","  3. This process continues until the last layer produces the final output. <br><br>\n","\n","We call this a Feed Forward Neural Network <br>\n","because the date always moves forward through the layers. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1E1pSNxUdkuecE6nAuiHRVHlzo9pohsTu\" alt=\"2-03\" width=\"500\">\n","\n","<br>\n","\n","Non of the connections go back to the network. $\\quad$ ( in a feed forward neural network ) <br><br>\n","\n","If they did, then the nodes output could influence it's own input. <br>\n","Such a neural network is called a Recurrent Neural Network ! \n","\n","\n","<br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wGj1fgD4OASI","colab_type":"text"},"source":["### Neural Network mechanics <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1cTEtbPge2_CQRr2a7P4H5z46ki1xKSnv\" alt=\"2-04\" width=\"500\">\n","\n","<br>\n","\n","When data is passed through a connection, <br>\n","a weight ($w_i$) is applied. <br><br>\n","\n","The node then sums up each of it's weighted inputs, <br>\n","and apply some activation function to this sum.\n","\n","<br><br>\n","\n","\n","$$\\tanh(x)$$ | logistic function ReLU | logistic function $f(x)$ | $$\\cdots$$\n","--- | --- | --- | ---\n","<img src=\"https://drive.google.com/uc?id=1Ad-nAZfQOQA79EelEB-yntkB8e_xwPms\" alt=\"2-05\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1uHaTAiqPs0WOhDvmzD1B-1CCxuosekFs\" alt=\"2-06\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1Ipa3N27XaEg6sLcZl0-MF_sMzkak0y2N\" alt=\"2-07\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1-6M_XUQbh82qNPyN68jyxAVFZhxirLzW\" alt=\"2-08\" width=\"500\">\n","\n","<br>\n","\n","The activation function is often a non-linear transformation. <br>\n","Common activation functions include Sigmodial functions, <br>\n","  - $\\tanh(x)$ <br>\n","  - the logistic function $\\quad$ ( rectified linear units or ReLU, or even thresholding units ) <br>\n","  - ...\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wILeN3YRWOZ1","colab_type":"text"},"source":["### Neural Network implementation <br><br>\n","\n","\n","Let's look at how to write a feed foward pass of a neural network mathematically.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=11BMTtBhmucgjVv4Rq7fXZ_sew76qObRn\" alt=\"2-09\" width=\"500\">\n","\n","<br>\n","\n","At each node we have two vectors, <br>\n","  - the inputs $\\quad s = [s_1. s_2]$ <br>\n","  - the weights for each input $\\quad w_1 = [w_{1,1}, w_{2,1}]^T$ <br><br>\n","  >The first subscript refers to the node that connection comes from, <br>\n","  >The second subscript refers to the node that connection feeds into.\n","\n","<br>\n","\n","We don't product the weights with the input , <br>\n","and pass the result through the activation functio $f(SW_1)$. <br><br>\n","\n","Notice <br>\n","here that $S$ is a row vector, <br>\n","so we do not need to write the dot-product with a transpose. \n","\n","<br><br>\n","\n","\n","Each layer consists of many such nodes. <br><br>\n","\n","A neural network is just a parameterized function <br>\n","though this can be a bit hard to see by thinking of it as a collection of connected nodes. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1RP8yHPAsQwKcKHDYXGHsmiY9eXyIVLxh\" alt=\"2-10\" width=\"500\">\n","\n","<br>\n","\n","To see this more explicitely, <br>\n","let's rewrite the operations in the network using matrix vector multiplication. <br><br>\n","\n","\n","We can view the inputs to a layer as a row vector <br>\n","$\\begin{array}\n","&S &= [s_1, s_2] \\end{array}$ <br>\n","and the weights for that layer as a matrix $\\quad$ ( matrix $\\quad : \\;$ 2-dimensional array ) <br>\n","\n","$\\begin{array} \n","&W &= [W_1, W_2, W_3] \\\\ \n","&= \\begin{align} \n","\\begin{bmatrix} \n","w_{1,1} & w_{2,1} & w_{3,1} \\\\ \n","w_{1,2} & w_{2,2} & w_{3,2} \n","\\end{bmatrix} \n","\\end{align}\n","\\end{array}$ <br><br>\n","\n","We can compute the output of an entire layer using matrix multiplication $S * W$. <br>\n","We then apply our activation function to each element of the outputted vector. \n","$\\text{output} = f(SW)$ <br><br>\n","\n","If the output isn't the final layer, <br>\n","this output vector becomes the input to the next layer. <br>\n","We repeat the process !\n","\n","\n","\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SbyhvOxWf5lk","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - Neural Networks consist of a network of nodes <br>which process and pass information along <br><br>\n","\n","  - A feedforward neural network is a parameterized function \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KMQnL2aOgf6s"},"source":["## $\\cdot$ Non-linear Approximation with Neural Networks <br><br>\n","\n","\n","  - Understand how neural networks do feature construction <br><br>\n","\n","  - Understand how neural networks are non-linear functions of state\n","\n","<br><br>\n","\n","\n","Tile Coding is one way to create a fixed set of features for a prediction. <br>\n","Neural Networks provide a strategy for learning a useful of features. <br><br>\n","\n","Today, <br>\n","we'll discuss how neural networks create these features. \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MX4KW4v1hgGR","colab_type":"text"},"source":["### Non-linear representations <br><br>\n","\n","\n","Let's revisit feed-forward neural networks. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1B_wzpI2yE4M3DhRHrgLr-_biqMUj7EsD\" alt=\"2-11\" width=\"500\">\n","\n","<br>\n","\n","When we first build the neural network, <br>\n","we need to specify the initial weights $w_{init}$. <br><br>\n","\n","The way we initialize the weights is important. $\\quad$ ( more on this later ) <br>\n","For now, lat's just assume there are drawn from some random distriburion $w_{init} \\sim \\mathcal{N}(\\mu,\\sigma)$. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1z5bQnSwUg9SRTY1kaHVxTEWJm1qYPh8F\" alt=\"2-12\" width=\"500\">\n","\n","<br>\n","\n","Let's take a look at what happens to a given input vector <br>\n","as we pass it through the network. <br><br>\n","\n","Let's start by looking at only one node of the network. <br><br>\n","\n","Each of the inputs are multiplied their corresponding weights. <br><br>\n","\n","Then, the weighted sum of these inputs is passed to a non-linear activation function <br>\n","resulting in non-linear function of the inputs.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1wSEdzEdYF8zIi3ItAIcLhI0vSYfDxKTK\" alt=\"2-13\" width=\"500\">\n","\n","<br>\n","\n","This process is done again and again for each of the nodes in the layer. <br>\n","Each node has a different set of weights, so will produce a different output, <br>\n","which we call a feature ! <br><br>\n","\n","All of these new features, collectively, are considered the new representation. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1pIM_CZJhhvqvk1JxHTSITsRHxtCf40KV\" alt=\"2-14\" width=\"500\">\n","\n","<br>\n","\n","This process is actually not that different from Tile Coding, <br>\n","we pass inputs to a tile coder, and get back a new representation. <br><br>\n","\n","So in both cases, <br>\n","we construct a non-linear mapping of the inputs to produce the features. <br><br>\n","\n","In both cases, <br>\n","we take a linear combination of the representation to produce the output, <br>\n","the approximate value of the current state ! \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YjgqRyampS9X","colab_type":"text"},"source":["### Improving features with data ( contrasting with Tile Coding ) <br><br>\n","\n","\n","Let's get more insight into the neural network representation <br>\n","by contrasting with tile coding. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1bpAgUUXWPd6bMSudrVgy6kFOeQ954LhD\" alt=\"2-15\" width=\"500\">\n","\n","<br>\n","\n","Recall that when we created the tile coder we had to set several parameters, <br>\n","the size and shape of the tiles, and the number of tilings. <br>\n","These parameters were fixed before learning. <br><br>\n","\n","In a neural network, we have similar parameters <br>\n","corresponding to the number of layers, the number of nodes in each layer, and the actiavation functions. <br>\n","These are all, typically, fixed before learning ! <br><br>\n","\n","In this sense, <br>\n","both use prior knowledge to help in contrasting features. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=11zvzvrTWr3vFk5JpN5uXEGMQIgDWVNbj\" alt=\"2-16\" width=\"500\">\n","\n","<br>\n","\n","However, in addition, <br>\n","the neural network also has adjustable parameters that change the features during learning ! <br><br>\n","\n","The neural network can use data to improve the features, <br>\n","whereas the tile coder cannot incorporate new information from data.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OuLCIbRTtdxq","colab_type":"text"},"source":["### Visualizing Neural Network features <br><br>\n","\n","\n","More tile coding and neural network produce features that are non-linear in the input space. <br>\n","We can understand this better by visualizing the hidden layer of a pre-trained neural network. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1DpcclJ3mtaGxsK95uu4-5yriLElwlh16\" alt=\"2-17\" width=\"500\">\n","\n","<br>\n","\n","We trained an agent in a continuous 2-D space that contains a narrow corridor between two walls. <br>\n","Here, we've potted the receptive feild for an individual feature learned by the network. <br><br>\n","\n","The axis correspond to $x, y$ locations, <br>\n","which is the state space of the MDP. <br><br>\n","\n","Each point in the plot corresponds to the feature value for that $x, y$ state, <br>\n","darker means larger activation. If it is white, the feature is not active for that state. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1WW92oGNJW3RAX30fXrZPtyMg0aM6aRn9\" alt=\"2-18\" width=\"500\">\n","\n","<br>\n","\n","\" So what does this plot means ? \" <br><br>\n","\n","This plot shows for which state this featrue is active, meaning the feature's magnitude is above a small threshold. <br>\n","So at a hiegher level this plot shows you how this feature generalizes. <br><br>\n","\n","Updates to it's weight change the values for all these states. <br><br>\n","\n","Looking at the receptive fields for several features, <br>\n","we can see that different features generalize in different ways, and form complex non-linear shapes. <br><br>\n","\n","The activations do not have hard boundaries like in tile coding. <br>\n","These learn features can have boundaries that change smoothly. <br><br>\n","\n","This feature has different degrees of generalization depending on the state. <br><br>\n","\n","It's also kind of fun that the feature activations in this network shows us the locations of the two walls.\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sajH1go8vkza","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - Neural Network can be viewed as (a way to) constructing features <br><br>\n","\n","  - Neural Netwrok are Non-linear functions of state.\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Ufn_AiX8SjV3"},"source":["## $\\cdot$ Deep Neural Networks <br><br>\n","\n","\n","  - Understand how deep neural networks are composed of many layers <br><br>\n","\n","  - Understand that depth can facilitate learning features through composition and abstraction\n","\n","\n","<br><br>\n","\n","The choice of neural network architecture can have a big impact on performance. <br>\n","This includes the number of nodes, the activation functions, and how the nodes are arranged an connected. <br><br>\n","\n","Today, <br>\n","we will provide some intuition on the role that depth can play in the network ! \n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"eGOAeKc6TewM","colab_type":"text"},"source":["### Modular architecture <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1fdAdfFdDXxqVAx-ehRBSuOY5psFGK9BA\" alt=\"2-19\" width=\"500\">\n","\n","<br>\n","\n","We can view a neural network as being a modular system where each layer is a module ! <br><br>\n","\n","We can add and remove layers, and we can change the types of layers that we use. <br><br>\n","\n","The depth of the network is defined by the number of hidden layers in the network. \n","\n","<br><br>\n","\n","\n","### Universal approximation <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1hrwxBiRRbbxalgmIgiAc8kedLVpPWaxN\" alt=\"2-20\" width=\"500\">\n","\n","<br>\n","\n","In theory, a neural network need not be deep. <br>\n","A neural network with a single hidden layer can approximate any conrinuous function given that is sufficiently wide. <br><br>\n","\n","We call this the universal approximation property. \n","\n","<br><br>\n","\n","\n","However, <br>\n","\" practical experience and theory suggests that deep neural netwroks may make it easier to approximate complec functions ! \"\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JiTvpotwumMA","colab_type":"text"},"source":["### Compositional features <br><br>\n","\n","\n","One reason for this is that \" the depth allows composition of features ! \" <br><br>\n","\n","Composition can produce more specialized features by combining modular components. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1CUNWZnt0aYE4dQ543SYvuK0TwRUarzvf\" alt=\"2-21\" width=\"500\">\n","\n","<br>\n","\n","Consider an intuitive example. <br><br>\n","\n","  0. The input is the raw pixels of an image. <br>\n","  1. The earlier layers might learn to capture low-level features like lines at various angles. <br>\n","  2. The next layer may learn to compose those lines into various shapes. <br>\n","  3. From these, the network might ba able to detect objects (or animals) in an image. <br><br>\n","\n","This typically works better than trying to immediately infet these directly from the raw pixels ! <br><br>\n","\n","By adding more layers or more units to each layer, <br>\n","we can represent more complex functions. \n","\n","\n","<br><br>\n","\n","\n","### Levels of abstraction <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1RYyT9jCVWuiP78JeiCjb76MeWzy60KMo\" alt=\"2-22\" width=\"500\">\n","\n","<br>\n","\n","Depth can also be helpful for obtaining abstractions. <br><br>\n","\n","Deep Neural Networks compose many layers of lower-level abstractions <br>\n","with each successive layer contributing to increasingly abstract representations. <br><br>\n","\n","In the previous example, <br>\n","the network might eventually represent the concept of an owl with a single bit ( 0 or 1 ). <br><br>\n","\n","A zero means of the image does not contain an owl, <br>\n","a one means the image does contain an owl. <br><br>\n","\n","At this level, <br>\n","we no longer know the color of the owl or even what's in the background. <br>\n","\" all the extra detail on the image has been removed ! \"\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bYmGKedq0aOs","colab_type":"text"},"source":["#### Deep Neural Network designed with Bottleneck layer <br><br>\n","\n","\n","In fact, we can explicitly design the network to remove unnecesssary details from the input. <br><br>\n","\n","For example, a network can be designed with a bottleneck layer. <br><br>\n","\n","The idea is simple. <br>\n","Each successive layer contains less nodes than the layer before. <br>\n","The representation is the layer with the fewest nodes, and contains the key details needed for prediction. \n","\n","\n","<br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KNBrVH0J23-p","colab_type":"text"},"source":["#### Depth and Architecture of Neural Network <br><br>\n","\n","\n","Overall, <br>\n","Depth in a network can significantly improve our agent's ability to learn features. <br><br>\n","\n","\n","In practice, <br>\n","Picking the right architecture can be difficult, and can significantly impact performance. <br>\n",">We'll not discuss the network architectures further in this course. <br>\n",">There are many good resources on this topic. Check them out if you want to know more.\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"l_CCsqz-1nrt","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - Neural Networks can be composed of multiple layers <br><br>\n","\n","  - Depth facilitates composition and abstraction\n","\n","<br><br>\n","\n","Today, <br>\n","we discussed some of the potential benefits of Deep Neural Networks. <br><br>\n","\n","Next, <br>\n","We will dicuss learning the parameters of a Neural Network.\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]}]}
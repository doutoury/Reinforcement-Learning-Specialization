{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. Week_1-4_Linear TD","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyPANky49D3W9dX5kMcqW/zm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"8RyE3Uv51nPn","colab_type":"text"},"source":["## $\\cdot$ Course 3 Introduction <br><br>\n","\n","\n","Course 3 is really a story of something old and something new. <br>\n","We begin Reinforcemnet Learning with function approximation. <br><br>\n","\n","In a way, this is totally new because we will for the first time discuss <br>\n","Objective Functions, Gradient Descent, Parameterized Functions, and Generalization & Discrimination. <br><br>\n","\n","But, you're totally ready for it. <br>\n","The idea of exploration, exploitation , value functions, policies, and many of the elements <br>\n","you've already learned about transfer in a natural way to the function approximation setting. \n","\n","<br><br>\n","\n","\n","The begining of course represents a significant change in perspective. <br>\n","We need change our focus to learn parameterized functions. <br>\n","We will no longer assume we can store the values for all states in a table ! <br>\n","In fact, we can't even guarantee we will see the same state more than once. \n","\n","<br><br>\n","\n","Instead, we will learn functions parameterized by set of weights, <br>\n","like a Neural Network to approximate the values. <br><br>\n","\n","We won't be able to get this approximation perfect. <br>\n","That means in some states we'll have to be okay with the value function being inaccurate. <br><br>\n","\n","Despite this, the move to parameterize value function is actually overall positive because of generalization. <br>\n",">In fact, this better reflects how we learn. <br>\n",">You generalize your predictions about how much work you'll have to do in this class based on experience taking other classes. \n","\n","<br><br>\n","\n","\n","The concepts will become a bit more technical. <br>\n","We will reason about the distribution of states our agents encounter, <br>\n","and how that impacts the accuracy of the value function. <br>\n","We will compute the gradient of objectuve functions, <br>\n","and derive new learning rules. \n","\n","<br><br><br>\n","\n","\n","\n","At this point of specializaion, you've learned about many things. <br>\n","Now we're going to learn about a bunch of new things. It's easy to get lost. <br>\n","Worse, you might lose sight of which algorithms are most relevant for the given task at hand. \n","\n","<br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5LCUETuD6A5q","colab_type":"text"},"source":["### Corese Map <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1V0qLeByU1nTiSJswFYahasEfqk5GXHKg\" alt=\"0-01\">\n","\n","<br>\n","\n","To help with this, we've developed a course map. <br>\n","The idea is that you should be able to start at the top of the tree, <br>\n","and work your way down to the algorithm that best fits your problem. <br>\n",">It does not include all the algorithm in R.L., <br>\n",">just the ones we have talked about in this course.\n","\n","<br>\n","\n","The map is designed to summarize the algorithms in this course. <br>\n","It is not necessarily the best way to categorize the broader set of algorithms in R.L. <br>\n","( so use this as a mental model of the course itself ) <br>\n","\n","\n","<br><br><br>\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TgQhsuLNU_a5","colab_type":"text"},"source":["### Remind $\\quad : \\;$ What we've covered in course 1 and 2 before <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1eN94KETRZ4tOD6TzF1q6mpnzIkxAs-F0\" alt=\"0-02\">\n","\n","<br>\n","\n","We started by assuming the agent could perfectly represent the values in a table. <br>\n","We did this to focus on the funcdamental concepts of Reinforcement Learning without getting bogged down by approximation.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=19gFg-8A_z1XURywnff6FXGMmZGqld78y\" alt=\"0-03\" width=\"500\">\n","\n","<br>\n","\n","1. The first R.L. methods we discussed use a model of the world <br>\n","that was given, not learned. <br><br>\n","\n","We use Dynamic Programming methods to compute value functions and optimal policies from the model, <br>\n","without ever interacting in the world. <br><br>\n","\n","The branches of this map show all three DP algorithms that we discussed. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1-lYwlcDJ7AHM7_7LtVBxLAQozKb3f3F3\" alt=\"0-04\" width=\"500\">\n","\n","<br>\n","\n","2. in the Next course, we covered Sample-based methods. <br><br>\n","\n","We first talked about Monte-Carlo methods <br>\n","which must wait until the end of an episode to make updates. <br><br>\n","\n","We'll learn about two Monte-Carlo methods for prediction and two for control. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1Hwo7ASw5WV4hoIDWkpVsaxmjYK1-j-zN\" alt=\"0-05\" width=\"500\">\n","\n","<br>\n","\n","3. After that, we introduced you to Temporal Difference learning. <br><br>\n","\n","This family of algorithms allows the agent to make updates to the value function and policy <br>\n","on each step of the episode. <br><br>\n","\n","Here, we learned about some of the most widely used algorithms from Reinforcement Learning <br>\n","including Q-learning, SARSA, and Expected SARSA.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1Hwo7ASw5WV4hoIDWkpVsaxmjYK1-j-zN\" alt=\"0-06\" width=\"500\">\n","\n","<br>\n","\n","We finished off this course_2 by looping backup to model-base planning methods. <br><br>\n","\n","4. We studied the Dyna architecture <br>\n","in which the agent learns a model while interacting with the world. <br><br>\n","\n","Q-planning, Dyna-Q, and Dyna-Q+For course_3, <br>\n","we only nedd the left side of this map.\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cdZmM7QlY83t","colab_type":"text"},"source":["### What we're learning in course 3 later <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1VubZcb58qWivSKUCdzTDBndpowuf9sLg\" alt=\"0-07\">\n","\n","<br>\n","\n","For course_3, <br>\n","we only nedd the left side of this map. <br><br>\n","\n","\n","We will first introduce parameterized function, <br>\n","and how we can use tham to approximate value function. <br><br>\n","\n","We will talk about Generalization & Discrimination as well as particular function approximators, including Coarse Coding and Neural Networks. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1ituqcWxkExk3OZisFOxwBNW56J_yWcXm\" alt=\"0-08\" width=\"500\">\n","\n","<br>\n","\n","5. As before, we'll start with prediction and derive new Monte-Carlo & TD algorithms <br>\n","using ideas from Supervised Learning and Gradient Descent. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1CyeEPUgZLn0A9PIyYr9LdUWQ11uJ51pT\" alt=\"0-09\" width=\"500\">\n","\n","<br>\n","\n","6. Then we'll talk about control algorithms for function approximation. <br>\n","This includes SARSA, Expected SARSA and Q-learning.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1pFpIq-cymC7eDast__wVJ6K2IeMTBs3s\" alt=\"0-10\" width=\"500\">\n","\n","<br>\n","\n","7. After that, we will totally blow your mind <br>\n","with a new way of formulating continuing control problems <br>\n","called Average Reward. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1tAWBP4OA5flFDXcEpZFcd-IDajWLXbYf\" alt=\"0-11\" width=\"500\">\n","\n","<br>\n","\n","8. We'll finish out course 3 with Parameterized Policies. <br><br>\n","\n","In a nutshell, we can parameterize policies just like we parameterize the value function. <br>\n","We will discuss how to do this in the Average Reward setting. \n","\n","<br><br>\n","\n","\n","By the end of this course 3, <br>\n","you will know much of what you need to scale R.L. to real problems. <br><br>\n","\n",">Then, <br>\n",">you'll get to use all this newfound understanding to implement a complete R.L. system in course 4.\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"39ARKswlLNZg","colab_type":"text"},"source":["## __4. Linear TD__ <br><br>\n","\n","\n","  - The Linear TD Update <br><br>\n","\n","  - The True Objective for TD <br><br>\n","\n","  - Week 1 Summary\n","\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"SrLl-g65LbKc","colab_type":"text"},"source":["## $\\cdot$ The Linear TD Update <br><br>\n","\n","\n","  - Derive the TD update with Linear function approximation <br><br>\n","\n","  - Understand that Tabular TD(0) is a special case of Linear Semi-gradient TD(0) <br><br>\n","\n","  - Understand why we care about Linear TD as a special case\n","\n","<br><br>\n","\n","\n","In principle, <br>\n","we can use any kind of function approximation for Reinforcement Learning. <br><br>\n","\n","Linear function approximation is an important special case ! <br>\n","It is simple enough that we can understand it well, <br>\n","but it is still quite powerful in general. <br>\n","\n",">In fact, <br>\n",">Linear function approximation with TD has been used to build Atari game agents <br>\n",">that exceed human performance in many games !\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DDf-knGM_lrm","colab_type":"text"},"source":["### TD update with Linear Function Approximation <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1Um2VasNutyPTDpw4jMkXtmdEwWJ_504T\" alt=\"4-01\" width=\"500\">\n","\n","<br>\n","\n","\n","Recall the update for Semi-gradient TD. <br><br>\n","\n","It adjust the weights $\\quad W$ <br>\n","and the direction of the TD error $\\quad \\delta_t$ <br>\n","times the gradient of the approximate value function with respect to the weights $\\quad \\nabla \\hat{v}(S_t,W)$. <br>\n","$W \\quad \\leftarrow \\quad W + \\alpha \\delta_t \\nabla \\hat{v}(S_t,W)$\n","\n","<br><br>\n","\n","\n","In the Linear case, <br>\n","$\\hat{v}(S_t,W) \\doteq W^T X(S_t)$ <br><br>\n","\n","the gradient of the approximate vlaue for a state is just the feature vector for that state. <br>\n","$\\nabla \\hat{v}(S_t,W) = X(S_t)$ <br><br>\n","\n","So the update for the Semi-gradient TD would look like this. <br>\n","$W \\quad \\leftarrow \\quad W + \\alpha \\delta_t X(S_t)$ \n","\n","<br><br>\n","\n","\n","The weight $W$ is updated <br>\n","in the direction of the feature vector times the TD error. $\\quad \\sim \\delta_t * X(S_t)$ <br><br>\n","\n","If a feature is large, <br>\n","then the corresponding weight can have a large impact on the prediction. <br>\n","If a feature is zero, <br>\n","then that weight has no impact on the prediction, and the gradient is therefore zero. \n","\n","<br><br>\n","\n","\n",">The fixed basis given by the expert design features has a large impact on the update. <br>\n",">If well-designed, we can get effective value function approximation with a simple update. \n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HtEUj8kTDCki","colab_type":"text"},"source":["### Tabular TD is a special case of linear TD <br><br>\n","\n","\n","We already saw how Linear value function approximation is <br>\n","a strict generalization of Tabular Value function approximation. <br><br>\n","\n","We can show that Linear TD is a strict generalization of both Tabular TD and TD with State Aggregation. <br><br>\n","\n","Let's take a closer look at the algorithm to see this explicitly for Tabular TD. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1ortSYg1Y6Xar8ckaDVbDxBRrgyiPsIIh\" alt=\"4-02\" width=\"500\">\n","\n","<br>\n","\n","Imagine we have a Tabular State representation. <br><br>\n","\n","Only one feature will be equal to one corresponding to the current state and the rest will be zero. <br>\n","$X(S_i) = \\big[ [0],[0],...,[0],[1],[0],...,[0] \\big]$ <br><br>\n","\n","The value approximation for a state is equal to the weight associated with the current state. <br>\n","$\\hat{v}(S_i,W) = w_i$ <br><br>\n","\n","We can therefore think of the weight vector as just a table of values, one for each state. (???) <br><br>\n","\n","\n","Here's the update for Semi-gradient TD with these Tabular features. <br>\n","$W \\quad \\leftarrow \\quad W + \\alpha \\big[ R_{t+1} + \\gamma \\hat{v}(S_{t+1},W) - \\hat{v}(S_t,W) \\big] \\color{brown}{X(S_t)}$ <br><br>\n","\n","In the update, the feature vector $X(S_t)$ selects a single weight $w_i$ associated with the current state $s_i$. <br>\n","This weight is just the value estimate for the state. $quad \\hat{v}(s_i,W) = w_i$ <br>\n","So this update corresponds to the Tabular TD update we saw in a previous course. <br>\n","$\\color{brown}{w_i} \\quad \\leftarrow \\quad \\color{brown}{w_i} + \\alpha \\big[ R_{t+1} + \\gamma \\hat{v}(S_{t+1},W) - \\hat{v}(S_t,W) \\big] \\color{brown}{1}$ \n","\n","<br><br>\n","\n","\n","We can use the same analysis <br>\n","to show that TD with State Aggregation is also a special case of Linear TD. \n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4q3eTttsb-XO","colab_type":"text"},"source":["### The utility of Linear function approximation <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1ZX8-_CnPZgxztPpdparVufbfKRZ-N-mv\" alt=\"4-03\" width=\"500\">\n","\n","<br>\n","\n","\" Why do we care so much about the special case of Linear function approximation ? \" \n","\n","<br><br>\n","\n","\n","  - Linear methods are simpler to understand and analyze mathmatically <br><br>\n","\n","  Much of the theory of TD learing is for the Linear setting. <br>\n","  ( we'll discuss this more in the next video ) \n","\n","<br><br>\n","\n","  \n","  - With good fewtures, Linear methods can learn quickly and achieve good prediction accury <br><br>\n","\n","  In some applications, you may have access to expert knowledge to help you design good features. <br>\n","  ( Perhaps, you are the domain expert or perhaps you can talk to someone who knows a lot about your application ) <br><br>\n","  \n","  If we can design these features well, then we can expect Linear methods to learn quickly, and achieve good prediction accuracy. <br>\n","  Feature design provids one way to incorporate domain knowledge that can result in good performance in parctice. \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5YAFBCmzeIOB","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - The Linear TD update <br><br>\n","\n","  - How Tabular TD is a special case of Linear Semi-gradient TD <br><br>\n","\n","  - Why linear function approximation might be useful\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Zezbk8BTenS2"},"source":["## $\\cdot$ The True Objective for TD <br><br>\n","\n","\n","  - Understand the fixed point of linear TD learning <br><br>\n","\n","  - Describe a theoretical guarantee on the Mean Squared Value Error at the TD fixed point\n","\n","\n","<br><br>\n","\n","\n","In a previous lecture, <br>\n","we introduced Semi-gradient TD as approximate gradient descent in the Mean Squared Value Error. <br><br>\n","\n","But this is a simplfication of a more nuanced story. <br>\n","TD does not precisely optimize this objective, <br><br>\n","\n","instead, <br>\n","there's a different objective we can consider for Semi-gradient TD with Linear funciton approcimation. <br><br>\n","\n","In fact, Linear TD probably converges to a well understood approximation <br>\n","$\\rightarrow \\quad$ called the TD fixed point. \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cXOsGBYxfPCF","colab_type":"text"},"source":["### The Expected TD Update <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1xH4ZOOfO0OnyUyaPuGdOqPhHLn8_Ao2a\" alt=\"4-04\" width=\"500\">\n","\n","\n","Let's take a closer look at the TD update with Linaer function approximation. <br><br>\n","\n","The value of a state is the inner product of the state features $X(s)$ and the learn weights vector $W^T$. <br>\n","$\\hat{v}(s,W) \\doteq W^T X(s)$ <br><br>\n","\n","To simplify the notation, <br>\n","We'll use $X_t$ to mean the features associated with the state $X(S_t)$. <br>\n","$X(S_t) \\quad \\rightarrow \\quad X_t$ \n","\n","<br><br>\n","\n","\n","We can then expend the TD update like second-line terms. <br>\n","We can then rewrite this using a bit of linear algebra like third-line tems. <br>\n",">First, we'll pull $X_t$ into the bracket. We then use the fact that taking the transpose of a scalar leaves it unchanged.\n","\n","<br><br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1ZjFJWLAIXVPDLxIfknIlQ1m7eR5jB8CJ\" alt=\"4-05\" width=\"500\">\n","\n","<br>\n","\n","Now, let's think about what this update looks like in expectation $\\mathbb{E}$ ! <br><br>\n","\n","Understanding the expected update is key for proving convergence. <br>\n","The TD update can be written as [ the expected update + a noise term ] <br>\n","So the TD update is largely dominated by the behavior of the expected update. <br><br>\n","\n","The expected update characterizes the expected change in the weight <br>\n","from one time step to the next. <br><br>\n","\n","The expected TD update can be written as [ a vector $b$ - a matrix $A$ * the current weights $W$ ]. <br>\n","$\\mathbb{E} = \\alpha (b - AW_t)$ <br><br>\n","\n","The matrix $A$ is defined in temrs of an expectation $\\mathbb{E}$ over the features $X(S)$, <br>\n","$A \\doteq \\mathbb{E}[X_t(X_t - \\gamma X_{t+1})^T]$ <br><br>\n","\n","The vector $b$ is defined in terms of the features $X(S)$ and the reward $R$. <br>\n","$b \\doteq \\mathbb{E}[R_{t+1}X_t]$ \n","\n","\n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mRoXYQeIhZN-","colab_type":"text"},"source":["### The TD Fixed Point <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1vZZPXvScbXS27_DTQ2lf4nx4KW-9CqIU\" alt=\"4-06\" width=\"500\">\n","\n","\n","The weights are said to converger, <br>\n","when this expexted TD update is zero ! <br><br>\n","\n","We call this point $W_{TD}$. <br><br>\n","\n","If $A$ is invertible, We can express this as <br>\n","$W_{TD} = A^{-1} b$ <br><br>\n","\n","More generally, <br>\n","$W_{TD}$ is a solution to this linear system. <br><br>\n","\n","\" We call this solution the TD Fixed Point. \"\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1GOYA9zflEq_3CiJ2geI4xY4HVWul9zBl\" alt=\"4-07\" width=\"500\">\n","\n","<br>\n","\n","It can be proven that linear TD converges to this point. <br><br>\n","\n","In fact, it can be showns that TD minimizes an objective that is based on this $A$ and $B$. <br>\n","$W_{TD} \\quad \\text{minimizes} \\quad (b-AW)^T(b-AW)$ <br><br>\n","\n","This objective extends the connection between TD and Bellman equations, <br>\n","to the function approximation setting. <br><br>\n","\n","Recall that in the Tabular setting, <br>\n","we describe TD as a sample based method for solving the Bellman equation. <br><br>\n","\n","Linear TD similarly approximates the solution to the Bellman equation, <br>\n","minimizing what is called the projected Bellman error. <br>\n",">The details of this objective are beyound the scope of this course, <br>\n",">but you can refer to the course textbook for more detail.\n","\n","<br><br>\n","\n","The key takeaway is that <br>\n","even though TD does not converge to the minimum of the Mean Squared Value Error, <br>\n","It does converge to the minimum of a principled objective, based on Bellman equations. \n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BaOjwQaHogyV","colab_type":"text"},"source":["### Relating the TD Fixed Point and the Minimum of the Value Error <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1kuVv5RbJn3zYRxc_AdbTu_9kZfTspWyE\" alt=\"4-08\" width=\"500\">\n","\n","<br>\n","\n","Nonetheless, we do still want to understand <br>\n","the relationship between the solution found by TD and the minimum value error solution. <br><br>\n","\n","We can formally characterize this relationship with this equation. <br><br>\n","\n","if $\\gamma$ is close to $1$, <br>\n","The difference between the TD fixed point and the minimum value error solution can be large.\n","If $\\gamma$ is very close to $0$, <br>\n","the TD fixed point is very close to the minimum value error solution. \n","\n","<br><br>\n","\n","This bound also depends on the quality of the features. <br><br>\n","\n","If the features are limited, <br>\n","both the minimum Mean Squared Value Error and the value of error (of?) the TD fixed point may be large. <br><br>\n","\n","If we can perfectly represent the value function, ( then regardless of $\\gamma$ ) <br>\n","The TD fixed point is equivalent to the minimum value error solution. <br>\n","( This is because both the left and right hand sides would be $0$ ) \n","\n","<br><br>\n","\n","\n","So in general, <br>\n","\" Why isn't the TD fixed point equal to the minimum value error solution ? \" <br><br>\n","\n","This is because of bootstrapping under function approximation. <br><br>\n","\n","If our estimate of the next state is persistently inaccurate due to function approximation, <br>\n","then TD forever update towards an inaccurate target. <br><br>\n","\n","If our function approximator is very good, then our estimate of the next state will become very accurate.<br>\n","So bootstrapping off this estimate is not problematic, and the error for the TD solution is close to the minimum value error.\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xhwg05spsWy8","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - (Why) Linear semi-gradient TD is guaranteed to converge to a fixed point, <br>\n","  called TD fixed point <br><br>\n","\n","  - (How) the TD fixed point relates to the minimum mean squared value error \n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kj7exBA1tCld"},"source":["## $\\cdot$ Week 1 Summary <br><br>\n","\n","\n","This week, <br>\n","we learned a lot about how to extend Reinforcement Learning <br>\n","from the Tabular case to function approximation. <br><br>\n","\n","In many real-world problems, <br>\n","it is not practical to enumerate every possible state in a table. <br><br>\n","\n","So moving to function approximation is a big step <br>\n","in making Reinforcement Learning more broadly applicable.\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"COKiTmQpt8rC","colab_type":"text"},"source":["### __MAP__ <br><br>\n","\n","\n","We've covered a lot of material already, and it's easy to get lost in all the different concepts. <br>\n","So let's step back, and see how this week's topics fit into the bigger picture. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1PofjkloGt-nwgND59kdoFt87dyS7EEKq\" alt=\"4-09\">\n","\n","\n","This map shows all the different algorithms we've covered in this SPECIALIZATION, <br>\n","and how they relate to each other. <br>\n",">Remember, <br>\n",">the map only shows the algorithms we present in the SPECIALIZATION.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=12nz_dXvTeR8UURnoUSlCa4FbYGkSoA81\" alt=\"4-10\">\n","\n","\n","The main conceptual change this week was <br>\n","moving to the framework of parameterized function approximation. <br><br>\n","\n","This means, <br>\n","we are no longer using a table to store the value of each state. <br>\n","This puts us on the left side of the map. <br>\n",">Our discussion of average reward will come later <br>\n",">For now, let's focus over here.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1JqETV-05uPP7KsdCOQcLjz2qYJjBYiFZ\" alt=\"4-11\">\n","\n","\n","This week, we focus on the policy evaluation problem. ( in the red box ) <br>\n","We cover two algorithms. <br>\n","  - Gradient Monte-Carlo <br>\n","  - Semi-Gradient TD <br><br>\n","\n","Graditent Monte-Carlo makes updates at the end of each episode. <br><br>\n","\n","(Semi-Gradient) TD bootstrps using the value estimate of the next time step. <br>\n","So it does not need to wait until the end of an episode to learn. \n","\n","<br><br><br>\n","\n","\n","\n",">Now that we've got the big picture. <br>\n",">Let's do a quick recap of some of the details.\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WcREbQSdyNTV","colab_type":"text"},"source":["### Parameterized Value Function Approximatoin <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1_rCljjXDdd5FXMjA69y2B38l5hJQUflb\" alt=\"4-12\" width=\"500\">\n","\n","<br>\n","\n","We described a parameterized value function <br>\n","as a mapping from state to real numbers. <br><br>\n","\n","The output is controlled by a vector of real-valued weights $W$. <br>\n","We use the notation $\\\\hat{v}(S,W)$ to represent the approximate value function. <br>\n","This indicates that the value estimate is determined by the current state and the learn weight vector. <br><br>\n","\n","One example of a parameterized value function is a linear function of fixed or expert design features. <br>\n","$\\hat{v} \\quad \\doteq \\quad <W,X(S)>$ <br>\n","Another example is a Neural Network. <br>\n","$X(S) \\quad - NN \\rightarrow \\quad \\hat{v}(S,W)$\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2m5qLBoNTZPv","colab_type":"text"},"source":["### Generalization and Discrimination <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1Q8CZqy_-dJ2_eF30hTPCL-y7RyvAuQWu\" alt=\"4-13\" width=\"500\">\n","\n","<br><br>\n","\n","\n","Two key properties of function approximation. <br>\n","  - Generalization <br>\n","  - Discrimination <br><br>\n","\n","\n","Updating the value of one state can improve the value estimates of other states. <br>\n","This Generalization can make learning faster. <br><br>\n","\n","We also want our value function to assign different values when states are very different. <br>\n","We call this Discrimination. <br>\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8IElOWsvUiug","colab_type":"text"},"source":["### Mean Sauared Value Error Objective <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=19Z4Q87kL-E8J_wLvAbWdem0z3D5zUYMN\" alt=\"4-14\" width=\"500\">\n","\n","<br>\n","\n","How to use ideas from Supervised Learning to learn approximate value functions. <br><br>\n","\n","We cannot quarantee perfect approximation for every state's value. <br>\n","So we need to define an objective <br>\n","Objective $\\quad : \\;$ A measure of the distance between our approximation and the true values \n","\n","<br><br>\n","\n","\n","We use the Mean Squared Value Error obejective. <br><br>\n","\n","The sum of the squared error <br>\n","between the value of each state and our approximate value weighted by the visitation frequency $mu$.\n","\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FSctWfcuWwvY","colab_type":"text"},"source":["### Optimizing the $\\overline{VE}$ <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1J1JT7Wye2J2lBQGZxiWxYnHPNZj4KyQq\" alt=\"4-15\" width=\"500\">\n","\n","<br>\n","\n","How Stochastic Gradient Descent can be used to optimize their objective. <br>\n","$W \\quad \\leftarrow \\quad W - \\alpha [v_{\\pi}(S_t) - \\hat{v}(S_t,W)] \\nabla \\hat{v}(S_t,W)$ <br><br>\n","\n","The Gradient Monte-Carlo algorithm performs stochastic gradient descent using sampled returns to update the weights. <br>\n","$W \\quad \\leftarrow \\quad W - \\alpha [\\quad \\color{brown}{G_t} \\;\\;\\; - \\hat{v}(S_t,W)] \\nabla \\hat{v}(S_t,W)$ <br><br>\n","\n","This means, <br>\n","it can learn approximate value from the experience generated by an agent interacting with the world. \n","\n","\n","\n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vBWfJCOpYkNx","colab_type":"text"},"source":["### Semi-Gradient TD(0) <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1GDV56dGTImNxlofKXz2tGw0cu5YWM_5w\" alt=\"4-15\" width=\"500\">\n","\n","<br>\n","\n","Semi-Gradient TD as an approximatoin to stochastic gradient descent. <br><br>\n","\n","Semi-Gradient TD takes advantage of bootstrapping, <br>\n","and in practive may converge much faster than Monte-Carlo. \n","\n","<br><br>\n","\n","\n","Semi-Gradient, part of the name remind us that it's not a gradient descent algorithm !\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JnW3mdqlZ1uO","colab_type":"text"},"source":["### Linear Semi-Gradient TD(0) <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1UMWztJnBNAvUFK9tHRaMjAroO1vw51LA\" alt=\"4-16\" width=\"500\">\n","\n","<br>\n","\n","Finally, <br>\n","Linear Semi-Gradient TD algorithm $\\quad$ (or TD with Linear function approximation )\n","\n","<br><br>\n","\n","\n","Liear TD probably converges to a well-understood point. <br>\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"eZ2mZDXAbowY","colab_type":"text"},"source":["### Fin. <br><br>\n","\n","\n","We now have a good foundation to understand Reinforcement Learning with parameterized functions. <br><br>\n","\n","Next, <br>\n","we will discuss different ways to construct state and state action features. \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]}]}
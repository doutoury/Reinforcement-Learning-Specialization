{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. Week_1-3_The Objective for TD","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyN786CuTmoslE7nZj3dejmq"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"39ARKswlLNZg","colab_type":"text"},"source":["## __3. The Objective for TD__ <br><br>\n","\n","\n","  - Semi-Gradient TD for Policy Evaluation <br><br>\n","\n","  - Comparing TD and Monte-Carlo with State Aggregation <br><br>\n","\n","  - Donia Precup : Building Knowledge for A.I. Agents with Reinforcement Learning\n","\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"SrLl-g65LbKc","colab_type":"text"},"source":["## $\\cdot$ Semi-Gradient TD for Policy Evaluation <br><br>\n","\n","\n","  - Understand the TD update for function approximation <br><br>\n","\n","  - Outline the Semi-Gradient TD(0) algorithm for value estimation\n","\n","<br><br>\n","\n","\n","TD learning is built on the idea that <br>\n","the agent can use it's own estimate of the value function to update it's predictions. <br><br>\n","\n","We've seen TD learning in the Tabular case. ( before ) <br>\n","Today we will deacribe how it works with function approximation. \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"g_6ajeUKJz3M","colab_type":"text"},"source":["### Recall $\\quad : \\;$ Gradient Monte-Carlo <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1xNlm2kMxaUBekzMBEM8JHrOn3trUTdnU\" alt=\"3-01\" width=\"500\">\n","\n","<br>\n","\n","Recall the Gradient Monte-Carlo update equation. <br><br>\n","\n","It updates our current value estimate to be closer to a sample of the reaturn $G_t$. <br>\n","But we can consider using other targets instead of the return $G_t$ ! <br>\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pbJAWkMsKlBP","colab_type":"text"},"source":["### The TD update for Function Approximation <br><br>\n","\n","\n","\n","In fact, <br>\n","__\" we can replace the return $G_t$ in this update with any estimate of the value ! \"__ <br><br>\n","\n","Let's call this estimate $U_t$. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1e85IfCJyktnJe8-FzudJOTM1zedFe29q\" alt=\"3-02\" width=\"500\">\n","\n","<br>\n","\n","If $U_t$ is an __unbiased__ estimate of the true value, <br>\n","then our function approximator will converge to a local optimum under the appropriate conditions. <br>\n","( This was the case for the return ??? )\\\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=18KnAjZ6De6wZrh0OX_S7Hxi3JlNA0v-8\" alt=\"3-03\" width=\"500\">\n","\n","<br>\n","\n","But we can also replace $U_t$ with a __bootstrap target__, <br>\n","such as the __one step TD target__. <br><br>\n","\n","This is still an estimate of the return, <br>\n","but in this case, the estimate is __biased !__ <br><br>\n","\n","The TD target uses our __current value estimate $\\quad \\hat{v}(S_{t+1},W)$__, <br>\n","which will likely not equal the true value function. <br><br>\n","\n","Because of this, we cannot guarantee this algorithm will converge to a loval minimum of the value error. \n","\n","<br><br>\n","\n","The upside is that <br>\n","the TD target often has lower variance than tha sample of the return ! <br>\n","This means TD will tend to converge in fewer updates. \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dOTD1GIGM2EA","colab_type":"text"},"source":["### TD is a Semi-Gradient method <br><br>\n","\n","\n","The TD update is not actually a Stochastic Gradient Descent updats. <br>\n","To see why we take the gradient of the error with respect to the weights for one sample.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1Bv6Zmx4hsAZKVSf5d043PasU96YZhQuK\" alt=\"3-04\" width=\"500\">\n","\n","<br><br>\n","\n","\n","Remember $U_t$ is equal to the TD target. <br>\n","$U_t \\doteq R_{t+1} + \\gamma \\hat{v}(S_{t+1},W)$ \n","\n","<br><br>\n","\n","\n","Using the chain rule, <br>\n","we get this expanded expression for the gradient of the Squared Error for one sample. <br>\n","$\\nabla \\frac{1}{2} [U_t - \\hat{v}(S_t,W)]^2 = (U_t - \\hat{v}(S_t,W))(\\nabla U_t - \\nabla \\hat{v}(S_t,W))$ <br><br>\n","\n","But, This doesn't look like the TD update ! <br>\n","$\\begin{align} \\qquad \\qquad \\qquad \\qquad \\qquad (U_t - \\hat{v}(S_t,W))(\\nabla U_t - \\nabla \\hat{v}(S_t,W)) \\not= &-(U_t - \\hat{v}(S_t,W)) \\nabla \\hat{v}(S_t,W) \\\\\n","&\\qquad \\qquad \\qquad \\uparrow \\\\\n","&\\qquad \\quad \\text{ the TD update } \\end{align}$ <br><br>\n","\n","These two expressions would only be equal if the gradient of $U_t$ is zero. $\\quad (\\nabla U_t = 0)$ <br>\n","This is not the case for TD ! \n","\n","<br><br>\n","\n","\n","In TD (For TD), <br>\n","the target contains an estimate of the value , which depends on the weights. $\\quad \\hat{v}(S+{t+1},W)$ (???) <br>\n","This means the gradient of $U_t$ is not $0$ as shown here ! $\\quad \\gamma \\nabla \\hat{v}(S_{t+1},W)$ <br><br>\n","\n","Therefore, TD is not performing Gradient Descent on the Squared Error. <br>\n","We call it a __Semi-gradient method__. \n","\n","<br><br>\n","\n","\n","Despite this, <br>\n","TD converges in many of the cases we care about. <br>\n","We will disuss TD's convergence properties in an upcoming video. \n","\n","\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0gjY2CQFPsDo","colab_type":"text"},"source":["### Algorithm <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1k-dSOCIVeznXRduoi05R89oW22hKlTdj\" alt=\"3-05\" width=\"500\">\n","\n","<br>\n","\n","Here is the pseudo-code for Semi-gradient TD. <br><br>\n","\n","\n","It's actually very similar to the TD argument for the Tabular setting. <br>\n","This algorithm does not have to wait until the end of an episode to make updates ! <br>\n","TD performs an update on each step ! $\\quad$ ( unlike the Gradient Monte-Carlo ) \n","\n","<br><br>\n","\n","On each step of the episode, the agent selects an action $A$, in state $S$. <br>\n","$\\text{Choose } A \\sim \\pi(\\cdot|S) \\\\ \\text{Take action } A, \\; \\text{Observe } R, S'$ <br><br>\n","\n","We use the resulting rewars in next state to compute the TD target, and update the weights immediately. <br>\n","$W \\quad \\leftarrow \\quad W + \\alpha \\big[ R + \\gamma \\; \\hat{v}(S',W) - \\hat{v}(S,W) \\big] \\; \\nabla \\hat{v}(S,W)$ <br><br>\n","\n","<br><br>\n","\n","\n","We conticue in this way until we reach the terminal state, <br>\n","$S \\quad \\leftarrow \\quad S' \\qquad \\text{until S is terminal}$ <br>\n","which is defined to have value $0$. <br>\n","$\\hat{v}(\\text{terminal},\\;\\cdot\\;)=0$ <br>\n","Then, we start a new episode. <br>\n","( Loop a next episode ) \n","\n","<br><br>\n","\n","\n","It's really not that different from Tabular TD.\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"opKxqaGCP5PH","colab_type":"text"},"source":["### Summmry <br><br>\n","\n","\n","  - You should now understand the Semi-gradient TD algorithm\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TSTqRiR5zPNn"},"source":["## $\\cdot$ Comparing TD and Monte-Carlo with State Aggregation <br><br>\n","\n","\n","  - Understand that TD converges to a biased value estimate <br><br>\n","\n","  - Understand that TD can learn faster than Gradient Monte-Carlo\n","\n","\n","<br><br>\n","\n","We just talked about Monte-Carlo with function approximation and TD with function approximation. <br>\n","\" How do these algorithms differ ? \" <br><br>\n","\n","We'll going to talk <br>\n","more about the bias in the TD update <br>\n","and compare it to Monte-Carlo in a small experient. \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aouDntw_0Svj","colab_type":"text"},"source":["### Gradient Monte-Carlo will converge to a Local Minimum of the Mean Squared Value Error <br><br>\n","\n","\n","Under reasonable assumptions, <br>\n","Gradient Monte-Carlo will approach a local minimum of the Mean Squared Value Error <br>\n","( with more an more samples )\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1dcmSyac3oFvRVDjbarFdUuzbmLk5Nl7W\" alt=\"3-06\" width=\"500\">\n","\n","<br><br>\n","\n","\n","This is because it uses an unbiased estimate $G_t$ of the gradient of the value error ! <br><br>\n","\n","In theory, <br>\n","we need to run the algorithm for a very long time <br>\n","and decay the step size parameters to obtain this convergence. <br><br>\n","\n","In practice, <br>\n","we use a constant step size. <br>\n","So the algorithm oscillates around a local minimum. \n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2p1eWUrp2EDg","colab_type":"text"},"source":["### Semi-Gradient TD will not necessarily converge to a local minimum of the Mean Squared Value Error <br><br>\n","\n","\n","On the other hand, <br>\n","the TD target depends on our estimate of the value in the next state $R_{t+1} + \\gamma \\hat{v}(S_{t+1},W)$. <br><br>\n","\n","This means our update could be biased, <br>\n","because the estimate in our target may not be accurate. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1O9Wf2ofamzo1t-iE4Z5ljSy5MqPlr442\" alt=\"3-07\" width=\"500\">\n","\n","<br><br>\n","\n","\n","Since our value approximation will never be perfect even in the limit, <br>\n","the target may remain biased. <br><br>\n","\n","We cannot guarantee that Semi-Gradient TD will converge to a local minimum of the Mean Squared Value Error. \n","\n","<br><br>\n","\n","\n","Of course, <br>\n","this bias will reduce as our estimates improved. <br>\n","\" But how do all these theoretical concerns impact performance in practice ? \"\n","\n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4Vkkv1T14dg5","colab_type":"text"},"source":["### Long run performance of TD on random walk <br><br>\n","\n","Let's return to the 1000 state Random walk example, <br>\n","to see the impact of bias in the TD update. <br><br>\n","\n","This time, <br>\n","we'll learn approximate values using Semi-gradient TD. $\\quad$ ( instead of Monte-Carlo )\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1pa9qdJROgGKUfxgENJWYVo6_j0p9RKF-\" alt=\"3-08\" width=\"500\">\n","\n","<br><br>\n","\n","\n","After running TD for a long time, <br>\n","such that our value estimates have stopped changing. This is what we get. <br><br>\n","\n","The value estimates do not align well with the true values for many states. <br>\n","TD's estimates are not as accurate as the estimates done by Monte-Carlo. <br><br>\n","\n","In this domain, ( the State Aggregation ), <br>\n","Monte-Carlo has an advantage in terms of long-run performance. \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jqdEgmIkxd-n","colab_type":"text"},"source":["### Which algorithm learns faster ? <br><br>\n","\n","\n","\" What about the speed of learning ? \" <br>\n","Which of TD and Monte-Carlo makes best use of limited samples ? <br><br>\n","\n","Let's run a different experiment to get to the bottm of this.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1SRD4xXxmrB09urMjoJc7XrUa1PBn3N3q\" alt=\"3-09\" width=\"500\">\n","\n","<br>\n","\n","To focus on early learning, <br>\n","we only run 30 episodes. $\\quad$ (in the last experiment, we used 1000 episodes ) \n","\n","<br><br>\n","\n","The performance of both algorithms depends on how we set the step size parameter $\\alpha$. <br>\n","The two algorithms may require very different values of $\\alpha$ <br><br>\n","\n","To do a fair comparison, <br>\n","we tested each algorithm with 100 evenly spaced values of $\\alpha$ between 0 and 1. <br>\n","We take the best performing $\\alpha$ for each algorithm. ( compare the performance ) <br><br>\n","\n","The best $\\alpha$ was defined as the one that resulted in the lowest value error after 30 episodes of training. <br>\n","For this task, <br>\n","the best $\\alpha$ we found for TD was $0.22$. <br>\n","the best $\\alpha$ we found for Monte-Carlo was $0.01$. \n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"i2g2-Yn_0jee","colab_type":"text"},"source":["### TD converges faster <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1ztXxbb6brUlxK1YyJ_ZYqxPhGrgve1zi\" alt=\"3-10\" width=\"500\">\n","\n","<br>\n","\n","Plot the performance of each algorithm <br><br>\n","\n","The y-axis shows the Root Mean Squared Value Error. ( for the 1000 states averaged over 100 runs ) <br>\n","The x-axis shows the episode number. <br><br>\n","\n","We expect a value error for each algorithm <br>\n","to start high and progressively decrease with more and more episodes. \n","\n","<br><br>\n","\n","\n","Result <br><br>\n","\n","We see the TD reaches a lower error faster ! <br>\n",">This is not a one-off result. <br>\n",">TD often learns faster the Monte-Carlo.\n","\n","<br>\n","\n","This is because <br>\n","TD can learn during the episode, and <br>\n","TD has lower variance updates. \n","\n","<br><br>\n","\n","Monte-Carlo is better on long-run performance, <br>\n","but it's not always the main concern. \n","\n","<br><br><br>\n","\n","\n","\n","We can never run our experiments to achieve asymtotic performance. <br>\n","Early lerning is perhaps more important in practice ! \n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-1T8RdQe2lD0","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - The TD update for function approximation can be biased <br><br>\n","\n","  - We often prefer TD learning over Monte-Carlo anyway because it can converge quickly\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. Week_1-2_The Objective for On-policy Prediction","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyNiCp+U6SPBwoD9SNgTZTSb"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"39ARKswlLNZg","colab_type":"text"},"source":["## __2. The Objective for On-policy Prediction__ <br><br>\n","\n","\n","  - The Value Error Objective <br><br>\n","\n","  - Introducing Gradient Descent <br><br>\n","\n","  - Gradient Monte for Policy Evaluation <br><br>\n","\n","  - State Aggregation with Monte-Carlo\n","\n","\n","\n","\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"SrLl-g65LbKc","colab_type":"text"},"source":["## $\\cdot$ The Value Error Objective <br><br>\n","\n","\n","  - Understand the mean squared value error objective for Policy Evaluation <br><br>\n","\n","  - Explain the role of the state distribution in the objective \n","\n","\n","<br><br>\n","\n","\n","We discussed how we can frame value estimation as a Supervised Learning problem. <br>\n","In Supervised Learning, an important step is to find the objective to optimize. <br><br>\n","\n","In this video, <br>\n","We'll be more precise about the objective we would like to optimize. \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lozXurCqoS9l","colab_type":"text"},"source":["### An idealized Scenario <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=12pHJSbpQRYAFbfj6NmFm3Q4P969fS1NT\" alt=\"2-01\" width=\"500\">\n","\n","<br><br>\n","\n","\n","Let's start by imagining an idealized scenario. <br><br>\n","\n","We get a sequene of pairs of states and true values $(S, v_{\\pi}(S))$. <br>\n","We want to use this date to find a parameterized function that closely approximates $v_{\\pi}$. <br>\n","$\\rightarrow \\quad \\hat{v}(s,W) \\; \\approx \\; v_{\\pi}(s)$ <br><br>\n","\n","We will do this by adjusting the weights <br>\n","so that the output of the function $\\hat{v}(s,W)$ closely matches the associated value for a given state $v_{\\pi}(s)$. <br><br>\n","\n","In general, <br>\n","we can't expect to perfectly match the value function on all states. <br>\n","The fucntion approximately we choose will will limit the value functions we can represent. \n","\n","<br><br>\n","\n","\n","To make our goal precise, <br>\n","we need to specify some measure of how close our approximation is to the value function. \n","\n","<br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NsiRXpBjcpN8","colab_type":"text"},"source":["### The Mean Squared Value Error Objective <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1MlxZbzujuBmZL8WBpzmdBwxc4ZfO19Av\" alt=\"2-02\" width=\"500\">\n","\n","<br>\n","\n","Consider a linear value function approximation denoted by $\\hat{v}$ <br>\n","Here to keep the visualization simple, the state is a single-dimentional and continuous. <br><br>\n","\n","Let's plot our estimate of the value function. $\\quad \\hat{v}(s,W)$<br>\n","Now, imagine the true value function looks like this (curvy). $\\quad v_{\\pi}(s)$ <br><br>\n","\n","Clearly, our approximation of $v_{\\pi}$ is not perfect, <br>\n","but how far off is it ? \n","\n","\n","<br><br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=13HsJI9q9Qa6uncTUgcIahb4JjnXj5Evw\" alt=\"2-03\" width=\"500\">\n","\n","<br>\n","\n","Let's define this more precisely ! <br><br>\n","\n","Let's start by defining a measure of the error between the value of a state and the approximate value. <br><br>\n","\n","A natural choice is the Squared Error. $\\quad [v_{\\pi}(s) - \\hat{v}(s,W)]^2$ <br>\n","That is the squared difference between the value anf our approximation. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1ECTgLqFtt_l5sPhZLsHPZDZVfEs3Uo1A\" alt=\"2-04\" width=\"500\">\n","\n","<br>\n","\n","However, this is not enough <br>\n","to define an objective function approximation. <br><br>\n","\n","Making the estimate more accurate in one state will often mean making it less accurate in another state ! \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1208RJOnXUOFZ39eqgITfYFramOwA-UGp\" alt=\"2-05\" width=\"500\">\n","\n","<br>\n","\n","For this reason, we need to specify how much we care about getting the value right for each state. <br>\n","We will call that $\\mu(s)$ $\\quad$ <br>\n",">( more about that in a second )\n","\n","<br>\n","\n","We can now write our full objective as a sum of the squared error over the entire state space <br>\n","where each state is weighted by $\\mu$ ! <br>\n","$\\Rightarrow \\quad$ We call this objective <br>\n","$\\qquad$ the Mean Squared Value Error\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1HCWre1KkcqmxQo2r80X7bchZb4nkMVP_\" alt=\"2-06\" width=\"500\">\n","\n","<br>\n","\n","Back to $\\mu$ <br>\n","What should we pick for $\\mu(s)$ ? <br>\n","Remeber, $\\mu(s)$ should tell us how much we care about each state. <br>\n","A natural measure is the fraction of time each state is visited under the policy. <br>\n","That means we want to minimize the average value error for that states we visit while following $\\pi$. <br>\n","The states that the policy spends more time in have a higher weight in the objective. <br>\n","We care less about errors in ths states the policy visites less frequently. \n","\n","\n","<br><br>\n","\n","\n","$\\mu(s)$ is a probability Distribution as shown on the slide ! <br><br>\n","\n","In this example, the policy spends little time at the extremes of the state space. <br>\n","This means, under the Mean Squred Value Error, we allow the value approximation to have higher error in those states. \n","\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YUsLN_dVj8WZ","colab_type":"text"},"source":["### Adapting the weights to minimize the mean squared value error objective <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1cjePDRfLnZeVShBKREDQsaA61sJ58qUO\" alt=\"2-07\" width=\"500\">\n","\n","<br>\n","\n","Now remeber the purpose of defining this objective. <br><br>\n","\n","We want to adapt our weights to make the Mean Squared Value Error as low as possible. <br><br>\n","\n","From now on we will call this objective $\\quad \\overline{VE} \\quad$ ( the mean sqaured Value Error ) <br>\n","Chaing the weights in one way may increase the value error <br>\n","while changing them in a different might decrease the value error. \n","\n","\n","<br><br><br>\n","\n","\n","\n","In the next lecture, <br>\n","We'll talk about how we can use Gradient Descent to incrementally adjust the weights. \n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"osV-47xX2c3o","colab_type":"text"},"source":["### Summary <br><br>\n","\n","  - Policy evaluation under function approximation requires us to specify an objective <br><br>\n","\n","  - Mean Squared Value Error is one possible objective\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9K52ZPos7HQo"},"source":["## $\\cdot$ Introducing Gradient Descent <br><br>\n","\n","\n","  - Understand the idea of Gradient Descent <br><br>\n","\n","  - Understand that Gradient Descent converges to stationary points\n","\n","<br><br>\n","\n","\n","Previously we looked at how value function estimation can be framed as Supervised Learning. <br>\n","We made this precise by specufying an objective. <br><br>\n","\n","Now we want to figure out \" how to find solutions to this objective \". <br><br>\n","\n","Today, we'll talk about a general strategy for minimizing objectives called Gradient Descent. <br>\n","We'll cover specific algorithms for the objectives in Reinforcement Learning. \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UnPOUt3a8n5h","colab_type":"text"},"source":["### Recap $\\quad : \\;$ Learning Parameterized Value Function <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1Zei_Z9iWIuaN3IAuS1gJNksaGVnGaap-\" alt=\"2-08\" width=\"500\">\n","\n","<br>\n","\n","We want to minimize the Mean Squared Value Error, <br>\n","to make our value estimate close to the true value function ! \n","\n","<br><br>\n","\n","Recall that <br>\n","our value estimates is given by a function of the state, parameterized by a set of real-valued weightes $\\; W$. <br><br>\n","\n","These weights determine how the value's computed for each state. <br>\n","changing the weights will modify the value estimate for many states. \n","\n","<br><br>\n","\n","We have to think about how to change the weights to minimize the overall value error. <br>\n",">To do that, we'll need a little bit of calculus.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xRzg5PLf9-i_","colab_type":"text"},"source":["### Understanding Derivatives <br><br>\n","\n","\n","Hopefully, you remember the idea of a derivative from calculus. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1_aoR1rqB4zeJd5IJzqECl-wxqqClKCkW\" alt=\"2-09\" width=\"500\">\n","\n","<br>\n","\n","Here we've plotted a function $f$, with scalar parameter $W$. <br><br>\n","\n","The derivative tells us how to locally change $W$ to increase or decrease $f$. <br>\n","The sign of the derivative of $f$ at a particular point $W$ indicates the direction to change $W$ to increase $f$. <br>\n","The magnitude of the derivative indicates the slope of the fucntion $f$ at the point $W$. <br>\n","That is how quickly $f$ will change as we vary $W$. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1WlSVulv7-aTx22o6Bcyb5WLOKyO1I__9\" alt=\"2-10\" width=\"500\">\n","\n","<br>\n","\n","Here, the derivative is positive. <br>\n","If we move $W$ in this direction, we increase $f$. <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1asau7LOCOTG7rkscTUpgg5OKd2CD0oqa\" alt=\"2-11\" width=\"500\">\n","\n","<br>\n","\n","At this other point, the derivative is negative. <br>\n","It is pointing in a direction to decrease $W$. <br>\n","But this direction still indicates how to increase $f$. <br>\n","If we move $W$ in this direction, it increases $f$. <br>\n","If we were to move it in the negative of this direction, it would decrease $f$. <br>\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"clHDDdKuGEYN","colab_type":"text"},"source":["### Gradient $\\quad : \\;$ Derivatives in multiple dimensions <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1n5kc3B3pY0LqtafKAZqeu9ACTboS5t06\" alt=\"2-12\" width=\"500\">\n","\n","<br>\n","\n","If $f$ is parameterized by more than one variable, <br>\n","then $W$ is a vector ! <br><br>\n","\n","In this case, <br>\n","we need to introduce the idea of a Gradient to describe \" how $f$ changes as the vector $W$ changes \".\n","\n","<br><br>\n","\n","The Gradient is a vector of partial derivatives, <br>\n","indicating how a local change in each component of $W$ affects the function. <br><br>\n","\n","Notice that <br>\n","  - the Gradient has to be the same size as the weight vector $W$. <br>\n","  - The sign of each component $\\frac{\\partial f}{\\partial w_k}$ of the gradient $\\nabla f$ specifies the direction <br>\n","  to change the associated component of $W$, in order to increase $f$. <br>\n","The direction is either positive or negative. <br>\n","  - The magnitude of the component specifies how quickly $f$ changes, as $W$ moves in that direction. <br>\n","  If the function is very steep in one dimension, the magnitude will be high. <br>\n","  If it is very flat, the magnitude of the Gradient in that dimension will be low. \n","\n","<br>\n","\n","Regardless of the magnitude, <br>\n","the Gradient gives the direction of steepest descent. <br>\n","It provides the direction to change $W$, so that locally $f$ is maximally increased !\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dbW-0XSfLBjH","colab_type":"text"},"source":["### Example $\\quad : \\;$ Gradient of a Linear Value Function <br><br>\n","\n","\n","To give a specific exaple, <br>\n","let's derive the Gradient of a Linear value function. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1RmSieZDV2qtcuQiN4jaeTKebVGfFFZt6\" alt=\"2-13\" width=\"500\">\n","\n","<br>\n","\n","  - Linear value approximation <br><br>\n","\n","  $\\hat{v}(s,W) \\doteq \\displaystyle \\sum w_i x_i(s)$ <br><br>\n","\n","  Remember, <br>\n","  a linear value approximation is just an inner product of the weights with the feature vector for the state. \n","\n","<br><br>\n","\n","\n","  - the Partial derivative of a linear value approximation <br><br>\n","\n","  $\\frac{\\partial \\hat{v}(s,W)}{\\partial w_i} = x_i(s)$ <br><br>\n","\n","  The partial derivative of the linear value approximation ( with respect to a single weight ) <br>\n","  is just the feature associated with that weight. <br><br>\n","\n","  Remember, <br>\n","  \" the features themselves do not depend on the weights. \" (???)\n","\n","<br><br>\n","\n","\n","  - the Gradient of Linear value approximation <br><br>\n","\n","  $\\nabla \\hat{v}(s,W) = X(s)$ <br><br>\n","\n","  This means <br>\n","  the Gradient of a linear value approximation is simply the feature vector for that state. \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zj79t4gVOp3n","colab_type":"text"},"source":["### Gradient Descent <br><br>\n","\n","\n","Our objective is a function of the weights. <br><br>\n","\n","For example, <br>\n","the mean squared value error is a function of the weights <br>\n","because it is a function of $\\hat{v}$. and $\\hat{v}$ is a function of the weights $W$. <br><br>\n","\n","\" Our goal is to adjest the weights to make the objective small. \"\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1-PsFnEFJF2Shqqcetk32ZxOqOHGYqPGw\" alt=\"2-14\" width=\"500\">\n","\n","<br>\n","\n","Here we have a hypothetical plot of our objective. <br><br>\n","To keep things simple, let's say our function is parameterized by a single weight. <br><br>\n","\n","\n","The x axis corresponds to this weight, <br>\n","The y axis corresponds to the objective value for this weight. <br><br>\n","\n","Because we have a single weight, <br>\n","this means our Gradient is just a scalar derivative. \n","\n","<br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=18xTIH3wC2EHnbzG3F6zglVM0lrbH9C9w\" alt=\"2-15\" width=\"500\">\n","\n","<br>\n","\n","If we want to decrease our objective function, <br>\n","we should move the weights in the direction of the negative of the gradient. <br><br>\n","\n","\" This is the idea of Gradient Descent. \" <br><br>\n","\n","$W_{t+1} \\doteq W_t - \\alpha \\nabla J(W_t)$ <br><br>\n","\n","Here's the Gradient Descent update rule that captures this intuition. <br><br>\n","\n","We make small changes in the direction that will most reduce the objective. <br>\n","We use the step size parameter $\\alpha$ to control how far we move. <br>\n","( as otherwise there is a risk of stepping too far ) <br>\n","This is becuse moving in this direction is only guaranteed to decrease the objective \" locally \".\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1ThtsKSb68ygye0HRTGLX6ak2FstjN9Wq\" alt=\"2-16\" width=\"500\">\n","\n","<br>\n","\n","By doing such updates repeatedly with a sufficiently small alpha $\\alpha$, <br>\n","We will eventually converge to a stationary point where the Gradient is $0$. <br><br>\n","\n","Likely, this will be a local minimum. <br>\n","This means these weights are better than all weights in the immediate vicinity. <br>\n","But may not be the best possible. <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1_pEMSss7q8igBxkM5h4yYztscDTMoT60\" alt=\"2-17\" width=\"500\">\n","\n","<br>\n","\n","Other possible stationary points are Local maxima and Saddle points. <br>\n","These are unstable solutions. <br>\n","The stochasticity inherent in our algorithms is usually enough to prevent getting stuck at these poor stationary points. <br><br>\n","\n","On the other hand, <br>\n","a local minima is stable. <br>\n","So even a stochastic algorithm will tend to converge there. <br><br>\n","\n","For this reason, <br>\n","we usually talk about our algorithms converging to local minima. \n","\n","<br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1W9cvNbFHuBPo2LS5ZqXPcFQUo67_dkwz\" alt=\"2-18\" width=\"500\">\n","\n","<br>\n","\n","In some cases, Gradient descent is guaranteed to converge to the Global minimum, <br>\n","which is the best possible setting of the weights for the objective. <br><br>\n","\n","For example, <br>\n","this is true for the mean squared value error with linar function approximation. <br>\n","For more complex fucntion approximators such as Neural Newrok, <br>\n","a stationary point may not be a Global minimum. \n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yTLBZgyO8J2f","colab_type":"text"},"source":["### Golobal Minima and Solution Quality <br><br>\n","\n","\n","Note that <br>\n","\" a Global Minimum does not necessarily correspond to the true value function \" <br><br>\n","\n","It is limited by our choice of function parameterization, <br>\n","and depends on our choice of objective. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1TbORzr73i4h10hqop7zW_oVCDgCnHJ-k\" alt=\"2-19\" width=\"500\">\n","\n","<br>\n","\n","Imagine a feature vector which just contains a single element that is always $1$. ( no matter what state you are in ) <br><br>\n","\n","The approximate value function that minimizes Mean Squared Value Error will converge to the average value over all the states. <br>\n","This is not a very good value function. <br><br>\n","\n","However, this is still the best we can do under that parameterization in terms of the value error objective. \n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"re_HtlWNA8xm","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - Gradient Descent can be used to find stationary points of objectives <br><br>\n","\n","  - These solutions are not always globaly optimal\n","\n","<br><br>\n","\n","\n","Next time, <br>\n","we'll talk about how to use Gradient Descent specifically for R.L.\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-iXxKgmSBnVl"},"source":["## $\\cdot$ Gradient Monte for Policy Evaluation <br><br>\n","\n","\n","  - Understand how to use Gradient Descent and Stochastic Gradient Descent to minimize Value Error <br><br>\n","\n","  - Outline the Gradient Monte-Carlo algorithm for Value Estimation\n","\n","<br><br>\n","\n","\n","You now know one strategy to minimize objectives, Gradient Descent. <br>\n","and we fomulated a clear objective for policy evaluation, the Value Error. <br><br>\n","\n","We should now be able to put these together to approximate values. <br>\n","Let's return to our old friend, Monte-Carlo ! <br>\n","\n",">Let's see how Monte-Carlo can be used to minimize the Value Error.\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uWtoKD1fC0GV","colab_type":"text"},"source":["### Gradient of the Mean Squared Value Error objective <br><br>\n","\n","\n","We just saw how to use Gradient Descent to minimize objectives. <br>\n","To use this approach, the first step is to find the gradient of your objective. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1FuV5sdGRLX7rB1CiFTD7-N5DYDO-VRSX\" alt=\"2-20\" width=\"500\">\n","\n","<br>\n","\n","$= \\nabla \\displaystyle \\sum_{s \\in S} \\mu(s) [v_{\\pi} - \\hat{v}(s,W)]^2$ <br>\n","So let's start by computing the gradient of the Mean Squared Value Error with respect to the weights of our approximation. \n","\n","<br><br>\n","\n","\n","$= \\displaystyle \\sum_{s \\in S} \\nabla \\mu(s) [v_{\\pi} - \\hat{v}(s,W)]^2$ <br>\n","Remember, this objective is a weighted sum of the squared error over all states. <br>\n","Following the ruled of calculus, we can pull the gradient inside the sum. \n","\n","<br><br>\n","\n","\n","$= - \\displaystyle \\sum_{s \\in S} \\mu(s) 2 [v_{\\pi} - \\hat{v}(s,W)] \\nabla \\hat{v}(s,W)$ <br>\n","We then take the gradient of each term inside the sum, <br>\n","which rquires the chain rule. <br><br>\n","\n","The gradient of $[v_{\\pi}(s) - \\hat{v}(s,W)]$ euquals the gradient of $\\hat{v}(s,W)$, <br>\n","because $v_{\\pi}(s)$ is not a function of $W$ ! <br>\n","In other words, changing $W$ does not change $v_{\\pi}(s)$\n","\n","<br><br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=11QkhcX01QDfP4Uluqr7JA44uzAY7FQQW\" alt=\"2-21\" width=\"500\">\n","\n","<br>\n","\n","The gradient of the value function approximation $\\nabla \\hat{v}(s,W)$ <br>\n","will depend on the particular parameterized function we are using. \n","\n","<br><br>\n","\n","\n","$\\hat{v}(s,W) \\doteq <W,X(s)>$ $\\qquad\\qquad\\qquad$ ( $<a,b>$ 는 내적 기호! ) <br>\n","In the case of linear function approximation, <br>\n","the gradient is particularly easy to compute ( as we showed in the previous video ) <br>\n","$\\nabla \\hat{v}(s,W) \\doteq X(s)$ <br>\n","It is simply the feature vector in that state $X(s)$. <br><br>\n","\n","\n","This gradient makes sense. <br>\n","The gradient of the value function approximation $\\nabla \\hat{v}(s,W)$ <br>\n","indicates how to change the weights $W$ to increase the value $\\hat{v}$ for that state $s$.\n","\n","<br><br>\n","\n","\n","error is positive | error is negative\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1fEpX_9F_fpKUntbTamsmrLa_fIkk77bz\" alt=\"2-22\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1lFTtOL5_e8yFoJ-LEm55h9tX40OueFad\" alt=\"2-23\" width=\"500\">\n","\n","<br>\n","\n","We multiply this direction by the difference between the true value $v_{\\pi}(s)$ and our estimate $\\hat{v}(s,W)$. <br><br>\n","\n","If the difference is positive, it means the true value is higher than our estimate. <br>\n","So we should change the weight in the direction that increases our estimate. <br><br>\n","\n","If the current error is negative, we should change the weights in the opposite direction. \n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qSZ1TYqAlxAx","colab_type":"text"},"source":["### From Gradient Descent to Stochastic Gradient Descent <br><br>\n","\n","\n","\n","$\\displaystyle \\sum_{s \\in S} \\mu(s) 2 [v_{\\pi} - \\hat{v}(s,W)] \\nabla \\hat{v}(s,W)$ <br><br>\n","\n","Computing the gradient for the Mean Squared Value Error requires summing over all states. <br>\n","This is generally not feasible. <br>\n","Also we likely do not know the distribution $\\mu$. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1_C_K68vkZpm7e-QSoZT6nvlWOfcQKMTK\" alt=\"2-24\" width=\"500\">\n","\n","<br>\n","\n","Instead, let's approximate this gradient. <br>\n","Imagine an idealized setting where we have access to $v_{\\pi}$ <br>\n","Though we do not explicitly have (distribution) $\\mu$, <br>\n","we can sample states from it simply by following the policy !\n","\n","<br><br>\n","\n","\n","1 | 2\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=11zX2bI8zRa8v02I-Cnzh0yeBknrxGouQ\" alt=\"2-25\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1gzhidFXLH_cig4x4ExRDicwbN8W9-zPZ\" alt=\"2-26\" width=\"500\">\n","\n","<br>\n","\n","Let's tatke one of these states, $S_1$. <br>\n","That occurs while following the policy with target $v_{\\pi}(S_1)$ of $S_1$. <br><br>\n","\n","We can use this pair $(S_1, v_{\\pi}(S_1))$ to make an update to decrease the error on that example ! \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1JhvI_YokNIGMBYw9yhKnuOUWCh0wniAD\" alt=\"2-27\" width=\"500\">\n","\n","\n","Here is the gradient for a single state. <br>\n","We can do gradient descent with this gradient to decrese the error on that state. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1XqK7cfJf5XXnZI1FrYUIAGvvpamRG_02\" alt=\"2-28\" width=\"500\">\n","\n","<br>\n","\n","So here's the corresponding gradient update rule for $S_1$. <br>\n","We can perform this update to decrease the error for this pair. <br>\n","Then we can do this again with another state $S_2$ observed while following $\\pi$. <br><br>\n","\n","By making small updates in the direction that improves error on each pair, <br>\n","we might sometimes increase the error on the full objective. <br>\n","But the overall trend will be to make progress for the full objective. <br><br>\n","\n","This updating approach is called Stochastic Gradient Descent, <br>\n","because it only uses a stochastic estimate of the gradient. <br>\n","In fact, the expectation of each stochastic gradient equals the gradient of the objective. \n","\n","<br><br>\n","\n","\n","You can think of this Stochastic Gradient as a noisy approximation to the gradient that is much cheaper to compute, <br>\n","but can nonetheless make steady progress to a minimum. \n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lGSBBoUTp7D1","colab_type":"text"},"source":["### Gradient Monte-Carlo <br><br>\n","\n","\n","$W_{t+1} \\doteq W_t + \\alpha [v_{\\pi}(S_t) - \\hat{v}(S_t,W_t)] \\nabla \\hat{v}(S_t,W_t)$ <br>\n","$\\qquad \\qquad \\qquad \\qquad \\downarrow \\qquad \\qquad \\qquad \\qquad \\;\\; \\downarrow$ <br>\n","$W_{t+1} \\doteq W_t + \\alpha [ \\quad G_t \\;\\; - \\hat{v}(S_t,W_t)] \\nabla \\hat{v}(\\; s \\; ,W_t)$ <br><br>\n","\n","\n","Stochastic Gradient Descent allowed us to efficiently update the weights on every step by sampling the gradient. <br><br>\n","\n","However, there is one remaining practical issue. <br>\n","We do not have access to $v_{\\pi}$. <br><br>\n","\n","Let's see how we can get rid of it from our update rule. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1HITKU122gVcSVyehjS9FJ2dUBO1bf91N\" alt=\"2-29\" width=\"500\">\n","\n","<br>\n","\n","Let's replace $v_{\\pi}(S_t)$ with an estimate. <br><br>\n","\n","One option is to use samples of the return for each visitied state $S_t$, <br>\n","as we did for Monte Caelo methods. <br><br>\n","\n","This makes sense because the value function is the expected value of these samples ! <br>\n",">$v_{\\pi}(s) \\doteq \\mathbb{E} \\big[ G_t | S_t=s \\big]$\n","\n","<br><br>\n","\n","\n","$\\mathbb{E}_{\\pi} \\big[ 2[ v_{\\pi}(S_t) - \\hat{v}(S_t,W)] \\nabla (S_t,W) \\big]$ <br>\n","$\\qquad \\quad \\;\\; \\downarrow$ <br>\n","$\\mathbb{E}_{\\pi} \\big[ 2[ \\quad G_t \\;\\; - \\hat{v}(S_t,W)] \\nabla (S_t,W) \\big]$ <br><br>\n","\n","\n","In fact, the expectation of the gradient, when we used a sampled return in place of the true value, <br>\n","is still equal to the gradient of the Mean Squared Vaalue Error. \n","\n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Y1ltXP8f-oet","colab_type":"text"},"source":["### the Gradient Monte-Carlo Algorithm <br><br>\n","\n","\n","This brings us to the Gradient Monte-Carlo algorithm for estimating $v_{\\pi}$ \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1dAzKclZdlQnN_xIs_xFDvS4W2S78_VI4\" alt=\"2-30\" width=\"1000\">\n","\n","\n","  1. line_1 <br>\n","  We take any policy $\\pi$ we wish to evaluate, <br>\n","  2. line_2 <br>\n","  We choose some function which maps states and weights to a real number that is differentiable in the weights. <br>\n","  For a given weight vector, whis $\\hat{v}$ is a function of state producing the approximate values. \n","  3. line_3 <br>\n","  We choose a value of the step size $\\alpha$, <br>\n","  and initialize the weights parameterizing our estimate however we like. ( for example $W=0$ ) <br>\n","  4. line_4. <br>\n","  On each iteration, the agent interacts with the environment to generate a full episode. <br>\n","  We compute the sample return for each visited state. <br>\n","  Then we loop through every step in the episode, <br>\n","  and perform a stochastic gradients and update based on the sampled returns. \n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_lTffh62DVig","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - How Stochastic Gradient Descent can be used to estimate a value function <br><br>\n","\n","  - How the Gradient Monte-Carlo works <br><br>\n","\n","\n","<br><br>\n","\n","Next time, <br>\n","You'll get to see the algorithm in action !\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2q33RnRJD54J"},"source":["## $\\cdot$ State Aggregation with Monte-Carlo <br><br>\n","\n","\n","  - Understand how state aggregation can be used to approximate the value function <br><br>\n","\n","  - Apply Gradient Monte-Carlo with state aggregation\n","\n","<br><br>\n","\n","\n","we'll discuss a concrete example of value approximation <br>\n","using a technique called State Aggregation. <br><br>\n","\n","We will use Monte-Carlo with State Aggregation <br>\n","to approximate the values for a large random walk domain. \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nhYAfZX8E2Mu","colab_type":"text"},"source":["### Example $\\quad : \\;$ Random walk <br><br>\n","\n","\n","A large random walk task <br>\n","An example where function approximation can help speed up learning \n","\n","<br><br>\n","\n","\n","1 | 2 | 3\n","--- | --- | ---\n","<img src=\"https://drive.google.com/uc?id=1Ctz_oVHx5HuZ2tioCynnn6lE3PVQMBdM\" alt=\"2-31\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1JM2XCMYql59oaUjla3i41rsO37T6gXhp\" alt=\"2-32\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1TsaxaXwnG1hRxd0RkoykjA0BU1yWQdDL\" alt=\"2-33\" width=\"500\">\n","\n","<br><br>\n","\n","\n","The states are numbered from 1 to 100 ( arranged in a line from left to right ) <br>\n","All episode begin in the middle of the line ( at state 500 ) <br>\n","Two actions : Left and Right <br>\n","In the left action, the agent jumps left to any state within 100 neighboring state uniformly at random. <br>\n","In the right action, the agent jumps right in the same way. <br><br>\n","\n","Let's evaluate the uniform random policy, <br>\n","which moves either left or right with equal probabiliity. \n","\n","<br><br>\n","\n","\n","States close to terminal state on the left <br>\n","have fewer than 100 neighbors to the left. <br>\n","All actions that would jump past the end of the chain go to the terminal state instead. <br>\n","( Likewist on the right ) <br><br>\n","\n","Terminating on the left gives a reward of $-1$. <br>\n","Terminating on the right gives a reward of $+1$. <br>\n","All other transitions give a  reward of $+0$ <br><br>\n","\n","The discount $\\gamma = 1$ \n","\n","<br><br>\n","\n","This task is fairly simple, but learning the values for 1000 states might take  a lot of time. <br>\n","We might benefit from some kind of function approximation. \n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kxW0RVurZvhv","colab_type":"text"},"source":["### State Sggregation <br><br>\n","\n","\n","Here we will use a technique called State Aggregation. <br>\n","As the name suggests, State Aggregation treats certain states as the same ! \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1IOn3XFk_kI8x8SvoW9zf8hFcfpYBfhdA\" alt=\"2-34\" width=\"500\">\n","\n","<br>\n","\n","In this table of eight states, <br>\n","we might choose to aggregate states together in groups of four. <br><br>\n","\n","So now instead of a table of eight entries for the value function, <br>\n","we just have two ! <br><br>\n","\n","When we update the value of any state in the first group, <br>\n","the values of all the other states in that group is updated ! \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1i5uDOjKtaE8R-iQx0kihY4R4VgEdimhT\" alt=\"2-35\" width=\"500\">\n","\n","<br>\n","\n","State Aggregation is another example of Linear function approximation. <br><br>\n","\n","There is one feature for each group of states. <br>\n","Each feature will be 1 if the current state belongs to the associated group, and 0 otherwise. <br>\n","The approximate value of a state is the weight associated with the group that state belongs to.\n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lSexqDjxc7D2","colab_type":"text"},"source":["### How to Comput the Gradient for Monte-Carlo with State Aggregation <br><br>\n","\n","\n","Let's look at the Gradient Monte-Carlo algorithm with State Aggregation <br>\n","We already know how to compute the value for a given state. <br><br>\n","\n","$\\qquad W \\; \\leftarrow \\; W + \\alpha [G_t - \\hat{v}(S_t,W)] \\; \\color{brown}{\\nabla \\hat{v}(S_t,W)}$ <br><br>\n","\n","Now we need to think about how we compute the gradient of the value function <br>\n","so that we can use our key update formula.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1OLswf9oNDrUQJiSIqCY5qcb5-ydBMYtd\" alt=\"2-36\" width=\"500\">\n","\n","<br>\n","\n","State Aggregation is an example of Linear function approximation, <br>\n","so the gradient is equal to the feature vector. <br>\n","$\\nabla \\hat{v}(S_t,W) = X(S_t) = \\big[ [0],[0],...,[0],[1],[0],...,[0] \\big]$\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1UA6B0mmer0dzM_Tti64q7Yj1sKcwrFDL\" alt=\"2-37\" width=\"500\">\n","\n","<br>\n","\n","The update rule modifies only the weight corresponding to the current active group. <br>\n","Let's think about how this update rule changes the weight. <br><br>\n","\n","If the estimated value was smaller than the sample of return $G_t$, <br>\n","then the weight is increasesd. <br><br>\n","\n","If the estimated value was greater than the sample of return $G_t$, <br>\n","then the weight is decreased. \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ANg_q32gmf9o","colab_type":"text"},"source":["### Constructing a State Agrregation for the Random walk <br><br>\n","\n","\n","Let's return to the Random walk, <br>\n","and see how we can apply State Aggregation.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=10zsLQwpPpaJlzktsGv1GmUEB9TBrMLb8\" alt=\"2-38\" width=\"500\">\n","\n","\n","First, we have to choose how we aggregate states. <br>\n","State aggregation foces states in the same group to use the same value estimate. <br>\n","So ideally, we should group states together if we have to believe their values will be similar. \n","\n","<br><br>\n","\n","In this problem, it's an easy choice. <br>\n","States close together should have similar values. <br><br>\n","\n","Now we have to choose how many states to put in each group. <br>\n","If the groups are small, our value estimates will ultimately be more accurate, but it'll take longer to learn. \n","\n","<br><br>\n","\n","We use 10 groups, each containing 100 states. <br>\n","This is a nice change from 1000 states we started with. \n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zTyF5Kj-0XKV","colab_type":"text"},"source":["### Monte-Carlo updates for a Single Episode <br><br>\n","\n","\n","Let's step through the Gradient Monte-Carlo algorithm to see how it works. <br>\n","\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1_ZfyTyL5OCEdjrzrGcWNqSmgu8QPCoCE\" alt=\"2-39\" width=\"500\">\n","\n","<br>\n","\n","We input the policy $(50/50)$, <br>\n","our chosen state aggregation function $[w_1, w_2, ..., w_{10}]$, <br>\n","and a small step-size parameter value $\\alpha = 2*10^{-5}$. <br>\n","We initialize the weights to $0$. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1A_TKrO8-Se0KsxLsFOqikvVIq7WDsstE\" alt=\"2-40\" width=\"500\">\n","\n","<br>\n","\n","First, we generate an episode. <br>\n","$500 \\rightarrow 423 \\rightarrow 482 \\rightarrow ... \\rightarrow 936$ \n","\n","<br><br>\n","\n","\n","Once we have a complete trajectory, <br>\n","We step through all the states we visited and make an update ! <br><br>\n","\n","The return for each state was $1$, <br>\n","becaue the agent terminated on the right with a reward of $R=+1$. <br>\n","All other transitions generated reward of $R=+0$. \n","\n","<br><br>\n","\n","\n","Our first update is to state $500$\\. $\\quad$ ( which belongs to group $w_5$ ) <br>\n","So we perform a Gradient Monte-Carlo update to that weight based on the sample return of $1$. <br><br>\n","\n","We do updates like this <br>\n","for every state in the trajectory. $\\quad$ ( $500 \\rightarrow 423 \\rightarrow 482 \\rightarrow ... \\rightarrow 936$ ) <br><br>\n","\n","Eventually, we finish with state $936$. $\\quad$ ( which was the last state we visited before terminating ) <br>\n","Sinse the state $936$ belongs to the group $w_10$, <br>\n","the last update we make is to weight $10$. \n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Ay4j3_PUC9Er","colab_type":"text"},"source":["### Value Estimates after One episode <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1GOrw3lfmXZPCppbFkmiaw8LTAjEhyyKd\" alt=\"2-41\" width=\"500\">\n","\n","<br>\n","\n","After all the updates have been made, <br>\n","our value estimates might look something like this ! <br><br>\n","\n","This plot shows the estimates after a single epsiode. <br>\n","We plotted the learn(ed?) value for each states from 1 to 1000. <br><br>\n","\n","The state space is not continuous, but it makes sense to plot the approximate value as a line. <br><br>\n","\n","Notice that <br>\n","the value estimates for each groups are non-negative. <br>\n","This is because the return for the first episode was positive and our initial estimates were 0. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1gHCMxYj1ZZq9DSaObBjVpmu24s3shKee\" alt=\"2-42\" width=\"500\">\n","\n","<br>\n","\n","Let's look at the same plot after another episode. <br>\n","This time the agent terminated on the left, getting reward of $R=-1$. <br>\n","( That means the reaturn from every state was $-1$ ) <br><br>\n","\n","The weights for the visited states decreased slightly. ( ??? )\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1coI3VJiAnf7GqA701hAfsqkel7DMWUR5\" alt=\"2-43\" width=\"500\">\n","\n","<br>\n","\n","After running for many episodes, <br>\n","we get state values that look something like this. <br><br>\n","\n","Each step in the plot corresponds to our group of states that share the same approximate value. <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1HTL-BQpfAD_Ci_zHBDRJrMT3s01fOuga\" alt=\"2-44\" width=\"500\">\n","\n","<br>\n","\n","For comparison, here's the true value function. <br>\n","Although we have heavily approximated by aggregating many states together, <br>\n","our value estimate is not that far off. <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1AZqCfyXsmx0hbRnu5ZH1PG9Q-HhHe-I-\" alt=\"2-45\" width=\"500\">\n","\n","<br>\n","\n","\" Why didn't the red line pass directly through the center of all the blue steps ? \" <br><br>\n","\n","This is where $\\mu$ plays an important role ! <br><br>\n","\n","Recall $mu(s)$ is the visitation frequency for state $s$. <br><br>\n","\n","States near the center of the chain are visited more often <br>\n","than those near the terminal states. ( as depicted here ) <br><br>\n","\n","For example, <br>\n","let's look at the leftmost group of states, states 1 to 100 $\\quad$ ( at the black-arrow $\\; : \\; w_1$ part ) <br>\n","States near state $100$ are visited much more than states near state $1$. <br>\n","The approximate value is skewed towards the true value for states near state $100$. <br>\n","This is because $\\mu$ weights these states higher in the value error. \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kuBcQFsGIBUO","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - How Gradient Monte-Carlo works in practice <br>\n","  when our value function approximation uses state aggregation\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]}]}
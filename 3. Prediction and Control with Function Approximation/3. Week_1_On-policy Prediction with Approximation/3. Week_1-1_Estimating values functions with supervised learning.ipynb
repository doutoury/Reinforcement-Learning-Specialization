{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. Week_1-1_Estimating values functions with supervised learning","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyOnN6UmhfrdfF5COmhX1FNb"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yydQ9qjsKBRW","colab_type":"text"},"source":["# Week_1 <br>\n","\n","INDEX <br><br>\n","\n","\n","  - Estimating values functions with supervised learning <br>\n","    - Moving to parameterized Functions <br>\n","    - Generalization and Discrimination <br>\n","    - Framing Value Estimation as Supervised Learning <br><br>\n","  \n","  - The Objective for On-policy Prediction\n","    - The Value Error Objective <br>\n","    - Introducing Gradient Descent <br>\n","    - Gradient Monte for Policy Evaluation <br>\n","    - State Aggregation with Monte-Carlo <br><br>\n","\n","  - The Obejective for TD <br>\n","    - Semi-Gradient TD for Policy Evaluation <br>\n","    - Comparing TD and Monte-Carlo with State Aggregation <br>\n","    - Doina Precup : Building Knowledge for A.I. Agents with Reinforcement Learning <br><br>\n","\n","  - Linear TD <br>\n","    - The Linear TD Update <br>\n","    - The Ture Objective for TD <br>\n","    - Week 1 Summary\n","\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"39ARKswlLNZg","colab_type":"text"},"source":["## __1. Estimating values functions with supervised learning__ <br><br>\n","\n","\n","  - Moving to Parameterized Functions <br><br>\n","\n","  - Generalization and Discrimination <br><br>\n","\n","  - Framing Value Estimation as Supervised Learning \n","\n","\n","\n","\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"SrLl-g65LbKc","colab_type":"text"},"source":["## $\\cdot$ Moving to Parameterized Functions <br><br>\n","\n","\n","  - Understand how we can use parameterized functions to approximate values <br><br>\n","\n","  - Explain linear value function approximation <br><br>\n","\n","  - Recognize that the tabular case is a special case of linear value function approximation <br>\n","\n","  - Understand that there are many ways to parameterize an approximate value function \n","\n","<br><br><br>\n","\n","\n","\n","Up until now, <br>\n","we've learned about tabular motheds. <br>\n","Methods that store table containing separately learn values, for each possible state. <br><br>\n","\n","In real-world problems, <br>\n","these tables will become intractably large. <br>\n","Imagine a robot that sees the world through a camera. <br>\n","We clearly cannot store a table entry for every possible image. <br><br>\n","\n","Luckily, this is not the only possibility. <br>\n","Today we'll talk about more general ways to approximate value function. \n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pPKg1i-z6Mwc","colab_type":"text"},"source":["### A value function can be represented in many different ways ! <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=17gBwBArH21whSZn8RMmWp1MbxMa6E1zS\" alt=\"1-01\" width=\"500\">\n","\n","<br>\n","\n","In previous courses, <br>\n","we spent a lot of time thinking about how to estimate value funnctions. <br>\n","So far, our value approximations have only the same form. <br><br>\n","\n","For each state, we store seperate value in a table. <br>\n","We look at values, and modify them in the table as learning progresses. \n","\n","<br><br>\n","\n","But this is not the only way to approximate a value function ! \n","\n","<br><br>\n","\n","In principle, we can use any function ! <br>\n","that takes a state, and produces a real number. <br><br>\n","\n","For example, in a grid like this, <br>\n","our value function approximation could take the X and Y position, <br>\n","and add them together to produce a value estimate. <br>\n","This was just to get your intuitions going. <br>\n",">We wouldn't want to use this as a value function, brcause we can't modify the approximation. <br>\n",">So we have no way to do learning. \n","\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vWIJTzAf3mhE","colab_type":"text"},"source":["### Parameterizing the value function <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1gXcpBrOoJuBvSuMTO5HRG6VRZOw8wRqS\" alt=\"1-02\" width=\"500\">\n","\n","<br>\n","\n","This is where the idea of a parameterized function comes in. <br>\n","We incorporate a set of real valued weights, <br>\n","which we can adjust to change the function !\n","\n","\n","<br><br><br>\n","\n","\n","\n","#### Example of Parameterized value function <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1Gdi9AHLKncuRW__qCAASapeCOdgOg6Xb\" alt=\"1-03\" width=\"500\">\n","\n","<br>\n","\n","For example in this grid, <br><br>\n","\n","instead of using a fixed sum of X and Y, <br>\n","we could use a function of the form $\\quad w_1 X + w_2 Y$. <br><br>\n","\n","The weights $w_1$ and $w_2$ paramiterize our function ! <br>\n","They allow us to change the output the function generates. <br><br>\n","\n","To indicate that this function approximates the true value function, <br>\n","we use the notation $\\hat{v}$ <br><br>\n","\n","$w$ is a vector containing all the weights that parameterize the approximation. <br>\n","We do not have to store a whole table of values !!! <br>\n","We only have to store two weights to represent our value function approximation !!! <br><br>\n","\n",">\" You can imagine how this compact representation is useful for large state spaces ! \"\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PERWn85A6jl1","colab_type":"text"},"source":["### How changes to the weight impact the value function <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=16dpQPiz5jjUS2S9J1ouGoIOoDkxNIoVj\" alt=\"1-04\" width=\"500\">\n","\n","<br>\n","\n","The tabular representation modifying the value estimates for one state leaves the other unchanged. <br>\n","This is no longer the case with a parameterized function. <br><br>\n","\n","Watch what happens to the value estimates of each state as we change the weights $w_1$. <br>\n","If we change either $w_1$ or $w_2$, we will change the value estimate for every state. <br>\n","Now, our learning outcomes will modify the weights, instead of the individual state values. \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bSft6ofz9HRP","colab_type":"text"},"source":["### Linear Value Function Approximation <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1Ifu0CtQc4o-RRCGM7COvglAVcZGXjV4E\" alt=\"1-05\" width=\"500\">\n","\n","<br>\n","\n","The example we just discussed is a special case, <br>\n","called Linear value function approximation. <br><br>\n","\n","In this case, the value of each state is represented by a linear function of the weights. <br>\n","This simply means that the value of each state is computed as the sum of the weights multiplied by some fixed attributes of the state called features. <br>\n","$\\rightarrow \\quad w_i x_i (s)$ <br><br>\n","\n","We can express this campactly. <br>\n","We write that the approximate value is given by the inner product of this feature vector and the weight vector. <br>\n","$\\rightarrow \\quad <W,X(s)>$ <br>\n",">We will use bold X of s to denote the feature vector.\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"M97AGzeH9xSF","colab_type":"text"},"source":["### Limitations of Linear Value Function Approximation <br><br>\n","\n","\n","Our choice of feature impacts the kinds of value function we can represent. <br>\n","For example, consider the approximation we've dicussed so far. <br>\n","With it, we can only represent value function which changed linearly as a function of the features X and Y.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1SnI2YUvhxHDbw0kEJUc_MbVrv6IVQ6DG\" alt=\"1-06\" width=\"500\">\n","\n","<br>\n","\n","Consider the case where the true value function is given by the numbers on this grid. <br><br>\n","\n","We cannot represent this as a linear function of X and Y ! \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1tU69Y2Swgz4yOpWX9iiaXuUyyzZKK4Ex\" alt=\"1-07\" width=\"500\">\n","\n","<img src=\"https://drive.google.com/uc?id=193QR9jejtLZ9skuVcB2UkUzrQH4kUT2f\" alt=\"1-08\" width=\"500\">\n","\n","<br>\n","\n","To get these outer values correct, $w_1$ and $w_2$ both need to be zero. <br>\n","However that would mean these inner values are not correct because at least one of $w_1$ and $w_2$ must be non-zero for the sum that equal five. <br><br>\n","\n","But we don't have to use X and Y as features ! <br>\n","Linear function approximation relies on having good features ! <br>\n","( Here, X and Y are not good feature for this problem T -T ) \n","\n","<br><br>\n","\n","But as you'll see later, <br>\n","there are many powerful methods to construct features ! \n","\n","\n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"M0rFHfPlBRzc","colab_type":"text"},"source":["### Tabular value functions are linear functions <br><br>\n","\n","\n","Linear function approximation is actually very general. <br>\n","In fact, even a Tabular representation is a special case of Linear function approximation. <br><br>\n","\n","To build a linear value function to represent this tabular value funciton, <br>\n","we need to define the features. \n","\n","<br><br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1tC8n2UqWADqZf2AZ81E6e7Gjp973WTz-\" alt=\"1-09\" width=\"500\">\n","\n","<br>\n","\n","Let's choose our features to be indicator functions for particular states (here the red-box). <br><br>\n","\n","For state $s_i$, feature $i$ ($i$-th element?) is one and the remaining features are zero. <br>\n","we have 16 features, one for each state. \n","\n","<br><br>\n","\n","Let's compute the approximate values for a state with these features. <br>\n","Since all the features are zero except for one of them, <br>\n","the inner product is equal to the single weight associated with that state. <br><br>\n","\n","Seince every state's value is represented by a seperate weight, <br>\n","this is equivalent to a tabular value function !\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BMlo70ptBg7G","colab_type":"text"},"source":["### Non-linear Function Approximation <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=15Oa8YusbCEswptSYMFUM_VnLc_v6Ds_q\" alt=\"1-10\" width=\"500\">\n","\n","<br>\n","\n","These parameterized values are general, <br>\n","and we can consider lots of different types of functions ! <br><br>\n","\n","Neural Networks are an example of a non-linear function of state ! <br><br>\n","\n","The output $\\hat{v}(s,W)$ of the network is our approximate value for a given state. <br>\n","The state $S$ is passed to the network as the input. <br>\n","All the connections in the network correspond to real valued weights $W$ ! <br><br>\n","\n","When date is passed through a connection, <br>\n","the weight is mutiplied by it's input. <br>\n","This process transforms the input state through a sequence of layer to finally produce the value estimate. \n","\n","<br><br>\n","\n","We will take a deep dive intro Neural Networks later !\n","\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"u6E5c2Y0EkBa","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - Parameterized functions and how they might be used to approximate value functions \n","\n","<br><br>\n","\n","In the comming week, <br>\n","we will discuss \" how to update these approximate value functions while the agent interacts with the world \" !\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_jxMeZOtFPj1"},"source":["## $\\cdot$ Generalization and Discrimination <br><br>\n","\n","\n","  - Understand what is meant by Generalizaion and Discrimination <br><br>\n","\n","  - Understand how Generalization can be beneficial <br><br>\n","\n","  - Explain why we want both Generalization and Discrimination from our function approximation\n","\n","\n","<br><br>\n","\n","Perhaps the most important consideration in function approximation is <br>\n","\" how it generalizes between states \". <br><br>\n","\n","Generalization is something that humans do naturally. <br>\n",">Once a person learns how to drive one car, they don't have to start from scratch to learn how to drive a different car. <br>\n",">They also don't have to start from scratch on a different street or when it is raining.\n","\n","<br>\n","\n","We'd like our agents to be able to generalize too !\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TQXMEFV7GyAu","colab_type":"text"},"source":["### Generalization $\\quad : \\;$ Updates to one state affect the value of other states <br><br>\n","\n","\n","Generalization intuitively means <br>\n","\" applying knowledge about specific situations to draw conclustion about a wider variety of situations ! \" \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1wbnzJ7gKB5I7FvLqBU0iI8Swee_pe_2j\" alt=\"1-11\" width=\"500\">\n","\n","<br>\n","\n","When we talk about Generalization in the contect of policy evaluation, <br>\n","we mean that \" updates to the value estimate of one state influence the value of other states \". \n","\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1NPrwJ8krmOwOG1kzuJMTJP9y_zSUFamG\" alt=\"1-12\" width=\"500\">\n","\n","<br>\n","\n","Imagine a robot tasked with collecting cans, observing the world through a set of distance sensors. <br><br>\n","\n","In many locations, it would take the same amount of time to drive to the nearest can. <br>\n","Even though they correspond to different sensor readings, these locations have similar values ! <br>\n","Thus, we might want the value function to generalize across those states. \n","\n","<br><br><br>\n","\n","\n","\n","Generalization can speed learning by making better use of the experience we have. <br>\n","You may not have to visit every state as much to get this values correct <br>\n","if we can learn it's value from similar states. \n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"y5mezu2jMR4W","colab_type":"text"},"source":["### Discrimination $\\quad : \\;$ The ability to make the value of two states different <br><br>\n","\n","\n","On the other hand, <br>\n","Discrimination means the ability to make the values for two states different <br>\n","to distinguish between the values for these two states. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1NESB3M6O0EX-UopEsrDjc4Y3jXL5ZNtD\" alt=\"1-13\" width=\"500\">\n","\n","<br>\n","\n","Going back to the example of a robot collecting cans. <br>\n","Imagine it is in a state where a can is three feet away, but behind a wall. <br>\n","Contrast this to a state where a can is three feet away, but with a clear paths to reach it. <br>\n","The robot would want to assign different values to these states. <br><br>\n","\n","So while it is useful to generalize between states with similar distance to the nearest can, <br>\n","it is also important that we discriminate between states ! <br>\n","based on other information when it is likely to impact their value.\n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xagDLQGyOCJB","colab_type":"text"},"source":["### Categorizing methods based on Generalization and Discrimination <br><br>\n","\n","\n","We can visualize the space of possible methods <br>\n","in terms of a two-dimentional plot of Generalization and Discrimination. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1uWvnU776IUaYjmhy8l_wHXcZOTfRVYPv\" alt=\"1-14\" width=\"500\">\n","\n","<br><br>\n","\n","\n","  - Tabular method <br>\n","  the tabular method we've discussed so far lie down bottom right. <br>\n","  They discriminate between different states perfectly, <br>\n","  but do not generalize the learn values at all. <br>\n","  each value is represented independently. <br><br>\n","\n","  - Aggregate all states <br>\n","  On the other extreme, we could treat all states as the same. <br>\n","  Each update generalizes to all states, <br>\n","  but cannot discriminate at all. <br>\n","  At best, we will be able to learn the average return independent of the current state. <br>\n","  This is not very useful. <br><br>\n","\n","  - $*$ <br>\n","  What we would really like is a learning method that achieves good generalization and a good discrimination. <br>\n","  Such a method would generalize the learn values to similar states, <br>\n","  allowing it to learn faster, <br>\n","  but it could also discriminate between states. <br>\n","  Meaning, with more date, the value function approximation can accurately represent the values. <br><br>\n","\n","  - $\\cdot$ <br>\n","  In practice, we are more likely to get a point here at the dot, <br>\n","  where we trade off some level of discrimination for genralizaiton. <br>\n","  >For example, <br>\n","  >we might combine similar states together and represent their values with one number. \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XmNtmIijR-hk","colab_type":"text"},"source":["### How should we generalize ? <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1jpFsEQEHBHox3Opm0Ff-hGiZGKqUE_29\" alt=\"1-15\" width=\"500\">\n","\n","<br>\n","\n","To build some more intuition, <br>\n","let's look at the game of chess as a concrete example. \n","\n","<br><br>\n","\n","\n","Take the extreme case <br>\n","where we treat all states as the same. <br><br>\n","\n","This value correspond do the probability of winning regardless of the state of the game. <br>\n","With equally matched palyers, this number might be 50%. \n","\n","<br><br>\n","\n","\n","On the opposite extreme, <br>\n","we have the tabular case, <br>\n","where we treat every state as totally different. <br><br>\n","\n","This is fine for small problems, but in a game like chess, it is impractical to even enumerate all the possible states. <br>\n","There are approximately 10 to the 46 states. <br>\n","Further, imagine how long it would take to individually learn the value of all these states. \n","\n","<br><br>\n","\n","We obviously want something in-between <br>\n","where we generalize between states with similar probabilities of winning. <br>\n","Identifying these similarities to get such groupings is a difficult question with no single answer. \n","\n","<br><br><br>\n","\n","\n","\n","How we generalize can have a major impact on the performance of our algorithms ! <br>\n","It is a central topic in Machine Learning and Reinforcement Learning ! \n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uHZSh5GYVZbt","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - Tabular representations have provided good Discrimination $\\quad$ ( but no Generailzation ) <br><br>\n","\n","  - Generalization is important for faster learning <br><br>\n","\n","  - Having both Generalization and Discrimination is ideal ! <br>\n","  ( Though at practice, there's typically a trade-off )\n","\n","\n","<br><br>\n","\n","Throughout the next few modules, <br>\n","We will visit these concepts as we examine particular function approximators. \n","\n","<br><br><br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BPd6iUzPWXIP"},"source":["## $\\cdot$ Framing value estimation as Supervised Learning <br><br>\n","\n","\n","  - Understand how value estimation can framed as a Supervised Learning problem <br><br>\n","\n","  - Recognize not all function approximation methods are well-suited for Reinforcement Learning\n","\n","\n","<br><br>\n","\n","Supervised Learning methods learn a function from a set of input target examples. <br>\n","This is very different from Reinforcement Learning. <br><br>\n","\n","But, <br>\n","Supervised Learning methods can be usedul for handling parts of the Reinforcement Learning problem ! \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rN41QPBLd-aE","colab_type":"text"},"source":["### Supervised Learning <br><br> \n","\n","\n","Supervised Learning involves approximating a function, <br>\n","given a dataset of input target pairs. \n","\n","<br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1CxsDQ9PTaeXaUAskuDxRXvVSPnfL555A\" alt=\"1-16\" width=\"500\">\n","\n","<br>\n","\n","For example, <br>\n","we has a list of house prices along with a set of attributes for each house. \n","\n","<br><br>\n","\n","\n","We could use Supervised Learning to train a function <br>\n","to take the attributes of a house as input and estimate the expected price of the house. \n","\n","<br><br>\n","\n","\n","The hope is that the function learned will also generalize to approximate the expected price for houses that were not in the training set. <br>\n","This parameterized function can be represented in a variety of ways. <br>\n","for example, a Neural Network. \n","\n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vTGt_LUje6tx","colab_type":"text"},"source":["### Framing policy evaluation as Supervised Learning <br><br>\n","\n","\n","The problem of policy evaluation in Reinforce Learning can be framed Similarity. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1EkzsxDlKUtvJmPxJy8v2GBwP-lP2AZlt\" alt=\"1-17\" width=\"500\">\n","\n","<br><br>\n","\n","\n","  - MC <br><br>\n","\n","  This Similarity is most obvious in the case of Monte-Carlo methods. <br>\n","  Remember, the Monte-Carlo methods estimate the value function using samples of the return. <br><br>\n","\n","  We can think of this as an example of a Supervised Learning problem, <br>\n","  where the input is the state and the targets are the returns. ! <br><br>\n","\n","  By training on enough examples, <br>\n","  we hope that the output of our learn(ed?) function will be a close approximation of the expected return, <br>\n","  in other words, the value in each state.\n","\n","<br><br>\n","\n","\n","  - TD <br><br>\n","\n","  TD can also be framed as Supervised Learning. <br>\n","  In this case, the targets are the one-step bootstrap return. \n","\n","<br><br><br>\n","\n","\n","\n","In principle, <br>\n","any function approximation technique from Supervised Learning can be applied to the policy evaluation task ! \n","\n","<br><br>\n","\n","However, not all are equally well-suited. <br>\n","Let's see why.\n","\n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qu1RgmlIhmEx","colab_type":"text"},"source":["### The function approximator should be compatible with On-line updates <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1nvLJ7omQZA9jF1sN7trC-HfVkbs1aK7i\" alt=\"1-18\" width=\"500\">\n","\n","\n","In Reinforcement Learning, <br>\n","an agent interacts with an environment and continually generates new data. <br><br>\n","\n","This is often called \" the On-line setting \". <br>\n","To distinguish it from the Off-line setting where the full dataset is avilable from the start and(?) remains fixed throughout learning. <br><br>\n","\n","If we want to use a function approximation technique, <br>\n","we should make sure it can work in the on-line setting. <br>\n","Some methods are not compatible with the on-line setting. <br>\n",">because they are either designed for a fixed batch of data <br>\n",">or they are not designed for temporally correlated date. <br>\n",">And the date in Reinforcement Learning is always correlated ! \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6N8U9pc3kN2A","colab_type":"text"},"source":["### The function approximator should be compatible with bootstrapping <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=15hSdkmulBFPbYh9-isttWxlbSVwfSTKV\" alt=\"1-19\" width=\"500\">\n","\n","<br>\n","\n","TD methods introduce an adiitional complication <br>\n","when applying techniques from Supervised Learning. <br><br>\n","\n","\n","TD methods use Bootstrapping, <br>\n","meaning that our targets now depend on our own estimates. <br>\n","These estimates change as learning progresses and so our targets continually change. <br><br>\n","\n","\n","This is different than Supervised Learning where we have access to a ground truth label as the target. <br><br>\n","\n","\n","For example, <br>\n","the price of a house not depend on our estimate <br>\n","nor does it change when we change our estimates. <br><br>\n","\n","\n","Supervised Learning methods are typically not designed for changing targets <br>\n","nor targets computed from the agent's own estimates !\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FaD__ToHl4o9","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - We can frame the policy evaluation task as a Supervised Learning problem <br><br>\n","\n","  - But not all methods from Supervised Learning are ideal for Reinforcement Learning <br>\n","  ( We will need methods that are compatible with online updating and bootstrapping ! )\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n"]}]}
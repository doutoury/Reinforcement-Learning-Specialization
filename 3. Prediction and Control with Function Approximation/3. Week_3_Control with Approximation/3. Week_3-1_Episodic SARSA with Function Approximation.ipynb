{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. Week_3-1_Episodic SARSA with Function Approximation","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyO5ROq9/c45VWgtuHzsp/ke"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yydQ9qjsKBRW","colab_type":"text"},"source":["# Week_3 <br>\n","\n","INDEX <br><br>\n","\n","\n","  - Episodic SARSA with Function Approximation <br>\n","    - Episodic SARSA with Function Approximation <br>\n","    - Episodic SARSA in Mountain Car <br>\n","    - Expected SARSA with Function Approximation <br><br>\n","  \n","  - Exploration under Function Approximation <br>\n","    - Exploration under Function Approximation <br><br>\n","\n","  - Average Reward <br><br>\n","    - Average Reward $\\quad : \\;$ A New Way of Formulating Control Problem <br>\n","    - Satinder Singh $\\quad : \\;$ On Intrinsic Rewards <br>\n","    - Week 3 Review\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"39ARKswlLNZg","colab_type":"text"},"source":["## __1. Episodic SARSA with Function Approximation__ <br><br>\n","\n","\n","  - Episodic SARSA with Function Approximation <br><br>\n","\n","  - Episodic SARSA in Mountain Car <br><br>\n","\n","  - Expected SARSA with Function Approximation\n","\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"SrLl-g65LbKc","colab_type":"text"},"source":["## $\\cdot$ Episodic SARSA with Function Approximation <br><br>\n","\n","\n","  - Understand how to consturct action-dependent features for approximate action-value <br><br>\n","\n","  - Explain how to use SARSA in episodic tasks with function approximation\n","\n","<br><br>\n","\n","This week, <br>\n","We'll talk about control with Function Approximation. <br><br>\n","\n","In this video, we'll start with a GPI ( Generalized Policy Iteration ) algorithmm, <br>\n","Episodic SARSA. \n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mH8FL7BzDeEy","colab_type":"text"},"source":["### State-values to action values <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=12S2JvKhsbQude4ceVvsqz241aE3CLbUj\" alt=\"1-01\" width=\"500\">\n","\n","<br>\n","\n","Recall that <br>\n","the va lue function approximation has two components. <br>\n","  - $W^T \\quad : \\;$ Weight vector <br>\n","  - $X(s) \\quad : \\;$ Feature vector \n","\n","<br><br>\n","\n","\n","In a given state $S$, <br>\n","the value of estimate is the dot product between these two components. <br>\n","$v_{\\pi} \\approx \\hat{v}(s,W) \\doteq W^T X(s)$\n","\n","<br><br>\n","\n","\n","To move from TD to SARSA, <br>\n","We need action-value functions. <br>\n","So the feature representation has to represent actions as well. <br>\n","$q_{\\pi} \\approx \\hat{q}(s,a,W) \\doteq W^T X(s,a)$\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KoWlDdpMDpmw","colab_type":"text"},"source":["### Representing actions <br><br>\n","\n","\n","One way to do this is <br>\n","to have a seperate function approximator for each action. <br>\n","This can be accomplished by stacking the features. <br><br>\n","\n","Stacking the features <br>\n","is we can use the same state features for each action, <br>\n","but only activate the features corresponding to that action. <br><br>\n","\n","To understand this more clearly, let's look at an example. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=10k2AxYGb0cyvRB5iTWc6MLlEO77H42K7\" alt=\"1-02\" width=\"500\">\n","\n","<br>\n","\n","Let's say there are four features $X(s)$ and three actions $A(s)$. <br>\n","The four features represent the state you are in. <br><br>\n","\n","But we want to learn a function of both states and actions. <br><br>\n","\n","We can do this by repeating the four features for each action. <br>\n","Now the feature vector has 12 components. <br><br>\n","\n","Here, each segment of four features corresponds to a seperate action $a_0, a_1, a_2$. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1tWLZunv7SH2h5VaC2CQ5tIpb1TWwR_QK\" alt=\"1-03\" width=\"500\">\n","\n","<br>\n","\n","We call this feature representation stacked, <br>\n","because the weights for each action are stacked on top of each other. <br><br>\n","\n","Thus, <br>\n","only the features for the specified action will be active, <br>\n","while thouse for the other actions will be set to $0$ !\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"meaHzqwoFxIF","colab_type":"text"},"source":["### Computing action-values <br><br>\n","\n","\n","Let's see an example of how to calculate the action-values from a given state. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1fCpXV54nIFKoMZ0d-kIsGBj36zVSPwGU\" alt=\"1-04\" width=\"500\">\n","\n","<br>\n","\n","Let's say that <br>\n","there are four features and three actions. <br>\n","The weight vector looks like this. <br><br>\n","\n","With stacked features, <br>\n","we get the following feature vector for action $a_0$ in state $s_0$. <br>\n","We zero out the features for the other actions. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=14nXJCXSGZ82x6CRWez-gh_UdYSy91PzP\" alt=\"1-05\" width=\"500\">\n","\n","<br>\n","\n","So we extract the segment of the weight vector corresponding to each action. <br>\n","The action-values are then the dot products between each segment of the weight vector and the feature vector.\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8VgQEWmCPxa8","colab_type":"text"},"source":["### Computing Action-values with a Neural Network <br><br>\n","\n","\n","You might think that stacking features to create action-values <br>\n","is specific to linear function approximation. <br>\n","But this is not ths case. <br><br>\n","\n","For example, <br>\n","the common way to represent action-values with a neural network is <br>\n","to gernerate multiple outputs, one for each action-value. \n","\n","<br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=16r_ES40mBpNzMKhMcJqVOKwdR-mLbnFO\" alt=\"1-06\" width=\"500\">\n","\n","<br>\n","\n","This, however, is equivalent to the stacking procedure we just described. <br><br>\n","\n","The neural network inputs the state, and the last hidden layer produces the state-features. <br>\n","Each action-value is computed from an independent set of weights using those state-features ! <br>\n","The weights for one action-value do not interact with those of another action-value, just like in stacking. \n","\n","<br><br><br>\n","\n","\n","\n","We might want to generalize over actions <br>\n","for the same reason generalizing over state can be useful. <br><br>\n","\n","\" How might this work with a neural network ? \"\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1C6uPK91PGu-fJtscyx7tMtt3beVxWJ5O\" alt=\"1-07\" width=\"500\">\n","\n","<br>\n","\n","We would input both the state and and the action to the network. <br>\n","There would only be one output. The approximate action-value for that state and action ! $\\hat{q}(s, a, W)$ <br><br>\n","\n","We can do something similar with Tile Coding by passing both the state and action as input.\n","\n"," \n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FmRZALTHRirf","colab_type":"text"},"source":["### Episodic SARSA with function approximation <br><br>\n","\n","\n","We now know how to handle action-values with function approximation. <br>\n","Let's talk about using SARSA for control of function approximation. <br>\n","The algorithm quites similar to the Tabular version (of SARSA), <br>\n","so we will just review the differences. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1lJ79aGaDd-IzxB8nMJ1DjPgUkLItSMGZ\" alt=\"1-08\" width=\"500\">\n","\n","<br>\n","\n","We use parameterized action-value function for the action-value estimates. <br>\n","$\\hat{q}(S,A,W), \\quad \\gamma \\hat{q}(S',A',W) - \\hat{q}(S,A,W)$ <br><br>\n","\n","The update also changes to use the gradient to update the weights similar to Semi-gradient TD. <br>\n","$\\nabla \\hat{q}(S,A,W)$ \n","\n","<br><br><br>\n","\n","\n","\n","That's it. <br>\n","That's the SARSA control algorithm with function approximation !\n"," \n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"an6uqx7PRv5Y","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - Talked about action-dependent features <br><br>\n","\n","  - Introduced Episodic SARSA with function approximation \n","\n","<br><br>\n","\n","We will cover a few more control algorithms in the coming lectures. \n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"017Khyv9UZbM"},"source":["## $\\cdot$ Episodic SARSA in Mountain Car <br><br>\n","\n","\n","  - Gain experience analyzing the performance of an approximate TD control method\n","\n","<br><br>\n","\n","\n","In this video, <br>\n","we get to see Episodic SARSA in action, on a continuous state domain. <br><br>\n","\n","We'll visualize the learn values, <br>\n","and gain some intuition about the solutions learned by SARSA. \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"l2sHLY80VHuL","colab_type":"text"},"source":["### Example $\\quad : \\;$ The Mountain Car environment <br><br>\n","\n","\n","Let's looak at an example of episodic SARSA on a classic control domain called Mountain Car.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1Ns6q5opfvsdhU3eGBSdS0OhTTPp_OTib\" alt=\"1-09\" width=\"500\">\n","\n","<br>\n","\n","Mountin Car is an episodic task where the objective is to drive an underpowered car up the side of the mountain. <br>\n","Gravity is stronger than the car's engine, so the agent cannot directly drive up the mountain. <br>\n","The only way to escape is to first drive backwards up the left slop. <br>\n","Then driving down the hill gives the car enough momentum to drive up the right slop and out. <br><br>\n","\n","The episode begincs with a car in a random location near the bottom of the valley. <br>\n","It ends when the car reaches the flags at the top of the hill. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1uGwUAIeC2deacVxIgLlp-F7T3dBsLUWt\" alt=\"1-10\" width=\"500\">\n","\n","<br>\n","\n","To encourage the agent to finish the episode quickly as possible, <br>\n","The reward is $-1$ on  each timestep, <br>\n","and no discounting is issued $\\gamma = 1.0$. <br><br>\n","\n","The agent can observe the current position and velocity of the car. <br>\n","This is a 2-dimensional countinuous-valued state. <br><br>\n","\n","The agent has a choice between three actions : <br>\n","accelerate forward, accelerate backward, or coast (no acceleration).\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3sx2lGZeeQuz","colab_type":"text"},"source":["### Feature representation <br><br>\n","\n","\n","For this example, we jointly tile-code(?) the position and velocity to produce features. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1yA8ytwY88pjWaCBquV09OavAIpLb63Ah\" alt=\"1-11\" width=\"500\">\n","\n","<br>\n","\n","  - We use eight tiles, <br>\n","  ( which means we use 8 by 8 grids for the 2-dimensional input space. ) <br>\n","  - We use 8 tilings, <br>\n","  ( which means we have 8 overlapping grids. ) <br>\n","  - We treat actions independently <br>\n","  by using a stacked feature representaion. \n","\n","<br><br>\n","\n"," \n","We initialize the weights to $0$. <br><br>\n","\n","This initalization is actually optimistic. <br>\n","This is because a reward is $-1$ per step, <br>\n","so the values under any policy will be less than $0$. <br><br>\n","\n","In this problem, these optimistic initial values cause extensive and systematic exploration. <br>\n","Because of this, we can act greedily without any additional random exploration.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2nuhsJMjk8Q8","colab_type":"text"},"source":["### Learned values <br><br>\n","\n","\n","Let's see what the vlue function looks like <br>\n","after we run the agent for a very long time. <br><br>\n","\n","Ideally, we would like to plot the value function for every state. <br>\n","However, there is an uncountably infinite number of states. <br>\n","So we'll have to sample the set of states. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1glSfQf0XvHdK_BXId8mnlnPQsAUMtkSE\" alt=\"1-17\" width=\"500\">\n","\n","\n","We'll use the max value in each sample state. <br>\n","$\\displaystyle \\max_a Q(s,a,W)$ <br><br>\n","\n","This number tells us the number of steps the agent thinks it will take to escape under it's greedy policy. <br><br>\n","\n","The reward is $-1$ for each step, <br>\n","so we negate this number $\\max_a Q(s,a,W)$ <br>\n","to produce the agent's estimate of the number of steps to go, from each state. \n",">negate : 음수를 취하다 ?\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1jwwVIexw8cSsJEO8wZSZC9yktqvlqxJ2\" alt=\"1-12\" width=\"500\">\n","\n","<br>\n","\n","Here's the agent's estimate of the expected number of steps after 9000 episodes. <br>\n","This is a really long time for this problem, so we're pretty sure the value estimates are about as good as they're going to get. <br>\n","But they're not perfect due to function approximation.\n","\n","<br><br>\n","\n","The green line represents the goal position <br>\n","which does not depend on the velocity. \n","\n","<br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1m3zkiG8cDowi4H3PfBKSQdXmViUnAEAm\" alt=\"1-13\" width=\"500\">\n","\n","<br>\n","\n","Near the goal, <br>\n","if the velocity is large enough, <br>\n","the agent can directly drive out. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1IwxqJgnbWEgk6ifO1ZvJlV-4r5EZp7cL\" alt=\"1-14\" width=\"500\">\n","\n","<br>\n","\n","However, <br>\n","if the agent is near the goal but velocity is too low, <br>\n","it will take many steps to escape. <br><br>\n","\n","This is because <br>\n","the agent must first go back down hill, <br>\n","and up the left side again to gain enough momentum to escape. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1ehBnOgJqBTY1Pg-S66uymUf-_VVFcfv-\" alt=\"1-15\" width=\"500\">\n","\n","<br>\n","\n","The pick corresponds to the start states, <br>\n","where it takes around 120 steps to reach the flag. <br><br>\n","\n","This green trajectory reveals the path <br>\n","taken through the state-space by the learn policy ! \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"C574J3GrnvS9","colab_type":"text"},"source":["### Learning Curves <br><br>\n","\n","\n","The value function looks pretty interesting. <br>\n","but let's look at some learning curves to get better insight into the speed of learning. <br><br>\n","\n","Let's try SARSA with three different values of $\\alpha$, <br>\n","so three variances of the algorithm. <br><br>\n","\n","Here we are interested in the steps to goal, <br>\n","which in this case corresponds to the return per episode. <br><br>\n","\n","We expect the number of steps per episode to decrease with learning. <br>\n","Lower on the x-axis corresponds to better policies, <br>\n","policies that can reach the goal in the fewest steps on average. <br><br>\n","\n","As always, we average the performance over many independent runs. <br>\n","Here are the results. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1aMim2DJe26yJFA2z5YICCpCQxV9Et-gx\" alt=\"1-16\" width=\"500\">\n","\n","<br>\n","\n","By 500 episodes, the number of steps per episode has roughly stabilized. <br>\n","All the learning curves exhibit the familiar exponential profile. <br><br>\n","\n","The smaller step size parameter value of 0.1 results in slower learning, <br>\n","while $\\alpha$ of 0.5 allow the agent to learn more quickly and find a better policy over 500 episodes. \n","\n","<br><br>\n","\n","\n","Notice <br>\n","we divided each $\\alpha$ value by $8$. <br><br>\n","\n","If we are not using a vector of step sizes, <br>\n","we often scale the step size parameter by the norm of the feature vector. <br><br>\n","\n","Here we use $8$ tilings in our tile coder. <br>\n","That means the number of $1$s in the feature vector is always $8$. (?) <br><br>\n","\n","As a simple exercise, prove yourslef that $8$ corresponds to the $L_1$ norm of the feature vector. \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Ez1JNwqgn_uj","colab_type":"text"},"source":["### Summary  <br><br>\n","\n","\n","  - Evaluated Episodic SARSA with linear function approximation in Mountain Car \n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hBRYnHUwOdQO"},"source":["## $\\cdot$ Expected SARSA with function approximation <br><br>\n","\n","\n","  - Explain the update of Expected SARSA with function approximation <br><br>\n","\n","  - Explain the update of Q-learning with function approximation\n","\n","<br><br>\n","\n","So far we've talked about using SARSA with function approximation. <br>\n","Let's continue our journey through approximate control methods and look at Expected SARSA. \n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"d_e3vtPDPMIV","colab_type":"text"},"source":["### From SARSA to Expected SARSA <br><br>\n","\n","Let's  see how we can turn SARSA into Expected SARSA <br>\n","when using function approximation. \n","\n","\n","<img src=\"https://drive.google.com/uc?id=1T4ryq4QY-ov-IR54R7F_pM53a1eOGZ32\" alt=\"1-18\" width=\"500\">\n","\n","<br>\n","\n","Recall the update equation for SARSA <br>\n","SARSA'a update target includes the action value for the next state and action $Q(S_{t+1}, A_{t+1})$. <br>\n","$Q(S_t, A_t) \\quad \\leftarrow \\quad Q(S_t, A_t) + \\alpha \\big( R_{t+1} + \\gamma \\color{brown}{Q(S_{t+1}, A_{t+1})} - Q(S_t, A_t) \\big)$ <br><br>\n","\n","Now recall the Expected SARSA instead using the expectation over it's target policy. <br>\n","To compute the expectation, <br>\n","we simply sum over the action values weighted by their probability under the target policy. <br>\n","$Q(S_t, A_t) \\quad \\leftarrow \\quad Q(S_t, A_t) + \\alpha \\big( R_{t+1} + \\gamma \\color{brown}{\\displaystyle \\sum_{a'} \\pi(a'|S_{t+1})Q(S_{t+1},a')} - Q(S_t, A_t) \\big)$\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"THy3-0HAR-dE","colab_type":"text"},"source":["### Expected SARSA with Function Approximation <br><br>\n","\n","The same expectation can be computed <br>\n","even when we use function approximation !\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1jf6pgakuMhm6Z6orhCBAdOU49iojmDOI\" alt=\"1-19\" width=\"500\">\n","\n","<br>\n","\n","First, recall the update for SARSA with function approximation. <br>\n","It looks similar to the Tabular setting <br>\n","except the action value estimates are parameterized by the weight factor (vector?) $W$. <br>\n","$\\color{brown}{\\hat{q}(S_{t+1},A_{t+1},W)}$ <br>\n","We also have a gradient term to distribute the error to the weights appropriately. <br>\n","$\\nabla \\hat{q}(S_{t},A_{t},W)$\n","\n","<br><br>\n","\n","Expected SARSA with function approximation follows a similar structure. <br>\n","We compute the action values from our weight vector for every action in the next state. <br>\n","Then we compute this expectation under the target policy. <br>\n","$\\color{brown}{\\displaystyle \\sum_{a'} \\pi(a'|S_{t+1}) \\hat {q}(S_t,A_t,W)}$\n","\n","<br><br>\n","\n","\n","That's all we have to do to change SARSA into Expected SARSA.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Ctsvy5vOaYge","colab_type":"text"},"source":["### Expected SARSA to Q-learning <br><br>\n","\n","\n","Now, how do we do this for Q-learning with function approximation ? <br><br>\n","\n","Fortunately, <br>\n","Q-learning is a special case of Expected SARSA. <br><br>\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=132kec-gyAFgf5I0VcDj1K9nXQrRkG5Rk\" alt=\"1-20\" width=\"500\">\n","\n","<br>\n","\n","In Q-laerning, <br>\n","the target policy is greedy with respect to the approximate action values. <br><br>\n","\n","$\\rightarrow \\quad$ Computing the expectation under greedy policy <br>\n","$\\qquad$ is the same es computing the maximum action value. <br>\n",">So the Q-learning update with function approximation is actually quite straightforwards.\n","\n","<br>\n","\n","$\\rightarrow \\quad$ We just use the max <br>\n","$\\qquad$ in place of the expectation.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hzlUiF2DDec3","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - Introduced Expected Sarsa with function approximation <br><br>\n","\n","  - Extended it to Q-learning with function approximation\n","\n","<br><br>\n","\n","Next time, <br>\n","We'll think about how to do exploration with function approximation !\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]}]}
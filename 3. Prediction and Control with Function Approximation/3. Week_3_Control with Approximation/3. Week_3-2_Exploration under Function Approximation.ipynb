{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. Week_3-2_Exploration under Function Approximation","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyN1wiDTGsGtd/z0Oxcpps23"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NY9hgTnZEI4H"},"source":["\n","## __2. Exploration under Function Approximation__ <br><br>\n","\n","\n","  - Exploration under Function Approximation\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kXW45XjlD8AE"},"source":["## $\\cdot$ Exploration under Function Approximation <br><br>\n","\n","\n","  - Describe how optimistic initial values and $\\epsilon - greedy$ can be used with function approximation. \n","\n","<br><br>\n","\n","The need to balance exploration and exploitation <br>\n","is one of the defining characteristics of the sequential decision-making problem. <br><br>\n","\n","We've talked about several simple ways to promote exploration in Bandits and Tabular Reinforcement Learning. <br><br>\n","\n","\" What about Function Approximation ? \" <br>\n","\" Is there anything special about exploration there ? \" <br>\n",">Today, we'll find out.\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ABUzHkTDMpv1","colab_type":"text"},"source":["### Optimistic Initial Values in the Tabular setting <br><br>\n","\n","\n","Let's do a breif refresher on <br>\n","how we use optimistic initial values in the Tabular setting. <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1tPzU4fTft6wfB2we05T8g3Ksld1ZrhZ_\" alt=\"2-01\" width=\"500\">\n","\n","<br>\n","\n","We initialize our values to be greater than the true values. <br><br>\n","\n","This is like the agent imagining that <br>\n","it can get more reward by taking that action than it actually can in reality.<br><br>\n","\n","Typically, <br>\n","initializing the value function this way causes the agent to systematically explore the state-action space. <br><br>\n","\n","As the agent's values become more accurate, <br>\n","they are impacted less and less by this initialization. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1MmoWz-5fEBne4-kbns0CMhcbhMO0TFmd\" alt=\"2-02\" width=\"500\">\n","\n","<br>\n","\n","This is straightforward to implement in a Tabular setting <br>\n","where the update to each state-action pair is independent of all the other state-action pairs. \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AAb9cK6gOShC","colab_type":"text"},"source":["### How to Initialize Values Optimistically under Function Approximation <br><br>\n","\n","In Function Approximation, <br>\n","OLptimistic Initial values corresponds to initializing the weights <br>\n","such that the resulting values are optimistic. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=18c0cejCQv4toJ56ofqjq6Jue98XKlRGn\" alt=\"2-03\" width=\"500\">\n","\n","<br>\n","\n","  1. Linear case <br><br>\n","\n","  In some cses this is straightforward,  <br>\n","  when the features are binary, we simply initialize each weight to be the largest possible return. <br><br>\n","\n","  Then, as long as each state has at least one feature active, the value will be optimistic and likely overly so.\n","\n","<br><br>\n","\n","\n","  2. Non-linear case <br><br>\n","\n","  In many cses, however, <br>\n","  It is difficult to initialize optimistically. <br><br>\n","\n","  For example, in a Neural Network, <br>\n","  the relationship between the final values and the features can be quite complicated. <br><br>\n","\n","  Imagine a network composed of $\\tanh$ activation functions. <br>\n","  The network could output negative values even with positive initial weights.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"r_RGcOcukBHO","colab_type":"text"},"source":["### How Optimisim interacts with Generalization <br><br>\n","\n","\n","But this isn't the whole story. <br>\n","Depending on how our features generalize, <br>\n","optimistic initial values may not result in the same kind of systematic exploration we see in the Tabular case.\n","\n","<br><br>\n","\n","1 | 2\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1P8U00bI3NyPTVZPRbGxsMFHf16h_WEJ5\" alt=\"2-04\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1AuyCsIuGyedD5hWatwpilprXQQNlqQpQ\" alt=\"2-05\" width=\"500\">\n","\n","<br><br>\n","\n","\n","  - Single feature <br><br>\n","\n","  Consider an extreme example, where we have only one feature that is always $1$. <br>\n","  We can initialize optimistically, but every update will change the value  for all states. <br><br>\n","\n","  This means that before some states are even visited, the value will already have decreased <br>\n","such that it is no longer optimistic ... \n","\n","<br><br>\n","\n","\n","  - Tile coding <br><br>\n","\n","  To facilitate systematic exploration, <br>\n","changes to the vlaue function need to be more localized. <br><br>\n","\n","  For example, <br>\n","  function approximation with Tile Coding can produce such localized updates.\n","\n","<br><br>\n","\n","\n","  - Neural Network <br><br>\n","\n","  Neural Networks also provide local updates, <br>\n","  but neural networks may also generalize aggressively. <br><br>\n","\n","  In practice, without special consideration, <br>\n","  a neural network will lose his optimism relatively quickly.\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"20trDYi3m5GI","colab_type":"text"},"source":["### $\\epsilon - greedy$ <br><br>\n","\n","\n","Epsilon-greedy is generally applicable and easy to use <br>\n","even in cases with Non-linear function approximation ! \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1zlqm2AU_KfUevVEkFjCRCHG6XQbfo9WN\" alt=\"2-06\" width=\"500\">\n","\n","<br>\n","\n","The only thing Epsilon-greedy needs are the action value estimate, $\\hat{q}(S_t, a, W)$, <br>\n","Independent of how they are initialized or approximated. <br><br>\n","\n","However, Epsilon-greedy is not a directed exploration method. <br>\n","It relies on randomness to discover better actions near states followed by the current policy. <br><br>\n","\n","It is therefore not as systematic as exploration methods that rely on optimism.\n","\n","<br><br>\n","\n","\n",">Improving exploration in the function approximation setting remains an open research question. <br>\n",">So in this course, we'll stick with this simple strategy. \n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"iZzPvkhIorb1","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - Many subtleties when combining Optimistic Initial Values aan Function Approximation <br><br>\n","\n","  - $\\epsilon - greedy$ can be combined with any Function Approximation\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n"]}]}
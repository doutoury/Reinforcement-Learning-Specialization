{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. Week_3-3_Average Reward","provenance":[],"private_outputs":true,"collapsed_sections":["39ARKswlLNZg","WuNT8VN5tMi6"],"authorship_tag":"ABX9TyOoKwBpyRKt3Aj0P/epTByd"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"39ARKswlLNZg","colab_type":"text"},"source":["## __3. Average Reward__ <br><br>\n","\n","\n","  - Average Reward $\\quad : \\;$ A New Way of Formulating Control Problem <br><br>\n","\n","  - Satinder Singh $\\quad : \\;$ on Intrinsic Rewards <br><br>\n","\n","  - Week 3 Review\n","\n","\n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"SrLl-g65LbKc","colab_type":"text"},"source":["## $\\cdot$ Average Reward $\\quad : \\;$ A New Way of Formulating Control Problem <br><br>\n","\n","\n","  - Describe the average reward setting <br><br>\n","\n","  - Explain when average reward optimal policies are different from policies obtained under discounting <br><br>\n","\n","  - Understand different value functions. \n","\n","<br><br>\n","\n","In countinuing task, <br><br>\n","\n","we might be interestend in extremely long horizon performance. <br>\n","Up until now, we've used dincounting in continuing problems to balance short-term performance and long-term gain. <br><br>\n","\n","However, this is not the only way to formulate the probelm. <br>\n","Today, we'll learn about a new way of formulating continuing problems, called the average reward formulation. \n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WuNT8VN5tMi6","colab_type":"text"},"source":["### A simple example <br><br>\n","\n","\n","Today is all about continuing tasks. <br><br>\n","\n","Imagine a simple task <br>\n","where the states are arranged in two intersecting rings. <br>\n","Let's call this the Nearsighted(근시안적인) MDP.\n","\n","<br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1TYR_XgAGFxBS57z0j4sx8-hYno4OaMfk\" alt=\"3-01\" width=\"500\">\n","\n","<br>\n","\n","State <br>\n","In most states, <br>\n","there's only one action, so there are no decisions to be made. <br>\n","There's only one state, $S$, where a decision can be made. <br>\n","In this state, the agent can decide which ring to traverse. <br><br>\n","\n","Action <br>\n","This means there are two deterministic policies, <br>\n","traversing the left ring or <br>\n","traversing the right ring. <br><br>\n","\n","Reward <br>\n","The reward is $0$ everywhere, except for in one transition in each ring. <br>\n","In the left ring, the reward is $+1$ immediately after state $S$, <br>\n","In the right ring, the reward is $+2$ immediately before state $S$. \n","\n","<br><br>\n","\n","\n","Intuitively, <br>\n","you would pick the right action because you know will get $+2$ reward. <br><br>\n","\n","But <br>\n","\" what would the value function tell us to do ? \" \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1sj6nDropcEivKCOTVMXfrniVko77fizE\" alt=\"3-02\" width=\"500\">\n","\n","<br>\n","\n","If we use discounting ($\\gamma$), <br>\n","What are the values of state $S$ under these two different policies ? <br><br>\n","\n","The policy that chooses the left action has a vlaue of $\\frac{1}{1-\\gamma^5}$ <br><br>\n","\n","The policy that chooses the right action has a value of $\\frac{2\\gamma^4}{1-\\gamma^5}$ <br><br>\n","\n",">How do we figure this out ? <br>\n",">If you write out the infinite discounted return, <br>\n",">you will see this is a fairly straightforward geometric series with a closed form solution. <br>\n",">( See if you can get the same answer that we did )\n","\n","<br><br>\n","\n","\n","1 | 2\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1PwsScLA4oL3io2n113vGTAzvw7--qLC5\" alt=\"3-03\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1omHia-ZwmSx2WxBUEes8MnC1EPXSFh8m\" alt=\"3-04\" width=\"500\">\n","\n","<br>\n","\n","\n","Let's think of the value of state $S$ under these two part policies <br>\n","for particular values of $\\gamma$. <br><br>\n","\n","If $\\gamma=0.5$, <br>\n","$v_L(S)$ is approximately $1$ and <br>\n","$v_R(S)$ is approximately $0.1$. <br><br>\n","\n","This means <br>\n","the policy that takes the left action is preferable <br>\n","under this more myopic(근시적인) discount $\\gamma$ of $0.5$. <br><br>\n","\n","Let's try larger value of $\\gamma=0.9$. <br>\n","$v_L(S)$ is approximately $2.4$ and <br>\n","$v_R(S)$ is approximately $3.2$. <br><br>\n","\n","So now we prefer the other policy ?! \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=15XOj_NrzaWJUVU2yJE7r4Fcj7T8cHi2b\" alt=\"3-05\" width=\"500\">\n","\n","<br>\n","\n","In fact, <br>\n","we can figure out the minimum value of $\\gamma$ so that the agent prefers the policy that goes right. <br><br>\n","\n","$\\gamma$ needs to be at least $0.841$. <br>\n","( for the state $S$ to be $v_R(S) > v_L(S)$ )\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1-xsDy1Kx4OJVBu2Lw-WbMXP-DwSsO4xs\" alt=\"3-06\" width=\"500\">\n","\n","<br>\n","\n","So the problem here is that <br>\n","the discount $\\gamma$ magnitude depends on the problem. <br>\n",">For this example, $\\gamma = 0.85$ is sufficiently large.\n","\n","<br>\n","\n","But if the rings had 100 states each, <br>\n","this discount factor $\\gamma$ would need to be over $0.99$. \n","\n","<br><br><br>\n","\n","\n","\n","#### Problem with large $\\gamma$ in continuing task <br><br>\n","\n","\n","In general, <br>\n","the only way to ensure that the agent's actions maximize reward over time <br>\n","is to keep increasing the discount factor $\\gamma$ towards $1$ ! <br><br>\n","\n","Depending on the problem, <br>\n","we might need $\\gamma$ to be quite large. <br>\n","Remember, <br>\n","we can't set it to $\\gamma = 1$ in a continuing setting <br>\n","because then the return might be infinite. \n","\n","<br><br>\n","\n","\n","Now then, <br>\n","\" what's wrong with having large $\\gamma$ ? \" <br>\n","Larger values of $\\gamma$ can also result in larger and more variables sums, <br>\n","which might be difficult to learn ! \n","\n","<br><br>\n","\n","\n","\" Is there an alternative ? \" <br>\n","$\\rightarrow \\quad$ The Average Reward objective\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XgnIdL7Rx_AG","colab_type":"text"},"source":["### The Average Reward objective <br><br>\n","\n","\n","Let's discuss a new objective, called the Average Reward. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=16_HpJZouYPQ153Edx1SoiT_lw7-mX_5p\" alt=\"3-07\" width=\"500\">\n","\n","<br>\n","\n","Imagine the agent has interested with the world for $h$ steps. <br>\n","$r(\\pi) \\quad \\doteq \\quad \\displaystyle \\lim_{h \\rightarrow \\infty} \\sum_{t=1}^h \\mathbb{E}\\big[ R_t | S_0,A_{0:t-1}\\sim\\pi \\big]$ <br>\n","This is the reward it has received on average across those $h$ steps. <br>\n","( In other words, it's rate of reward ) <br><br>\n","\n","If the agent's goal is to maximize this average reward, <br>\n","then \" it cares equally about nearby and distant rewards. \" <br><br>\n","\n","We denote the average reward of a policy with $r(\\pi)$.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1Si9hDIwylZXMOl2KveARqW92GaZeQl9d\" alt=\"3-08\" width=\"500\">\n","\n","<br>\n","\n","More generally, <br>\n","we can write the average reward using the state visitation, $\\mu_{\\pi}(s)$. <br>\n","$r(\\pi) \\quad = \\quad \\displaystyle \\lim_{h \\rightarrow \\infty} \\sum_s \\mu_{\\pi}(s) \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a)r$ <br><br>\n","\n","This inner term is the expected reward in a state under policy $\\pi$. <br>\n","The outer sum takes the expectation over how frequently the policy is in that state. <br><br>\n","\n","Together, <br>\n","we get the expected reward across states ! <br>\n","( In other words, the Average Reward for a plicy )\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1o-TQE7CvcLhVilg2gTz5KHfdXiJUpPEh\" alt=\"3-09\" width=\"500\">\n","\n","<br>\n","\n","In the nearsighted example, <br>\n","the two deterministic possible policised visit either the left loop or the right loop indefinitely. <br>\n","In both cases, the five states in each loop are visited equally many times. <br><br>\n","\n","In the left loop, <br>\n","the immediate expected reward is $+0$ for all states, <br>\n","except one which gets $+1$. <br><br>\n","\n","This results in an average reward of $1$ every $5$ steps. <br>\n","$r(\\pi_L) = \\frac{1}{5} = 0.2$ <br><br>\n","\n","In the right loop, <br>\n","most states have $+0$ reward to expected reward, <br>\n","but the last state gets $+2$. <br><br>\n","\n","This gives an average reward of $2$ every $5$ steps <br>\n","$r(\\pi_R) = \\frac{2}{5} = 0.4$\n","\n","<br><br>\n","\n","\n","$\\Rightarrow \\quad$ the Average Reward puts preference on the policy that receives more reward in total <br>\n","$\\qquad$ without having to consider larger and larger discounts !\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Xwfbwtmj-3Bm","colab_type":"text"},"source":["### Returns for Average Reward <br><br>\n","\n","\n","The Average Reward definition is intuitive for saying <br>\n","\" If one policy is better than another \". <br><br>\n","\n","But <br>\n","\" How can we decide which actions from a state are better ? \" <br><br>\n","\n","What we need are action values for this new setting.\n","\n","<br><br>\n","\n","\n","1 | 2\n","--- | ---\n","Return | <img src=\"https://drive.google.com/uc?id=1Txm5TLHW3_Tput4cqAavxpD1oU1Is47x\" alt=\"3-10\" width=\"500\">\n","&ensp; | $$\\downarrow$$\n","Differential Return | <img src=\"https://drive.google.com/uc?id=1T9pn-oZXZb53oGdF07hrFXWx3qxrHgu7\" alt=\"3-11\" width=\"500\">\n","\n","<br><br>\n","\n","\n","The first step is to figure out what the retern is. <br><br>\n","\n","In the Average Reward setting, <br>\n","returns are defined in terms of differences between rewards and the average reward $R_{\\pi}$. <br>\n","$G_t \\quad = \\quad R_{t+1}-r(\\pi) \\quad + \\quad R_{t+2}-r(\\pi) \\quad + \\quad R_{t+3}-r(\\pi) \\quad + \\quad ...$ <br><br>\n","\n","This is called the differential return. \n","\n","<br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZGgFSqZVItQ4","colab_type":"text"},"source":["#### Example $\\quad : \\;$ nearsigted MDP <br><br>\n","\n","\n","Let's look at what the differential returns are in our nearsighted MDP. <br><br>\n","\n","The differential return represents <br>\n","how much more reward the agent will receive from the current state in action compared to the average reward of the policy.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1yFK_7mrz4-6jx-YBVbCzwfE-Ka6h-27u\" alt=\"3-12\" width=\"500\">\n","\n","<br>\n","\n","Left case <br><br>\n","\n","Let's loot at the differential return <br>\n","starting in state $S \\;\\; \\rightarrow \\;\\;$ first choosing action $L \\;\\; \\rightarrow \\;\\;$ then following $\\pi_L$ afterwards.  <br><br>\n","\n","The average reward for this policy $\\pi_L$ is $0.2$. <br>\n","$r(\\pi_L) = 0.2$ <br><br>\n","\n","The differential return is <br>\n","the sum of rewards into the future with the average reward subtracted from each one. <br><br>\n","\n","This sum starts in state $S$ with the action $L$. <br>\n","We can compute it by summing to some finite horizon $H$, then taking the limit as $H$ goes to infinity $\\infty$. <br>\n","We can simplifying things slightly with this limit notation. <br><br>\n","\n","While notation provided works in many cases, <br>\n","we need to use a different technique when the environment is periodic. <br>\n","In this case, we compute the return using a more genreal limit the Cesaro Sum. <br>\n","( but this technical detail is not critical ) <br>\n","( The main point here is the intuition ) <br><br>\n","\n","We find that the differential return is $0.4$. \n","\n","<br><br>\n","\n","\n","Now, <br>\n","let's look at the other action. <br><br>\n","\n","This time we can break the differential return into two parts. <br><br>\n","\n","First, <br>\n","the sum for a single trajectory through the right loop. <br>\n","We can write the sum explicitly and it is equal to $1$. <br><br>\n","\n","Then, <br>\n","the sum corresponding to taking the left action indefinitely. <br>\n","This sum is the same as the differential return we just computed, $0.4$. <br><br>\n","\n","Adding the two parts together, <br>\n","we find that the differential return is $1.4$.\n","\n","<br><br><br>\n","\n","\n","??? <br>\n","So if the agent's policy is to always take the left action, <br>\n","it can observe it's differential returns and realize it should switch to taking the right action (???) <br>\n","???\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DH14H3yfM1fw","colab_type":"text"},"source":["<img src=\"https://drive.google.com/uc?id=1_NEK4QWhG7pZFTrJoAUf5sI1ftiv82AA\" alt=\"3-13\" width=\"500\">\n","\n","<br>\n","\n","Right case <br><br>\n","\n","Now, <br>\n","let's look at the differential returns <br>\n","if the agnet's policy is to always take the right action. <br><br>\n","\n","This policy reusults in an average reward of $0.4$ <br>\n","and the differential return for the policy that takes the right action in state $S$ is $-0.8$. \n","\n","<br><br>\n","\n","\n","Now, <br>\n","what's the differential return <br>\n","for taking the left action once in state $S$ and <br>\n","then taking the right action indefinitely ? <br><br>\n","\n","Like before, <br>\n","we break up the sum into two parts. <br>\n","taking the left loop once results in a sum of $-1$ over the first five time steps. <br>\n","Adding the differential return from following $\\pi_R$ from state $S$, which we found to be $-0.8$ results in our answer of $-1.8$.\n","\n","<br><br>\n","\n","Once again, <br>\n","we see that the right action is preferred. <br><br>\n","\n","You may have noticed that the differential returns for $\\pi_R$ were lower than the differential returns for $\\pi_L$, <br>\n","even though $\\pi_R$ has a higher average reward. <br><br>\n","\n","This is because the differential return represents <br>\n","\" How much better it is to take an action in a state then on average under a certain policy \" <br><br>\n","\n","The differential return can only be used to compare actions, <br>\n","if the same policy is followed on subsequent time step ! <br><br>\n","\n","To compare policies, <br>\n","their average reward should be used instead. <br><br>\n","\n","Interestingly, <br>\n","the differential return is only a convergent sum <br>\n","if the subtracted constant is equal to the true average reward. <br>\n","If a lower or higher number is subtracted, the sum will diverge to positive or negative infinity !\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-kUVcavSP7Qg","colab_type":"text"},"source":["### Value Functions for Average Reward <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1-D3Fq1WobLiR04kCbaRtBF5OsIlE7Fpj\" alt=\"3-14\" width=\"500\">\n","\n","<br>\n","\n","Now that we have a valid definitoin of the return for Average Reward, <br>\n","$G_t \\quad = \\quad R_{t+1}-r(\\pi) \\quad + \\quad R_{t+2}-r(\\pi) \\quad + \\quad R_{t+3}-r(\\pi) \\quad + \\quad ...$ <br><br>\n","\n","we define value functions in the usual way, as the expected return. <br>\n","Similarly we can also define differential value functions as the expected differential return <br>\n","under a policy from a given state or state-action pair. <br>\n","$q_\\pi(s,a) \\quad = \\quad \\mathbb{E} \\big[ G_t|S_t=s,A_t=a \\big]$ <br><br>\n","\n","Like in the discounted setting, <br>\n","differential value functions can be written as Bellman Equations. <br>\n","Conveniently, they look like the previous ones we've seen. <br>\n","The only differ in that they subtract $r_\\pi$ from the immediate reward $r$, and there is no discounting $\\gamma$. <br>\n","$q_\\pi(s,a) \\quad = \\quad \\displaystyle \\sum_{s',r} p(s',r|s,a) \\big( r - r(\\pi) + \\sum_{a'} \\pi(a'|s')q_\\pi(s',a') \\big)$ \n","\n","<br><br>\n","\n","\n","This quantity captures <br>\n","how much more reward the agent will get by starting in a particular state <br>\n","than it would get on average over all states if it followed a fixed policy. \n","\n","<br><br>\n","\n","\n","\n","\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"R95U3T3GQ-Gz","colab_type":"text"},"source":["### Algorithm $\\quad : \\;$ Differential SARSA <br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1aqYX3viMr1xnI8-jCanztQ0sgRzcrQBv\" alt=\"3-15\" width=\"500\">\n","\n","<br>\n","\n","Many algorithms from the discounted case can be rewritten to apply to the Average Reward case. <br><br>\n","\n","For example, <br>\n","differential SARSA is very similar to the SARSA algorithm you've seen before. <br><br>\n","\n","Let's step through the differences. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1c8tad4_X9ljMLn5FG0cGRqdg1cAxqFt7\" alt=\"3-16\" width=\"500\">\n","\n","<br>\n","\n","A key difference is that differential SARSA has to track an estimate of the Average Reward $\\bar{R}$ under it's policy <br>\n","and subtract it from the sample reward in it's update. <br>\n","\n","This implementation does so <br>\n","with the incremental averaging techniques we've seen throughout the course. <br>\n","$\\bar{R} \\leftarrow \\bar{R}+\\beta(R-\\bar{R})$\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=12-qmnIIBN0ph-9A0twqA3Jt8A6s-9IA9\" alt=\"3-17\" width=\"500\">\n","\n","<br>\n","\n","Given this estimate, <br>\n","it then subtracts $\\bar{R}$ from the sampled reward $R$ in it's update. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1NxSP5SCb80NWLedkAsTVB81Cch_PrrAV\" alt=\"3-18\" width=\"500\">\n","\n","<br>\n","\n","In practice, <br>\n","we can get better performance with a slight modification to this algorithm. <br>\n","Instead of the exponential average of the reward to compute $\\bar{R}$, <br>\n","We use this update which has lower variance. <br>\n","$\\bar{R} \\leftarrow \\bar{R}+\\beta\\delta$\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"eQlp_lNjF7hv","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","  - Introduced the average reward objective <br><br>\n","\n","  - Defined differential returns and differential value functions for this setting\n","\n","<br><br>\n","\n","Next week, <br>\n","we'll talk about another way to optimize this average reward objective.\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]}]}
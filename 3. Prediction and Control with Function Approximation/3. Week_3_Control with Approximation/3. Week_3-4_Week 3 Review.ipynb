{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. Week_3-4_Week 3 Review","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyOT+W6J5m286c3uhJ7aLIH2"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1aQWc4yRhlOr","colab_type":"text"},"source":["### Week 3 Review <br><br>\n","\n","\n","This week, we talked about how to do control when using fucntion approximation. <br>\n","Let's go over the main ideas.\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TGBE8v38Ay7w"},"source":["### Summary <br><br>\n","\n","\n","That's all for this week. <br><br>\n","\n","  - We extended our Tabular control algoorithms to function approximation, <br>\n","  - discussed how exploration changes, <br>\n","  - and introduce a new way to think about the control problem.\n","\n","<br><br>\n","\n","Next week, <br>\n","we'll talk all about how to do Reinforcement Learning without learning the value function !\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rI5SHEULAzpA"},"source":["### Summary <br><br>\n","\n","\n","That's all for this week. <br><br>\n","\n","  - We extended our Tabular control algoorithms to function approximation, <br>\n","  - discussed how exploration changes, <br>\n","  - and introduce a new way to think about the control problem.\n","\n","<br><br>\n","\n","Next week, <br>\n","we'll talk all about how to do Reinforcement Learning without learning the value function !\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QADXZEHLA0wd"},"source":["### Summary <br><br>\n","\n","\n","That's all for this week. <br><br>\n","\n","  - We extended our Tabular control algoorithms to function approximation, <br>\n","  - discussed how exploration changes, <br>\n","  - and introduce a new way to think about the control problem.\n","\n","<br><br>\n","\n","Next week, <br>\n","we'll talk all about how to do Reinforcement Learning without learning the value function !\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TvSFbgf5iCo3","colab_type":"text"},"source":["Respresenting actions <br><br>\n","\n","\n","1 | 2\n","--- | ---\n","<img src=\"https://drive.google.com/uc?id=1b7muAzvArm-AtBDPxFY1Pnfea_63Sq3s\" alt=\"4-01\" width=\"500\"> | <img src=\"https://drive.google.com/uc?id=1mGymCYGjJcHd3kGSJPyOTIrqM_4egkgk\" alt=\"4-02\" width=\"500\">\n","\n","<br>\n","\n","First, we showed you how to estimate action values with function approximation. <br><br>\n","\n","If the action space is discrete, <br>\n","it's probably easiest to stack the state fearues. <br><br>\n","\n","If the action space is continuous or you want to generalize over actions, <br>\n","The action can be passed as an input like any other state variable. \n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"llKSfnqvi-Ud","colab_type":"text"},"source":["### TD Control with Approximation <br><br>\n","\n","\n"," <img src=\"https://drive.google.com/uc?id=11FT01h3rqx33gHUseqotMJOnEdPnSo2C\" alt=\"4-03\" width=\"500\">\n","\n","<br>\n","\n","Let's get some context abouth the next part of the module by looking at the algorithm map. <br><br>\n","\n","\n","Function appoximation puts us on the left side of the map. <br>\n","the focus of the first lecture was on the control algorithms in the bottom left corner, <br>\n","SARSA / Expected SARSA / Q-learning <br><br>\n","\n","These are all extensions of the Tabular control algorithms we covered in Course 2.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1rgeSmGfG4xJg5awcXjxz1OfCC2RniMg-\" alt=\"4-04\" width=\"500\">\n","\n","<br>\n","\n","The only difference between these algorithms and their tabular counterparts are the update equations. <br>\n","The ipdates are all adapted for function approximation in the same way, using the gradient to update the weights. <br><br>\n","\n","We also saw how episodic SARSA could be used to solve the mountain car problem. <br>\n","In this case, the larger step size is $0.5$ was able to learn more quickly. \n","\n","<br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SCfQYyUGlGan","colab_type":"text"},"source":["### Exploration with Approximation <br><br>\n","\n","\n","Next, <br>\n","we talked about exploration. \n","\n","<img src=\"https://drive.google.com/uc?id=1qAMvEFnTTlJ61fwKHTJdA2RvcqyYf6hE\" alt=\"4-05\" width=\"500\">\n","\n","<br>\n","\n","Optimistic initialization <br>\n","Optimistic initialization can be used with some structured featrue representations like Tile-Coding. <br><br>\n","\n","But in general, <br>\n","it's not clear how to optimistically initialize values with non-linear function approximators like neural networks. <br>\n","And it might not behve as expected. <br><br>\n","\n","For example, <br>\n","the optimism may fade too quickly.\n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1joxxaoJZ3UEQb1n7SzKOjFLqsawJ681l\" alt=\"4-06\" width=\"500\">\n","\n","<br>\n","\n","$\\epsilon - greedy$ <br>\n","Epsilon-greedy can be used regardless of the function approximator.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pnaHoa0hqjzj","colab_type":"text"},"source":["### Average Reward <br><br>\n","\n","\n","Finally, <br>\n","we talked about a new way to think about the continuing control problem. <br><br>\n","\n","Instead of maximizing the discounted return from the current state, <br>\n","we can think about maximizing the average reward that a policy receives overtime. \n","\n","<br><br>\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1J5Q2fChoDYfVDsO11PSXOCqMtaODhyJm\" alt=\"4-07\" width=\"500\">\n","\n","<br>\n","\n","We defined differential returns and differential values. <br><br>\n","\n","These enable the agent to assess the relative value of actions in the average reward setting ! \n","\n","<br><br><br>\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1F8ZzDw2Gecwi2WF-tspVc_KlmSLlVl9d\" alt=\"4-08\" width=\"500\">\n","\n","<br>\n","\n","### Differential semi-gradient SARSA <br><br>\n","\n","\n","Finally, <br>\n","we introduced differential semi-gradient SARSA, <br>\n","that approximates differential values to learn policies. <br><br>\n","\n","Differential SARSA is also in the left half of the algorithm map. <br><br>\n","\n","But unlike the algorithms we covered earlier in the week, <br>\n","it uses the average reward framework.\n","\n","\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BWYpAyBQt6D7","colab_type":"text"},"source":["### Summary <br><br>\n","\n","\n","That's all for this week. <br><br>\n","\n","  - We extended our Tabular control algoorithms to function approximation, <br>\n","  - discussed how exploration changes, <br>\n","  - and introduce a new way to think about the control problem.\n","\n","<br><br>\n","\n","Next week, <br>\n","we'll talk all about how to do Reinforcement Learning without learning the value function !\n","\n","<br><br><br><br><br>\n","\n","\n","\n","\n"]}]}